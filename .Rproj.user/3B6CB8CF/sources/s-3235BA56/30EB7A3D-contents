--- 
title: "Probabilités & Statistiques"
cover-image: "ProbStats.png"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
github-repo: dco61/MAT104

---

# Preface {-}

Lorsqu'on m'a demandé d'offrir un cours d'introduction aux probabilités et aux statistiques dans le cadre d'une formation propédeutique, à des étudiants fraîchement sortis du Lycée, j'ai dû mettre de côté 45 ans d'expérience en enseignement universitaire pour mettre au point une approche adaptée à cette clientèle. En effet, ces étudiants ne sont pas encore impliqués dans un contexte de recherche scientifique et de ce fait, ils ne sont pas susceptibles de saisir l'importance des méthodes statistiques dans ce contexte. Aborder le sujet de manière classique, analytique, tel que proposé dans la majorité des manuels d'introduction aux méthodes quantitatives ne saurait soulever l'intérêt et la motivation à appréhender cette matière. Tout ne serait qu'abstraction et la formation serait inefficace. Quel niveau de motivation aurait-on pour apprendre le Mandarin, si on ne se trouve pas en Chine et qu'on n'a pas l'occasion d'y séjourner de manière significative?

J'ai considéré plutôt une approche empirique, intuitive, fondée sur la simulation informatique. Mais les étudiants ne sont pas tous versés dans la programmation, de sorte qu'il fallait trouver un système suffisamment simple pour qu'on puisse manipuler les données sans avoir à maîtriser l'ensemble d'un langage. Les logiciels R et RStudio se sont vite immiscés dans le développement du cours, en raison bien sûr de la puissance de calcul qu'ils offrent, mais surtout pour la possibilité d'offrir des fonctions suffisamment simples d'utilisation pour l'ensemble des étudiants, peu importe leur habileté en programmation informatique. Il suffisait de produire ces fonctions selon les démonstrations à faire.

Une librairie contenant l'ensemble des fonctions utilisées et décrites dans le cours a été produite. Pour l'étudiant, il suffit d'en faire l'installation sous RStudio. On retrouvera les définitions de ces fonctions en Appendice.

Pour rendre les discussions plus concrètes encore, il fallait que l'apprentissage se fasse dans un contexte familier aux étudiants. J'ai donc créé un 'micro-monde', une banque de données, calqué sur la population Haitienne de 2019: 11,262,702 sujets répartis dans les 10 départements en respectant les proportions de la population totale, la pyramide d'âge, la scolarisation, etc. Des variables ont été générées pour simuler les opinions des adultes concernant un certain nombre de candidats à la présidence, ou concernant leur avis concernant un projet de nouvelle Constitution; et des variables ont été générées pour simuler un indice de condition de vie, un indice de propension à la criminalité, entre autres.

Une grande proportion des exemples, des exercices et des travaux tourne autour de cette banque de données, d'où il est simple de tirer des échantillons, d'étudier les distributions d'échantillonnage de différents indices statistiques, et de simuler des recherches réalistes.

Le présent document est une compilation de tout ce qui a été couvert au cours de la session Printemps 2021, mais est en constante évolution. La première version a été produite sous pression, et on y trouvera un grand nombre d'erreurs de tout type. Plusieurs chapitres manquent à l'appel à ce stade, mais seront ajoutés dans un futur que j'espère proche.

Tout commentaire constructif et suggestion sera bienvenu! Mon objectif est de produire un document qui pourra aider les étudiant(e)s de l'ISTEAH à se familiariser avec les fondements des méthodes quantitatives qu'ils(elles) seront appelé(e)s à utiliser pour leur travail de deuxième ou troisième cycle.

Daniel Coulombe, Ph.D.

<!--chapter:end:index.Rmd-->

---
title: 'MAT104 - Cours 1: <br> Introduction à R et opérations de base'
author: Daniel Coulombe
date:  "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
  pdf_document: 
    toc: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction 

## Qu’est-ce que le HASARD ?
* Le hasard est ce qu’on présume être à l’origine d’une observation survenant sans que l’on puisse en déterminer la cause. Par exemple, Un générateur de nombres aléatoires produit des valeurs qu’on ne peut prévoir: elles ne sont que le fait du hasard.

* Or, l’humain cherche les causes de toute chose, mais lorsqu’on n’en trouve pas, on fait appel au hasard! Un monde sans hasard serait un monde complètement déterminé, un monde où tout serait fixé, dans lequel on pourrait prédire exactement le futur de toute chose et de ce fait, on pourrait véritablement parler de DESTIN auquel on ne saurait échapper! Tout serait écrit d’avance!

* La notion de hasard réfère à ce qu’on n’arrive pas à expliquer. Le premier objectif de la Science étant d'expliquer les phénomènes, la recherche scientifique s'efforce de réduire la part de hasard dans l’explication de ces phénomènes. Pour nos besoins, on associera hasard à **erreur**, ou **résidu**!

## Hasard et Probabilité  

Le hasard est lié à la notion de PROBABILITÉ: 

* Le lancer d’un dé donnera une valeur de 1 à 6, et cette valeur sera le fruit du hasard: on ne peut prédire le résultat à moins que le dé ait été truqué
* Par contre, on peut calculer les chances d’obtenir une valeur donnée: puisqu’un dé a 6 faces et qu’il va nécessairement tomber sur l’une d’elles, on dira qu’on a une chance sur 6 d’obtenir un ‘1’, ou un ‘2’, ou un ‘3’...
* ... et connaissant la probabilité d’obtenir un résultat, on peut prendre une décision dans un contexte donné...
* Par exemple, ‘la présence de gros nuages gris à l’horizon m’indique qu’il va probablement pleuvoir aujourd’hui. En conséquence, je n’irai pas à la plage!’
* La notion de probabilité est omniprésente en recherche scientifique, tout comme dans le quotidien de tout individu...

## Qu'est-ce que l'INFINI
  
* Sans s'embourber dans les considérations philosophiques, considérons la compréhension populaire (mais pas nécessairement juste!) de l'infini: ce qui n'est borné par aucune limite...
* Dans ce sens, ce qu'on perçoit comme infiniment grand est de fait infiniment petit!
* Ce qu'on perçoit comme infiniment petit est de fait infiniment grand!
* Pour nos besoins, on associera **infini** à **très grand nombre**

## Objectif de la Science

* Le scientifique cherche à repousser le hasard, autant que possible, dans l’explication d’une observation. Il cherche à déterminer les ‘causes’ de cette observation.
* Mais en général, il n’y parvient que partiellement: une part d’erreur, ou une part résiduelle demeurera présente et une tâche essentielle du chercheur sera de quantifier cette part d’erreur.
* A ce niveau, la notion de probabilité deviendra partie du décors!

## Objectif global du cours: 

Développer une culture **PROBABILISTE** dans le contexte d’une approche scientifique de l’examen des déterminants d’une observation...

Pour y parvenir, nous utiliserons une approche avant tout intuitive, fondée sur la simulation plutôt que sur les formalisations mathématiques. Et la simulation ne peut se réaliser qu'à l'aide d'outils informatiques appropriés: **R** et **RStudio**. Il sera donc nécessaire de maîtriser quelques éléments de base de la programmation.

De nombreux ouvrages et sites Internet présentent d'excellentes introductions au langage R. On trouvera des références à la fin de ce chapitre, dans lequel on abordera le strict minimum pour nos besoins immédiats. Le lecteur est fortement encouragé à approfondir ses connaissances en parcourant les ouvrages cités. 


## Introduction: Installation des logiciels

Pour les besoins des discussions à venir, et surtout de la méthodologie utilisée, les logiciels R et RStudio seront nécessaires. De plus, sans que l'on cherche à devenir spécialistes de la programmation R, nous devons connaître les éléments fondamentaux du langage. Et comme pour l'apprentissage de tout langage, incluant R, la pratique active est essentielle. En effet, il ne suffit pas de lire les documents fournis, mais de mettre la main à la pâte et d'utiliser les fonctions permettant de résoudre un problème et d'obtenir réponse aux questions qui pourraient se poser. Bref, vous devez être **activement engagés** dans l'apprentissage du langage R et des outils qui en facilitent l'utilisation. Prenez le temps nécessaire pour examiner comment les diverses fonctions et opérateurs sont utilisés pour la solution des problèmes présentés, et exécutez les codes R proposés pour en vérifier les résultats. Les opérations à effectuer sont relativement simples, et très souvent il suffit de copier un texte et de le coller dans une fenêtre spécifique. Plus vous serez actifs dans l'apprentissage du système, plus vos compétences seront manifestes.

### Installation de R et de RStudio:

L'installation des logiciels requis est simple et n'est pas différente de l'installation de tout autre logiciel. Effectuez les deux étapes suivantes dans l'ordre:

* Rendez-vous sur le site de téléchargement de [**R**](http://www.r-project.org/)  et installez la version de R correspondant à votre système d'exploitation.

* Rendez-vous sur le site de téléchargement de [**RStudio**](https://www.rstudio.com/products/rstudio/download/#download)   et cliquez sur le lien indiqué sous **Download**, et finalement sur le bouton libellé **DOWNLOAD RSTUDIO FOR WINDOWS**. Si vous utilisez un système d'exploitation différent, sélectionnez la version pertinente dans la liste apparaissant sous **ALL INSTALLERS** . Suivez les instructions pour installer RStudio.

Pour vous assurer que tout a bien fonctionné, ouvrez le programme RStudio et allez dans **FICHIER** > **NOUVEAU** > **SCRIPT R**. Un document vide s'ouvrira. Dans ce document, tapez le texte indiqué dans le cadre ci-dessous (NOTE: l'opérateur d'assignation de variables, '<-', peut s'obtenir en appuyant simultanément sur <ALT>'-') :

```{r 01-Chap1-1, eval = FALSE}
x <- 25
x == 10
```

Sélectionnez ensuite que vous venez d'écrire en cliquant et en faisant glisser le curseur pour surligner les deux lignes de code. Finalement, cliquez sur le bouton marqué "**Exécuter**". Si tout fonctionne correctement, la console doit afficher **FALSE**.

Notez que vous pouvez exécuter une ligne de votre script à la fois en déplaçant votre curseur sur cette ligne et en appuyant sur **CTRL-ENTER** ou **COMMAND-RETURN**, selon que vous utilisez Mac OSX, Linux ou Windows. Un autre raccourci utile est *CTRL-A** (COMMAND-A sur Mac) qui met en évidence toutes les lignes de code dans l'éditeur de texte.


## L'environnement de travail

Par défaut, le langage utilisé par les logiciels R et RStudio est l'anglais. Pour utiliser le français, et pour les menus et les messages apparaissant à l'écran, il suffira d'inscrire dans la console R, ou celle de RStudio, la commande:

```{r 01-Chap1-2}
Sys.setenv(LANG = "fr")
```
On peut également obtenir un environnement espagnol en remplaçant "fr" par "es", dans cette commande.

Les logiciels requis (**R** et **RStudio**) sont maintenant prêts à l'action. Dans cette section, nous explorerons votre environnement de travail en détail. Une bonne connaissance de cet environnement vous donnera une grande efficacité dans les opérations à venir.


### La console R

Lorsque vous lancez **R**, une fenêtre s'ouvre, quelques paragraphes apparaissent, suivis par une invite de commande: **>**. Vous êtes à la **CONSOLE**. L'interaction de base avec R se fait en inscrivant des commandes à ce niveau.

![Console](./images/rterminal.png)

#### Utilisation de la Console

- Tout en bas de la CONSOLE, vous trouverez le symbole “>”. Il indique l'endroit où vous pouvez inscrire des expressions à exécuter. Vous inscrivez une commande et appuyez sur 'ENTRÉE'. Le résultat de la commande est immédiatement affiché (sauf si la commande est une définition de variable!). Si la commande est erronée, un message d'erreur sera affiché. Par exemple:

```{r 01-Chap1-4}
6+7*5/2-2

```
On retrouve dans cette expression les 4 opérateurs arithmétiques de base: +, -, * et /.  La multiplication (*) et la division (/) ont précédence et sont exécutées en premier lieu. Par la suite, l'addition (+) et la soustraction sont appliquées. Le résultat de l'expression ci-dessus est donc obtenu par:

7*5/2 = 35/2 = __17.5__

6+17.5-2 = __21.5__

On peut également utiliser les parenthèses pour altérer la séquence des opérations. Reprenant l'exemple précédent:
```{r 01-Chap1-5}
(6+7)*5/2-2

```
Ici, le contenu de la parenthèse est exécuté avant toute autre opération, suivi des opérations impliquant multiplication et division, et finalement on termine par les opérations impliquant addition et soustraction:

42*5/2 = __105__

105-2 = __103__


Vous pouvez donc utiliser la console comme un simple calculateur! Mais il y a beaucoup plus à faire avec R...

Transcrivez, ou Copiez/Collez les lignes suivantes dans la console:

```{r 01-Chap1-6, eval=FALSE}
x <- 1:10
y <- round(rnorm(10, x, 1), 2)
df <- data.frame(x, y)
df
```
Ce que vous venez de faire, c'est de créer une variable **x** contenant les entiers de 1 à 10; puis vous avez généré 10 nombres au hasard et les avez arrondis à 2 décimales près, formant une nouvelle variable **y**. Finalement, vous avez uni les deux variables dans un contenant, **df** pour ensuite afficher le résultat!  

```{r 01-Chap1-7, echo=FALSE}
x <- 1:10
y <- round(rnorm(10, x, 1), 2)
df <- data.frame(x, y)
df
```

Sous R, il est très facile d'obtenir des graphiques. Essayez les commandes suivantes, et voyez le résultat:

```{r 01-Chap1-9}
plot(x)
plot(x,y)
boxplot(x~rep(1:2,5))
```
  
Essentiellement, l'environnement R permet d'effectuer autant les calculs simples comme le ferait une calculatrice, que des calculs aussi complexes que nécessaire pour solutionner un problème. 

Plusieurs fonctions sont regroupées dans les différents onglets du menu principal dans la portion supérieure de la console. Mais on aura avantage à utiliser un interface graphique offrant beaucoup plus de versatilité: **RStudio**...


#### Exercice 1

Lancez R (pas RStudio...) et effectuez les calculs proposés ci-dessous:

1. Au marché, vous achetez pour 200 gourdes de pommes de terre, pour 327 gourdes de raisins, pour 178 gourdes d'ananas et pour 1238 gourdes de poisson. Utilisez la console R pour calculer le coût total de vos achats.
1. Arrivé à la caisse, vous vous rendez compte que qu'un des ananas est gâté et vous décidez de le retirer de votre achat, ce qui réduit le coût de 53 gourdes. Utilisez R pour calculer la somme qu'il vous reste à payer.

3. Pour que tout soit bien clair dans la transaction que vous effectuez au marché, lancez les commandes suivantes dans la console:

  
```{r 01-Chap1-10, eval=FALSE}
pomme.de.terre <- 200
raisins <- 327
ananas <- 178 - 53
poisson <- 1238

a <- 'Le montant total à payer ce matin est: '
montant <- pomme.de.terre + raisins + ananas + poisson
cat(a,montant,' gourdes')

```
4. Les prix de certaines denrées ont changé depuis votre dernière visite au marché. Pour les pommes de terre, il y a une augmentation de 20%, et pour le poisson, une réduction de 38 gourdes. Vous voulez acheter les mêmes denrées que la dernière fois. Obtenez le montant de votre facture,,,


On peut donc utiliser la console de R comme une simple calculatrice. Mais il y a beaucoup plus de choses que R nous permet de faire. Et RStudio nous rend la tâche plus facile et efficace!


### RStudio: interface graphique pour R

RStudio se compose de 4 fenêtres principales ('panes'):

- Éditeur
- Console (similaire à la console R)
- Environnement de travail / Historique des commandes
- Fichiers / Graphiques / Librairies / Aide

![Console](./images/rstudio_panes.png)


#### RStudio: Survol des différentes fenêtres

1. __Éditeur__ : Création de fichiers que l'on peut sauvegarder et exécuter plus tard. On peut également créer des fichiers Markdown intégrant texte. commandes et résultats. 

2. __Console__ : Commandes inscrites directement ou importées du presse-papier pour obtenir un résultat donné.Cette fenêtre reproduit simplement la console de R

3. __Espace de travail/Historique__ : Liste des variables, fonctions, commandes utilisées antérieurement

4.  __Fichiers/Graphiques/Librairies/Aide__ : Affichage des graphiques, des documents d'aide et différents autres items.

##### La fenêtre CONSOLE

![La console R](./images/pane_console.png)

- Utilisez la **Console** pour inscrire ou coller des commandes et obtenir des résultats...

- ... ou pour consulter un fichier d'aide concernant une fonction: inscrivez '?*Nom_de_la_fonction*. Par exemple: ?mean

Une fenêtre s'ouvre dans votre fureteur:

![Console](./images/aide.png)


##### Le panneau de l'éditeur

![L'éditeur](./images/pane_source.png)
  
- Vous utiliserez la fenêtre **Éditeur** pour créer et éditer des scripts R et des fichiers Rmd (Markdown), principalement.

- La barre de menu dans la portion supérieure de cette fenêtre contient des raccourcis pour transmettre des lignes de code à la console pour une évaluation.

Dans la figure suivante, des commandes ont été inscrites dans l'éditeur. Puis, le bouton 'RUN' de la barre d'outils a été activé, et l'ensemble des commandes ont été transmises et exécutées dans la Console:

![L'éditeur](./images/EditCons.png)

##### La fenêtre *Fichiers/Graphiques/Librairies/Aide*

![L'éditeur](./images/pane_plots.png)

- Par défaut, les graphiques produits dans R sont affichés dans cette fenêtre, sous l'onglet **Plots** 
  - La barre de menu vous permet un Zoom, d'exporter et de naviguer entre les différents graphiques produits dans une session. 
- Lorsqu'une aide est requise (par exemple '?mean'), la documentation apparait sous l'onglet **Help**

##### Fenêtre Historique/Environnement/Connexions/Tutoriels

La fenêtre supérieure/droite présente 4 onglets permettant:
- l'affichage des objets se trouvant dans votre espace de travail: variables et fonctions
- l'historique des commandes qui ont été exécutées
- les connexions établies avec différentes sources de données
- des tutoriels sur différents aspects de la programmation R

## Le langage R: élements de base

R est un langage composé d'une foule d'éléments dont une gamme extrêmement étendue de fonctions. Certaines de ces fonctions font partie du système de base, d'autres proviennent de la communauté R, et d'autres encore que vous pourriez être amenés à écrire! L'ensemble forme un outil très sophistiqué qui ouvre des portes à une multitude de méthodes de traitement de données. Mais puisqu'il faut un commencement à tout, examinons les éléments les plus simples.

- Sous R, à peu près tout ce que nous faisons, c'est d'appliquer des **fonctions** sur des **données**.
- **Données**:  simples ou complexes, comme 7, "sept" ,  $7.000$, la matrice $\left[ \begin{array}{ccc} 7 & 7 & 7 \\ 7 & 7 & 7\end{array}\right]$

- **Fonctions**: des termes tels que $\log{}$, $+$ (2 arguments), $<$ (2 args), $\mod{}$ (2 args), `mean` (un seul argument). Une fonction peut n'impliquer aucun argument, ou en nécessiter un ou plusieurs.

> Une fonction est un outil qui traite un objet à l'entrée (**arguments**), pour en produire un nouvel objet à la sortie (**valeur retournée**). Il est possible que cette opération produise aussi des **effets secondaires** (des éléments de sortie secondaires), selon le cas. 

### Les données

Plusieurs types de données existent:

- **Booléennes** ('boolean'): données binaires. Sous R, ces valeurs sont identifiées par `**TRUE**` ou `**FALSE**`.

- **Entiers** ('integers'): nombres entiers (positifs, négatifs ou zéro)

- **Caractères** ('characters'):  bloc de longueur fixe de valeurs binaires;
**Alphanumériques** ('strings'): séquence de caractères

- **Nombre réel** ('Floating point numbers') : un nombre en continuité avec les valeurs adjacentes, comme 3.14159 

- **Données manquantes ou non-définies** ('Missing or ill-defined values'): `NA`, `NaN`, etc.


### Opérateurs arithmétiques
Vous pouvez utiliser R, vous vous en doutez bien, comme un super-calculateur!...

Commande| Description
--------|-------------
**`+,-,*,\`** | addition, soustraction, multiplication, division
**`^`** | élever à la puissance...
**`%%`** | Ce qui reste après une division (ex: `8 %% 3 = 2`)
**`( )`** | changement dans l'ordre d'exécution des opérations
**`log(), exp()`** | logarithmes and exposants (ex: `log(10) = 2.302`)
**`sqrt()`** | racine carrée
**`round()`** | arrondi à l'entier le plus près (ex: `round(2.3) = 2`)
**`floor(), ceiling()`** | arrondi vers le bas, ou le haut 
**`abs()`** | valeur absolue
 
  
```{r 01-Chap1-11}
7 + 5 # Addition
7 - 5 # Soustraction
7 * 5 # Multiplication
7 ^ 5 # Exponentiation
7 / 5   # Division
7 %% 5  # Modulus
7 %/% 5 # Division entière 
```
  
Il existe aussi des termes faisant référence à des constantes dont un exemple est **pi**:
  
```{r 01-Chap1-12}
2*pi*5        # Circonférence d'un cercle de rayon = 5
(4*pi*5^2)/3  # Volume d'une sphère de rayon = 5
```

Plusieurs autres types d'opérations impliquent des fonctions, comme on le verra plus loin. Une d'entre elles est **sqrt()**, qui permet d'obtenir la racine carrée de son argument (le contenu des parenthèses). Par exemple, la racine carrée de 98.74 est:

```{r 01-Chap1-13}
a <- 98.74
sqrt(a)
```
  
De la même manière, notons les fonctions:
  
```{r 01-Chap1-14}
round(3.14159,2)  # Arrondissement
floor(3.14159)    # Arrondissement à l'entier inférieur
ceiling(3.14159)  # Arrondissement à l'entier supérieur
```

Plusieurs autres fonctions de ce type s'ajouteront bientôt à la liste!... et il est très probables que vous soyez amenés à écrire vos propres fonctions sous peu!

### Exercice 2

Utilisez l'éditeur de RStudio pour répondre aux questions suivantes:

1. La largeur d'un rectangle mesure 4 centimètres, et sa hauteur mesure 18 centimètres. Quelle est la périphérie de ce rectangle?
2. Quelle est la surface du rectangle décrit en (1)?
3. Dans un mètre, il y a 39.37008 pouces. Combien y'a-t'il de pouces dans 2.5 mètres?
4. Le volume d'une sphère est égal à $(4\pi r^2)/3$. Quel serait le volume d'une sphère dont le **diamètre** serait égal à 8.92745?
5. Si x = 4.5327 et y = 8.4893, quelle est la valeur de l'expression: $$\sqrt{(x^3+y^4)}-(x/2+x\cdot y/5)^3 $$ 

### Opérateurs de comparaisons logiques

Les **Comparaisons** sont des opérateurs binaires. Elles prennent deux objets, les comparent, et produisent une valeur booléenne ('TRUE' ou 'FALSE'):

```{r 01-Chap1-15}
7 > 5   # plus grand que...
7 < 5   # plus petit que...
7 >= 7  # plus grand ou égal...
7 <= 5  # plus petit ou égal...
7 == 5  # égal à...
7 != 5  # différent de...
"Carl"=="Marie"   # comparaison de caractères alphanumériques
"Carl"<"Marie"    # Ordre alphabétique
substr("Marie",1,2)==substr("Judith",1,2)  # Comparaison de portions de texte (2 premiers caractères) 

```
À ces opérateurs logiques, on peut ajouter:

!x : Négation de x

```{r}
!(5>7)
```

%in%: Est-ce que le premier argument se trouve dans le second?

```{r}
x <- c(1,2,3,4)
y <- c(2,4,6,8,10)
x %in% y
```
identical(): Est-ce que les arguments sont identiques?

```{r}
identical(x,y)
a <- "Jacques"
b <- "Jacques"
identical(a,b)
```

Plusieurs autres fonctions s'ajouteront à cette courte liste selon les besoins.

### Opérateurs Booléens
  
& ('and'): l'opérateur '&' prend deux arguments logiques et retourne TRUE seulement s'ils sont tous les deux vrais:

```{r}
x <- c(TRUE,TRUE,FALSE,FALSE)
y <- c(TRUE,FALSE,TRUE,FALSE)
x & y
```
| ('or'): L'opérateur '|' prend aussi deux arguments logiques et retourne TRUE si **au moins** un d'eux est vrai:

```{r}
x <- c(TRUE,TRUE,FALSE,FALSE)
y <- c(TRUE,FALSE,TRUE,FALSE)
x | y
```
xor() : OR exclusif: la condition est vraie seulement si un seul des éléments comparés est vrai:

```{r}
x <- c(TRUE,TRUE,FALSE,FALSE)
y <- c(TRUE,FALSE,TRUE,FALSE)
xor(x,y)
```

&& et || : Même résultat que '&'  et '|' pour des valeurs scalaires, mais pour des vecteurs, l'opération ne se fait que sur le premier élément des vecteurs impliqués:

```{r}
x <- c(TRUE,TRUE,FALSE,FALSE)
y <- c(FALSE,FALSE,TRUE,FALSE)
x && y
x || y
```

Ces opérateur seront particulièrement utiles lorsqu'une programmation nécessitera des branchements conditionnels. Dans ces cas, les opérateurs logiques et booléens seront généralementliés:

```{r 01-Chap1-16}
(5 > 7)               # Condition A 
(6*7 == 42)           # Condition B
(5 > 7) & (6*7 == 42) # Condition A ET Condition B
(5 > 7) | (6*7 == 42) # Condition A OU Condition B
xor((5 > 7), (6*7 == 42))
``` 

###  Autres fonctions utiles
  
- `typeof()` : Fonction affichant le type d'objet (type = **integer**, **double**, **logical**, **complex**, **character**). Par exemple:
  
  
```{r 01-Chap1-17}
x <- "Pierre"
typeof(x)

x <- 3.14159
typeof(x)

x <- 1:5
typeof(x)

typeof(x>2)

```

- `is.'_type_`()` Fonction retournant des valeurs booléennes si l'argument est d'un type donné (type = **integer**, **double**, **logical**, **complex**, **character**). Par exemple:

  
```{r 01-Chap1-18}
x <- "Pierre"
is.character(x)

x <- 3.14159
is.integer(x)

x <- 1:5
is.logical(x)

is.logical(x>2)

```


- `as.`_type_`()` : Fonction qui tente de "convertir" son argument en une valeur d'un autre type (eg: alphanumérique vers numérique). Par exemple:

```{r 01-Chap1-19}
x <- "Pierre"
as.logical(x)   # La conversion n<est pas possible

x <- "3.14159" 
typeof(x)
as.numeric(x)   # Conversion d<un texte en valeur numérique

x <- 3.14156 
typeof(x)
as.character(x) # Conversion d'une valeur numérique en texte

```

<small>**Cas Spécial**: `as.factor()` est une fonction importante permettant d'informer R lorsque des nombres sont des encodages et non pas des valeurs numériques (e.g., 1 = primaire; 2 = secondaire; 3 = Baccalauréat) </small>

  
### Variables
  
Comme c'est le cas en algèbre, les variables sont une forme de raccourci. Au lieu d'écrire 3.1415926... tout le temps, nous pouvons simplement écrire **pi**.

Les variables sont créées avec l'**opérateur d'assignation**, `<-`, '->', ou `=`. L'affectation de données à une variable se fait généralemende droite à gauche - la valeur de droite est affectée au nom de gauche. Mais l'inverse fonctionne aussi (assignation 'fléchée' seulement). Donc, 3 façons de d'assigner une valeur à une variable::

```{r 01-Chap1-20}

x = 8
x <- 8 # Méthode préférée!
8 -> x
x

```
Vous pouvez utiliser presque n'importe quoi comme nom de variable dans R. Les seules règles sont :

1. le nom d'une variable est formé de lettres et de chiffres, majuscules et/ou minuscules, et des deux seuls symboles permis: '.' et '_'.
2. Le nom d'une variable ne doit pas commencer par un nombre ou un '_'
3. Le nom d'une variable est sensible à la case:  A1 et a1 sont deux variables différentes!

Des exemples valides de noms de variables seraient:

```{r 01-Chap1-21}
abcd <- 3
Abcd <- 5
A.bcd <- 8
A_bcd <- 2
A1.B2_c3 <- 7

```
  
**Notes:**
  
1. L'utilisation de noms  pour désigner des variables rend le code plus facile à lire et à corriger au besoin.

- Utilisez des noms descriptifs pour les variables
  - Bien: `n.étudiants <- 35`
  - Mauvais: `ns <- 35 `

2. Variables et espace de travail:
  - On peut déterminer les noms de variables utilisés en utilisant la fonction **ls()**:
  
```{r 01-Chap1-22}
ls()
```

On peut éliminer des variables avec la fonction **rm()**:
  
```{r 01-Chap1-23}
rm("A.bcd")
ls()
```

3. L'assignation de valeurs à une variable existante écrase la valeur existante

### Exercice 3

1. Obtenez la liste des variables qui ont été créées dans votre espace de travail: fonction **ls()**
2. Nous n'avons plus besoin de ces variables. Effacez 3 des variables qui se trouvent dans votre espace de travail: fonction **rm()**
3. Pour vous faciliter la tâche, effacez toutes les variables en utilisant **rm(list=ls())**. Cette commande efface tout le contenu de **list**, objet qui est lui-même défini par la liste de toutes les variables...
4. Il y a 6.28981077 $m^3$ de pétrole dans un baril. Définissez une variable contenant ce facteur de conversion.
5. Un baril de pétrole vaut $60. Vous avez besoin de 25 $m^3$ de pétrole pour vos opérations hebdomadaires. Quel sera le montant de votre facture de pétrole?

### Introduction aux structures de données: les vecteurs

En R, un vecteur est un ensemble ordonné d'objets du même type. Il est formé par la fonction **c()**, `c()` retourne un vecteur contenant tous ses arguments en séquence:

```{r 01-Chap1-24}
a1 <- 1:5                           # séquence 1, 2, 3, 4, 5
a1

a2 <- c("Pierre", "Jean", "Jacques")   # Vecteur d'items alphanumériques
a2

a3 <- c(3, 5, 8, 6, 9)                 # Vecteur num/rique
a3

a4 <- c(1, TRUE, "Pierre")             # Trois types différents!
a4
```
  
Ce dernier exemple nécessite des explications:
  - Si un vecteur est composés d'items de types différents, tous les items adoptent le type de l'item du type le plus élevé dans la hiérarchie:
    1. Donnée logique:  TRUE, FALSE
    2. Donnée entière:  1, 2, 3...
    3. Donnée numérique: pi, 0.27, 4,37
    4. Donnée alphanumérique: "R est génial!"
    

### Indexation:

- `vec[1]` est le premier élément, et `vec[4]` est le quatrième élément du vecteur `vec`
```{r 01-Chap1-25}
a3        # Vecteur original
a3[4]     # Quatrième élément de a3
a3[1:3]   # Trois premiers éléments de a3
```
- `vec[-4]` est un vecteur contenant tous les éléments du vecteur 'vec' sauf le quatrième:

```{r 01-Chap1-26}
a3
a3[-4]
```
-`which()` retourne les indices d'un vecteur booléen qui sont VRAIS ('TRUE'):

```{r 01-Chap1-27}

x <- c(2,7,6,8,4,4,5,8,2,5,3,4)
which(x <= 5)
x[which(x<=5)]
```
Il est également possible d'éditer un item faisant partie d'un vecteur. Par exemple, on peut multiplier par 10 le troisième élément de x:

```{r 01-Chap1-28}
x
x[3] <- x[3] * 10
x

```

### Arithmétique vectorielle

Les vecteurs permettent d'effectuer facilement de nombreux calculs en une seule fois : ajouter un chiffre à une liste de nombres, les diviser tous par 3, etc. Et tant que deux vecteurs sont de même longueur, nous pouvons les combiner de manière naturelle :

```{r 01-Chap1-29}
x <- 1:10    # Séquence des nombres 1 à 10
x + 5
x * 5
sqrt(x)     # Racine carrée des éléments du vecteur

y <- 25:34   # Séquence des nombres 25 à 34
y
y - x
x < 6

```

### Fonctions appliquées sur des vecteurs

Comme en mathématiques, une fonction est un moyen de faire correspondre une entrée à une sortie. Vous pouvez facilement repérer les fonctions car elles utilisent des parenthèses : (). Nous avons déjà vu la fonction de concat/nation, **c()** utilisée (par exemple) pour créer des vecteurs.

Nous pouvons également appliquer un certain nombre de fonctions omniprésentes à notre entrée vectorielle. En voici un petit (**très petit!**]) avant-goût :

Commande| Description
--------|------------
`sum(vec)` | Somme de tous les éléments de `vec`
`mean(vec)` | moyenne de `vec`
`median(vec)` | médiane de `vec`
`min(vec), max(vec)` | élément minimum et maximum de `vec`
`sd(vec), var(vec)` | écart-type et variance de `vec`
`length(vec)` | nombre d'éléments dans `vec`
`pmax(vec1, vec2), pmin(vec1, vec2)` | exemple: `pmax(quiz1, quiz2)` retourne le plus élevé de quiz 1 et de quiz 2 pour chaque étudiant
`sort(vec)` | retourne `vec` en ordre croissant
`order(vec)` | retourne l'indice qui ordonne `vec`
`unique(vec)` | retourne une liste des éléments uniques `vec`
`summary(vec)` | retourne les 5 indices résumant 'vec'  
`any(vec), all(vec)` | Estce qu'au moins 1 élément de 'vec' est vrai?; Est-ce que tous les éléments de 'vec' sont vrais?

Par exemple:

```{r 01-Chap1-30}
x <- round(runif(10,10,20))  # 10 nombres aléatoires entre 10 et 20

sum(x)                      # Somme des 10 valeurs de x

prod(x)                     # Produit des 10 valeurs de x

range(x)                    # Étendue des valeurs de x 

log(x)                      # Logarithme naturel des 10 valeurs de x 

length(x)                   # Longueur du vecteur: nombre d'éléments dans x

y <- sample(1:5,10,replace=TRUE) # Sélection de 10 nombres compris entre 1 et 5, au hasard
y
unique(y)
```

### Création de séquences régulières de nombres

Très souvent, il sera utile de générer des séquences de nombres également espacés. On a déjà rencontrer des expressions telles que x = 1:10. Une telle expression produit un vecteur des nombres entiers compris entre 1 et 10. Mais on n'a pas à se limiter aux nombres entiers:

```{r 01-Chap1-31}
x <- 1:10                    # Séquence des entiers de 1 à 10
x

y <- seq(0, 10, by = 0.5)    # Séquence de 0 à 10, par étape de 0.5
y

z <- seq(0, 10, length.out = 20) # 20 valeurs équidistantes entre 0 et 10
z

w <- rep(c(1,2,3,4,5),3)     # 3 Répétitions du vecteur
w

```
### Autres fonctions vectorielles utiles

Le nombre de fonctions disponibles en R est très élevé! Vous apprendrez à les connaître à l'usage. Mais certaines doivent être mentionnées maintenant en raison de leur utilité:

head(): permet d'afficher les premiers éléments d'un vecteur
tail(): permet d'afficher les derniers éléments d'un vecteur
class(): permet d'afficher ce que R considère être le type d'une varialbe

```{r 01-Chap1-32}
x <- runif(50,1,20)  # Génération de 50 nombres aléatoires compris entre 1 et 20

head(x)             # Affichage des 6 premières données (défaut)

head(x, 10)         # Affichage des 10 premières données

tail(x)             # Affichage des 6 dernières données (défaut)

tail(x, 10)         # Affichage des 10 dernières données

class(x)            # Type d'objet, vu par R

```
### Exercice 4

1. Exécutez les commandes suivantes pour générer deux vecteurs composés de 20 nombres entiers compris entre 1 et 10. Ces données seront sauvegardés dans les variables x et y respectivement:
```{r 01-Chap1-33}
x <- runif(20,1,10)
y <- runif(20,1,10)
```
2. Dans la variable x, quelles sont les valeurs supérieures à 5?
3. Dans la variable x, quelle est la somme des valeurs supérieures à 5?
4. Quelle est la valeur de $\sum_{i = 1}^{20}{x_i^2}$ ?
5. Définissez un vecteur contenant la séquence des valeurs de 1 à 20
6. Calculez la somme de x et du vecteur que vous venez de définir.
7. Déterminez l'étendue de la variable x.
8. Calculez la somme des produits xy.
9. Déterminez le vecteur des valeurs les plus élevées de x et de y.
10. Déterminez le vecteur des valeurs les plus petites de x et de y.


### Trucs utiles
Touche | Description
----------|-------------
`<tab>` | auto-compléter les commandes et les noms de fichiers, et fournit une liste des argumens de fonctions. Très utile!
`<up>` | revient aux commandes précédentes au niveau de la console
`<ctrl-up>` | fournit une liste historique des commandes précédentes correspondant à une commande non terminée
`<ctrl-enter>` | Colle la ligne courante de l'éditeur dans la console. Cela permet de vérifier le bon fonctionnement des commandes.
`<ESC>` | Terminer une commande non complétée


### Entrée de données

Afin de procéder au traitement de données, il faut être en mesure d'introduire les données à traiter dans l'environnement R. Plusieurs méthodes existent à cette fin.

#### Entrée manuelle de données: création de vecteurs 

Lorsque le nombre de données est petit, on peut les entrer directement dans la console en créant une structure de données, tel qu'un vecteur comme on l'a vu dansles paragraphes précédents. Par exemple, si les notes obtenues par 23 élèves à un examen de mathématiques sont:

0, 1, 2, 12, 12, 14, 18, 21, 21, 23, 24, 25, 28, 29, 30, 30, 30, 33, 36, 44, 45, 47, 50

on pourra définir une variable telle que:

```{r 01-Chap1-34}
Notes <- c(0, 1, 2, 12, 12, 14, 18, 21, 21, 23, 24, 25, 28, 29, 30, 30, 30, 33, 36, 44, 45, 47, 50)
Notes

```
Ici, l'opérateur de concaténation, __c()__, est utilisé pour unir l'ensemble des 23 notes dans un vecteur unique auquel on a donné le nom de **Notes**.  Pour accéder à une donnée particulière, on utilisera l'indice de cette donnée, ou sa position dans la séquence. Par exemple, la 10ième donnée est:

```{r 01-Chap1-35}
Notes[10]
```
Et les cinq premières données sont:

```{r 01-Chap1-36}
Notes[1:5]
```
Il est possible de modifier une donnée spécifique faisant partie d'un objet. Par exemple:

```{r 01-Chap1-37}
Notes[10] <- 37
Notes
```


On rencontrera souvent des ensembles de données comprenant plusieurs variables organisées en colonnes, avec plusieurs observations (rangées) pour ces variables.

Supposons l'ensemble de données suivant:

```{r 01-Chap1-38, echo=FALSE,results="asis"}
library(knitr)
A <- c(7,5,2,6,4,9,5)
B <- c(7,9,5,4,1,5,5)
C <- c(9,6,9,7,2,7,3)
D <- c(4,8,7,7,1,3,8)
E <- c(6,3,8,2,6,3,7)
dt=data.frame(A,B,C,D,E)
kable(dt,caption='__Tableau de données__')
```

On y trouve 5 variables (A - E) et 7 observations pour chacune d'elles. Ces données peuvent prendre la forme d'un objet appelé __data.frame()__ sous R. Un data.frame contient une collection de variables pouvant être de différents types, et de tailles égales, comme c'est le cas ici. Au départ, les 5 variables sont 5 objets distincts:
```{r 01-Chap1-39}
A <- c(7,5,2,6,4,9,5)
B <- c(7,9,5,4,1,5,5)
C <- c(9,6,9,7,2,7,3)
D <- c(4,8,7,7,1,3,8)
E <- c(6,3,8,2,6,3,7)


```
On peut alors former:

```{r 01-Chap1-40}
X <- data.frame(A,B,C,D,E)
X
```
Par la suite, on peut extraire une ou l'autre des variables en utilisant le __nom_du_dataframe$nom_de_la_variable__. Par exemple:

```{r 01-Chap1-41}
X$A
X$B
X$C
```
Il est également possible d'opérer sur l'ensemble des éléments que contient le data.frame:

```{r 01-Chap1-42}
X**2
```
#### Lecture / Écriture de fichiers de donnes
Généralement, l'ensemble des données à traiter est trop volumineux pour permettre une simple concaténation et une entrée manuelle. On devra très souvent lire des données que se trouvent dans des fichiers externes à l'environnement R. On voudra importer des fichiers produits sous Excel, des fichiers textes, ou provenant de d'autres logiciels. Pour illustrer la procédure, générons un ensemble de données comprenant 50 observations pour 5 variables (V1-V5), et sauvegardons l'ensemble dans un fichier appelé **data1.txt**:

```{r}
x <- matrix(rbinom(250,10,0.5),nrow=50, byrow=TRUE) # Création d'une matrice 50x5, données aléatoires
head(x)
write.table(x,file="data1.txt")
```
La fonction **write.table()** effectue la sauvegarde des données dans le répertoire de travail. Par la suite, ces données peuvent être ré-introduites dans l'environnement de travail à l'aide de la fonction **read.table()**:

```{r}
y <- read.table(file="data1.txt")
head(y)
```

Cette commande importera l'ensemble des données contenues dans data1.txt dans un data.frame nommé y, en incluant le nom des variables s'il y a lieu. 

La fonction '__head()__' permet d'afficher les premières lignes de données, alors que la fonction '__tail()__' en affiche les dernières lignes:

```{r 01-Chap1-45}
tail(X,n=5)
```

#### Éditeur de données

Un éditeur de données prenant la forme d'un chiffrier est disponible pour entrer manuellement un ensemble de données. On accède à ce chiffrier en créant un data.frame vide, et en l'ouvrant avec la fonction __fix()__:
```{r 01-Chap1-46, eval=FALSE}
NouveauX <- data.frame()
fix(NouveauX)
```
On peut alors modifier le nom des variables (première ligne), et entrer les données comme on le ferait dans Excel, par exemple:

![Editeur de Données](./images/editeur.png)

De fait, on peut utiliser la fonction __fix()__ pour éditer toute variable. Essayez d'éditer la variable X définie plus haut, en lançant la commande: 
```{r 01-Chap1-47, eval=FALSE}
fix(X)
```
![Editeur de Données](./images/editeur2.png)
et 

```{r 01-Chap1-48, eval=FALSE}
fix(Notes)
```
![Editeur de Données](./images/editeur3.png)

#### Génération de nombres aléatoires

Finalement, il est souvent approprié de générer des nombres aléatoires. R offre plusieurs fonctions pour ce faire. Une seule sera présentée à ce stade, permettant de générer n données aléatoires comprises entre deux valeurs (minimum et maximum). Ainsi, pour obtenir 20 données comprises entre 1 et 10:

```{r 01-Chap1-49}
n <- 20
minimum <- 1
maximum <- 10
z <- runif(n,minimum,maximum)
z
```
ou simplement:
```{r 01-Chap1-50}
z <- runif(20,1,10)
z
```
On obtient de nouvelles données à chaque exécution de cette commande.

#### Simulation de données

En exécutant le code suivant, on peut générer *n* observations d'une population de forme rectangulaire dont les limites sont choisies au hasard à l'intérieur d'une étendue définie. De cette manière, on ne peut connaître à l'avance les caractéristiques de la population. Donc, d'une population donnée, on tire un échantillon de *n* observations dont les valeurs sont comprises entre deux valeurs indéterminées, et on obtient quelques indices calculés sur ces observations, de même qu'un graphique. Ce type de génération de données sera très utile dans la suite du cours. 

```{r 01-Chap1-51}
n <- 50  # nombre d'observations à générer
a <- c(10,20)  # étendue de la limite inférieure (population)
b <- c(25,50)  # étendue de la limite supérieure (population)
liminf <- runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup <- runif(1,b[1],b[2]) # limite supérieure (Pop)
x <- runif(n,liminf,limsup)
x
summary(x)
hist(x)
```

Le code suivant poursuit le même objectif, sauf que la distribution de la population est supposée normale, avec une certaine moyenne et un certain écart-type, qui sont sélectionnés au hasard à l'intérieur d'étendues données.Ces paramètres sont donc indéterminés. 

```{r 01-Chap1-52}
n <- 50      # nombre d'observations à générer
moy <- 100   # Moyenne (population)
et <- 15     # Écart-Type (population)
sem <- et/sqrt(n)
moypop <- runif(1,moy-1.96*sem,moy+1.96*sem)
etpop <- runif(1,et-et/3,et+et/3)
x <- rnorm(n,moypop,etpop)
x
summary(x)
hist(x)
```

Dans les unités suivantes, la génération de tels nombres aléatoires nous sera très utile.

## Création de fonctions

Dans les paragraphes précédents, plusieurs fonctions faisant partie de l'environnement R de base ont été présentées. Cet environnement contient une foule de fonctions disponibles sans qu'il soit nécessaire de charger une librairie. On peut obtenir une liste de ces fonctions en lançant la commande:

```{r eval=FALSE}
library(help = "base")
```
S'ajoutent à ces fonctions de base un grand nombre de librairies de fonctions permettant la production de graphiques, par exemple. Sous RStudio, une liste de ces librairies est disponible sous l'onglet **Packages** du panneau **Files/Plots/Packages...**, dans la section libellée **System Library**:

![Librairies chargées automatiquement](./images/Packages.png)
Malgré l'ampleur des fonctions disponibles dans l'environnement R de base, il est souvent utile, ou même nécessaire, de pouvoir définir des fonctions permettant des opérations répétitives. Le processus est simple:  
1. Choisir un nom descriptif pour la fonction
2. Définir le ou les arguments que l'on veut soumettre au traitement
3. Écrire le code approprié pour la tâche à exécuter
4. Faire en sorte que le résultat soit accessible au sortir de la fonction.  
  
L'exemple suivant illustre la création d'une fonction et son utilisation. Supposons qu'il vous soit nécessaire de calculer la ou les racines d'une fonction quadratique. Vous vous rappelez que pour une fonction du second degré, telle que $y = ax^2+bx+c$, y est nulle pour les valeurs de x données par:

$$x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$$
Créons une fonction nommée **racquad()**, dont les arguments seront le coefficient quadratique **a**, le coefficient linéaire **b** et la constante **c**. La syntaxe appropriée serait:

```{r}
racquad = function(a,b,c){
  # Calcul de la quantité sous la racine carrée
  num <- b^2-4*a*c 
  # Valeur négative? ==> Nombre complexe
  if(num<0){num = as.complex(num)}
  # Calcul des racines de la fonction et sortie
  r <- (-b+c(1,-1)*sqrt(num))/(2*a)
return(r)
}
```
La fonction **as.complex()** utilisée dans l'énoncé conditionnel **if()** fait de **num** un nombre complexe dans l'éventualité que la quantité sous la racine carrée est négative. 

Vérifions notre travail en obtenant les racines de la fonction

$$y = 2.7x^2-6.3x+3.2$$

```{r}
a <- 2.7
b <- -6.3
c <- 3.2
rac = racquad(a,b,c)
cat("Les racines de cette fonction sont: ",rac)
```
> Notez qu'une fonction de ce type peut ne pas avoir de racines, auquel cas R rapportera **NA** comme réponse. 

Quelles sont les racines, si elles existent, de $y = 5.7x^2-8.3x+12.2$:

```{r}
a <- 5.7
b <- -8.3
c <- 12.2
cat("Les racines de cette fonction sont: ",racquad(a,b,c))
```
Les racines de cette fonction sont des nombres imaginaires.

La définition de fonctions devient vite une activité routinière pour tout analyste utilisant l'environnement R. Prenez note de la syntaxe générale et mettez-la en application dès qu'un ensemble d'opérations à effectuer sur des données est répétitif. Pour plus de détails, l'article suivant pourra vous être utile: [Fonctions en R](https://stt4230.rbind.io/programmation/fonctions_r/)

## L'ajout de LIBRAIRIES ('PACKAGES')

Une librairie ('package') est un ensemble d'objets (données, fonctions) produit par des individus et mis à la disposition de la communauté. Ces librairies sont ajoutées à l'environnemnt R selon les besoins de l'analyste. Il existe plusieurs milliers de telles librairies, librement disponibles. Il est donc important d'être en mesure de charger une librairie dans notre espace de travail. Sous RStudion, cette tâche devient vite triviale.

### Librairies provenant de dépôts (CRAN)

Le premier item apparaissant sous l'onglet 'TOOLS' du menu principal de RStudio est: '**INSTALL PACKAGES...**':

![Menu TOOLS](./images/Tools_install_package.png)

Cette option produit la fenêtre suivante:

![Install Packages](./images/Install_Packages.png)

Supposons que l'on ait besoin d'une librairie portant le nom 'Desctools', contenant plusieurs fonctions susceptibles d'être utiles pour l'analyse descriptive d'ensembles de données: 

![Installation de la librairie DescTools](./images/Install_DescTools.png)

En cliquant '**OK**', le processus se poursuit dans la console. De manière alternative, on peut simplement lancer la commande suivante dans la console:

**install.packages("DescTools")**

Une librairie demeure en permanence en dormance dans votre système. Lorsque vous en avez besoin, il suffit de la ramener dans l'espace de travail avec la fonction 'library()':

```{r 01-Chap1-53}
library(DescTools)
```
Une fois chargée dans l'espace de travail, l'ensemble des objets que la librairie contient devient disponible pour une utilisation immédiate. Par exemple:

```{r 01-Chap1-54}
library(DescTools)
x <- rnorm(1000)
PlotFdist(x)
```  

Ici, la fonction **PlotFdist()** n'est qu'une des fonctions mises à votre disposition dans la librairie DescTools.

### Chargement de librairies à partir de GITHUB

Certaines librairies ne sont pas accessibles de la manière décrite ci-dessus, par contre. C'est le cas d'une librairie créée spécifiquement pour ce cours, contenant l'ensemble des fonctions qui seront utilisées pour illustrer les concepts qui seront à l'étude. Pour la charger dans l'environnement de travail, suivez les étapes suivantes:

1. Installez la librairie **devtools**
2. Chargez cette librairie dans votre environnement
3. Installez la librairie **isteahMAT104**, à partir de GitHub
4. Chargez cette librairie dans votre environnement

```{r eval = FALSE}
install.packages("devtools")
library(devtools)
install_github("dco61/isteahMAT104")
library(isteahMAT104)

```
#### Chargement d'une fonction particulière

Lorsque le nom d'une fonction faisant partie d'une librairie est connu, et que l'on a besoin spécifiquement de cette fonction, on peut l'appeler sans avoir à charger la librairie entière. Il suffira d'utiliser la syntaxe suivante, appliquée à la fonction **PlotFdist()**, comme exemple:

```{r}
x <- rnorm(1000)
DescTools::PlotFdist(x)

```
De la même manière, l'installation de la librairie **isteahMAT104** est simplifiée:

```{r eval=FALSE}
# devtools::install_github(Nom_de_Compte/Nom_de_librairie)

# Exemple:

devtools::install_github("dco61/isteahMAT104")

```

#### Liste des fonctions contenues dans une librairie

Lorsqu'une librairie est chargée dans votre environnement, vous pouvez obtenir une liste des fonctions qu'elle comporte à l'aide de la fonction **ls()**, ou de la fonction **lsf.str()**, qui ajoute la liste des arguments de chaque fonction. Par exemple:

```{r}
library(isteahMAT104)
ls("package:isteahMAT104")
lsf.str("package:isteahMAT104")
```
#### Aide concernant une librairie

Vous pouvez obtenir de l'aide concernant une librairie en lançant la commande:

```{r 01-Chap1-55, eval=FALSE}
??DescTools
```
La fenêtre d'aide affiche un guide d'utilisation de la librairie.

#### Examiner le code R d'une fonction

Parfois, il est intéressant d'examiner le code R définissant une fonction particulière. On peut visualiser ce code en inscrivant simplement le nom de la fonction (sans argument, ni parenthèses) dans la console. Par exemple:

```{r}
RandNorm

```
#### Charger une librairie à l'intérieur d'une fonction

Lors du développement de fonctions, il est souvent nécessaire d'appeler d'autres fonctions provenant de librairies. On peut évidemment charger ces librairies au préalable, de sorte que les fonctions nécessaires se trouvent dans l'environnement au moment où on en a besoin. Par contre, il est facile d'oublier de charger ces librairies. La fonction **require()** permet de charger une librairie selon les besoins. Par exemple, un énoncé conditionnel tel que le suivant installera **DescTools** s'il n'est pas déjà dans votre collection de librairies, à l'intérieur d'une fonction:

```{r eval=FALSE}
if (require("DescTools")) install.packages("DescTools")
```
Pour vérifier qu'une librairie (**DescTools** par exemple) est installée, on peut lancer une commande telle que:

```{r}
"DescTools" %in% rownames(installed.packages())

```
On obtiendra **TRUE** si la librairie est présente à l'appel, et **FALSE** dans le cas contraire.


Lorsqu'approprié, on vous informera des librairies que vous devrez charger dans votre système.


### Chargement de librairies à partir de fichiers .ZIP ou .TAR.GZ

Finalement, il est également possible d'installer une librairie à partir d'un fichier local. Par exemple, le fichier **isteahMAT104_0.5.1.tar.gz** contient localement l'intégralité de la librairie **isteahMAT104**.  On peut le télécharger directement de [GITHUB](https://github.com/dco61/isteahMAT104/archive/refs/heads/main.zip), ou du site réservé pour ce cours.

Sous RStudio, il suffira d'utiliser **Tools** - **Install Packages...** - **Install from...** - **Package Archive file (.zip, .tar.gz)**, et naviguer vers le fichier contenant la librairie:

![Installation d'une librairie à partir d'un fichier local](./images/InstPack.png) 

Finalement, en supposant que le fichier se trouve dans le répertoire de travail (x:\), la fonction suivante fera un travail similaire: 

```{r eval=FALSE}
install.packages("x:/isteahMAT104.tar.gz",repos=NULL,type="source")
```

Le contenu de cette librairie sera décrit dans les chapitres suivants.

## Sauvegarde/Récupération de l'environnement de travail

L'environnement de travail contient l'ensemble des objets (variables, fonctions, structures de données) qui vous sont accessibles dans une session. Il est souvent utile de sauvegarder cet environnement de manière à pouvoir le recharger pour une session ultérieure. Cette opération est rendue possible à partir du menu principal de RStudio:

![Sauvegarde de l'Environnement de travail](./images/SavEnv.png)
  
On peut également lancer la commande suivante dans la console, avec le même effet:
```{r eval=FALSE}
save.image("fichier_sauvegarde.RData")
```
en remplaçant **fichier_sauvegarde** par le chemin et le nom du fichier dans lequel la sauvegarde doit se faire.

Lorsque nécessaire, on peut récupérer l'environnement de travail en utilisant:

![Sauvegarde de l'Environnement de travail](./images/LoadEnv.png)
  
et en naviguant jusqu'au fichier préalablement sauvegardé. On peut également obtenir le même résultat en lançant la commande:

```{r eval=FALSE}
load("fichier_sauvegarde.RData")
```
Encore une fois, on remplacera **fichier_sauvegarde** par le chemin et le nom du fichier contenant la sauvegarde à récupérer.
  
## Conclusion

Dans cette section, nous avons abordé les éléments fondamentaux de l'environnement R. Les opérations arithmétiques élémentaires ont été décrites, de même que quelques stratégies d'entrée et d'édition de données.  Nous avons introduit RStudio avec lequel nous travaillerons de préférece, en raison de la versatilité que cet interface offre. L'ensemble constitue une boîte à outils qui deviendra rapidement indispensable dans notre étude des probabilités et des statistiques. Les compétences qui vous développerez avec ce logiciel vous seront très utiles pour l'ensemble de vos études dans le programme de Sciences de l'ISTEAH. Prenez le temps de revoir ce matériel, manipulez des variables, appliquez les fonctions qui ont été présentées à diverses situations auxquelles vous pourriez penser. Plus vous vous immergerez dans cet 'univers', plus vous deviendrez confiant(e)s et efficaces dans vos démarches quantitatives.

## Exercice 5

1. Créez 4 vecteurs suivants:
  a. nom : "Julien"   "Mathieu"  "Judith"   "Myrlande" "Harold" 
  b. sexe: "Homme"    "Homme"    "Femme"    "Femme"    "Homme"
  c. age:   23        22         25         26         32
  d. ssn: "134-34-2345" "349-44-789"  "556-34-443"  "898-98-987"  "679-67-676" 

2. Déterminez le type de chacune des variables
3. Générez une séquence des valeurs de 1 à 5, sauvegardant le tout dans la variable 'subj'
4. Formez une 'data.frame' regroupant les 6 variables que vous venez de créer. Placez la variable "subj" dans la première colonne.
5. Utilisez la fonction fix() pour éditer votre travail, et ajoutez une nouvelle ligne contenant:  6, "Martin", "Homme", 28, "453-64-822"
6. Ajoutez une nouvelle variable nommée 'education', de nature numérique, et contenant les valeurs: 12, 14, 11, 16, 13, 15
7. Fermez l'éditeur pour revenir à la console. Examinez le résultat en inscrivant le nom de votre 'data.frame'.
8. Calculez la somme de la variable 'Education', puis celle de la variable 'Age'
9. Affichez les noms par ordre alphabétique
10. En utilisant le code permettant de générer des données aléatoires provenant d'une population rectangulaire, répété ci-dessous:


```{r 01-Chap1-56, eval=FALSE}
n <- 50  # nombre d'observations à générer
a <- c(10,20)  # étendue de la limite inférieure (population)
b <- c(25,50)  # étendue de la limite supérieure (population)
liminf <- runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup <- runif(1,b[1],b[2]) # limite supérieure (Pop)
x <- runif(n,liminf,limsup)

```

générez un ensemble de 100 données de telle sorte que la plus petite valeur est comprise entre 50 et 65, et la plus grande valeur est comprise entre 100 et 125. (Note: prenez soin d'éditer le code R pour y introduire ces valeurs!).

11. Quelles sont les valeurs minimum et maximum de cet ensemble de données?
12. Combien de données sont supérieures à 90?
13. Combien de données sont inférieures à 80?  
14. Tracez un histogramme représentant la distribution de ces données.
15. Une librairie pourrait nous être utile dans les discussions futures: '**resample**'. Utilisez **TOOLS > INSTALL PACKAGES...** pour installer cette librairie sur votre ordinateur.
16. Lorsque l'installation de la librairie sera complétée, chargez-la dans votre espace de travail:

```{r 01-Chap1-57}
library(resample)
```

et exécutez la commande suivante pour en vérifier le bon fonctionnement:

```{r 01-Chap1-58}
bootC <- bootstrap(x, mean, seed = 0)
print(bootC)
hist(bootC)
qqnorm(bootC)
quantile(bootC, probs = c(.25, .975))
CI.percentile(bootC)
```

Si des graphiques et des items étiquettés **Summary Statistics** apparaissent, l'installation s'est déroulée correctement, et de nouvelles fonctions ont été ajoutées à votre système.


## Solutionnaire

### Exercice 1

> 1. Au marché, vous achetez pour 200 gourdes de pommes de terre, pour 327 gourdes de raisins, pour 178 gourdes d’ananas et pour 1238 gourdes de poisson. Utilisez la console R pour calculer le coût total de vos achats.

```{r}
200+327+178+1238
```
> 2. Arrivé à la caisse, vous vous rendez compte que qu’un des ananas est gâté et vous décidez de le retirer de votre achat, ce qui réduit le coût de 53 gourdes. Utilisez R pour calculer la somme qu’il vous reste à payer.

```{r}
200+327+178+1238 - 53
```
> 3. Pour que tout soit bien clair dans la transaction que vous effectuez au marché, lancez les commandes suivantes dans la console:

```{r}
pomme.de.terre <- 200
raisins <- 327
ananas <- 178 - 53
poisson <- 1238

a <- 'Le montant total à payer ce matin est: '
montant <- pomme.de.terre + raisins + ananas + poisson
cat(a,montant,' gourdes')
```

> 4. Les prix de certaines denrées ont changé depuis votre dernière visite au marché. Pour les pommes de terre, il y a une augmentation de 20%, et pour le poisson, une réduction de 38 gourdes. Vous voulez acheter les mêmes denrées que la dernière fois. Obtenez le montant de votre facture,,,

```{r}
pomme.de.terre <- 200*1.2 # 20% d'augmentation
raisins <- 327
ananas <- 178 - 53
poisson <- 1238 - 38

a <- 'Le montant total à payer ce matin est: '
montant <- pomme.de.terre + raisins + ananas + poisson
cat(a,montant,' gourdes')

```
### Exercice 2

> 1. La largeur d'un rectangle mesure 4 centimètres, et sa hauteur mesure 18 centimètres. Quelle est la périphérie de ce rectangle?

```{r}
# La périphérie d'un rectangle est 2 fois sa largeur + sa hauteur
2*(4 + 18) 
```

> 2. Quelle est la surface du rectangle décrit en (1)?

```{r}
# La surface d'un rectangle est sa largeur x sa hauteur
4*18

```

> 3. Dans un mètre, il y a 39.37008 pouces. Combien y'a-t'il de pouces dans 2.5 mètres?

```{r}
2.5 * 39.37008
```

> 4. Le volume d'une sphère est égal à $(4\pi r^2)/3$. Quel serait le volume d'une sphère dont le **diamètre** serait égal à 8.92745?

```{r}
# Le rayon est la moitié du diamètre. Donc...
rayon <- 8.92745/2
cat("Le volume de la sphère = ",(4*pi*rayon^2)/3)
```
> 5. Si x = 4.5327 et y = 8.4893, quelle est la valeur de l'expression:

$$\sqrt{(x^3+y^4)}-(x/2+x\cdot y/5)^3 $$ 
```{r}

x <- 4.5327 
y <- 8.4893
sqrt(x^3+y^4)-(x/2+x*y/5)^3
```

### Exercice 3

> 1. Obtenez la liste des variables qui ont été créées dans votre espace de travail: fonction ls()

```{r}
ls()
```

> 2. Nous n’avons plus besoin de ces variables. Effacez 3 des variables qui se trouvent dans votre espace de travail: fonction rm()

```{r}
rm(rayon,x,y)

```

> 3. Pour vous faciliter la tâche, effacez toutes les variables en utilisant rm(list=ls()). 

```{r}
rm(list=ls())
```

> 4. Il y a 6.28981077 $m^3$ de pétrole dans un baril. Définissez une variable contenant ce facteur de conversion.

```{r}
baril.m3 <- 6.28981077
```
> 5. Un baril de pétrole vaut $60. Vous avez besoin de 25 $m^3$ de pétrole pour vos opérations hebdomadaires. Quel sera le montant de votre facture de pétrole?

```{r}
cat("Facture hebdomadaire = ", 25/baril.m3*60)
```

### Exercice 4

> 1. Exécutez les commandes suivantes pour générer deux vecteurs composés de 20 nombres entiers compris entre 1 et 10. Ces données seront sauvegardés dans les variables x et y respectivement:

```{r}
x <- runif(20,1,10)
y <- runif(20,1,10)
```
> 2. Dans la variable x, quelles sont les valeurs supérieures à 5?

```{r}
cat("Les indices des valeurs de x > 5 sont: ",which(x>5),"\n")  

cat("Leurs valeurs sont: ",x[which(x>5)])
```
> 3. Dans la variable x, quelle est la somme des valeurs supérieures à 5?

```{r}
cat("La somme de ces valeurs est : ",sum(x[which(x>5)]))
```

> 4. Quelle est la valeur de $\sum_{i = 1}^{20}{x_i^2}$ ?

```{r}
cat("La somme des carrés de x = ",sum(x^2))
```

> 5. Définissez un vecteur nommé $y$ contenant la séquence des valeurs de 1 à 20.

```{r}
y <- 1:20
```

> 6. Calculez la somme de x et du vecteur que vous venez de définir.

```{r}
x + y
```

> 7. Déterminez l’étendue de la variable x.

```{r}
range(x)
```

> 8. Calculez la somme des produits xy.

```{r}
sum(x*y)
```

> 9. Déterminez le vecteur des valeurs les plus élevées de x et de y.

```{r}
pmax(x,y)

```
> 10. Déterminez le vecteur des valeurs les plus petites de x et de y.

```{r}
pmin(x,y)

```

### Exercice 5

> 1. Créez 4 vecteurs suivants:
  a. nom : "Julien"   "Mathieu"  "Judith"   "Myrlande" "Harold" 
  b. sexe: "Homme"    "Homme"    "Femme"    "Femme"    "Homme"
  c. age:   23        22         25         26         32
  d. ssn: "134-34-2345" "349-44-789"  "556-34-443"  "898-98-987"  "679-67-676" 
  
```{r}
nom <- c("Julien","Mathieu","Judith","Myrlande","Harold")
sexe <- c("Homme","Homme","Femme","Femme","Homme")
age <- c(23,22,25,26,32)
ssn <- c("134-34-2345", "349-44-789",  "556-34-443",  "898-98-987",  "679-67-676")
```

> 2. Déterminez le type de chacune des variables

```{r}
typeof(nom)
typeof(sexe)
typeof(age)
typeof(ssn)
```

> 3. Générez une séquence des valeurs de 1 à 5, sauvegardant le tout dans la variable 'subj'

```{r}
subj <- 1:5
```

> 4. Formez une 'data.frame' regroupant les 6 variables que vous venez de créer. Placez la variable "subj" dans la première colonne.

```{r}
df <- data.frame(subj,nom,sexe,age,ssn)
```

> 5. Utilisez la fonction fix() pour éditer votre travail, et ajoutez une nouvelle ligne contenant:  6, "Martin", "Homme", 28, "453-64-822"

```{r eval=FALSE}
fix(df)
```

> 6. Ajoutez une nouvelle variable nommée 'education', de nature numérique, et contenant les valeurs: 12, 14, 11, 16, 13, 15

```{r eval=FALSE}
fix(df)
```

> 7. Fermez l'éditeur pour revenir à la console. Examinez le résultat en inscrivant le nom de votre 'data.frame'.

```{r}
df
```

> 8. Calculez la somme de la variable 'Education', puis celle de la variable 'Age'

```{r}
cat("Somme de Education = ",sum(df$education))
cat("Somme de Age = ",sum(df$age))
```

> 9. Affichez les noms par ordre alphabétique

```{r}
sort(df$nom)
```

> 10. En utilisant le code permettant de générer des données aléatoires provenant d'une population rectangulaire, répété ci-dessous:


```{r eval=FALSE}
n <- 50  # nombre d'observations à générer
a <- c(10,20)  # étendue de la limite inférieure (population)
b <- c(25,50)  # étendue de la limite supérieure (population)
liminf <- runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup <- runif(1,b[1],b[2]) # limite supérieure (Pop)
x <- runif(n,liminf,limsup)
x
summary(x)
hist(x)
```

>générez un ensemble de 100 données de telle sorte que la plus petite valeur est comprise entre 50 et 65, et la plus grande valeur est comprise entre 100 et 125. (Note: prenez soin d'éditer le code R pour y introduire ces valeurs!).

```{r}
n <- 100  # nombre d'observations à générer
a <- c(50,65)  # étendue de la limite inférieure (population)
b <- c(100,125)  # étendue de la limite supérieure (population)
liminf <- runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup <- runif(1,b[1],b[2]) # limite supérieure (Pop)
x <- runif(n,liminf,limsup)

```

> 11. Quelles sont les valeurs minimum et maximum de cet ensemble de données?

```{r}
cat("Valeur minimum = ",min(x))
cat("Valeur maximum = ",max(x))
```

> 12. Combien de données sont supérieures à 90?

```{r}
cat(sum(x>90)," valeurs de x sont supérieures à 90")

```

> 13. Combien de données sont inférieures à 80?

```{r}
cat(sum(x>90)," valeurs de x sont supérieures à 90")

```

> 14. Tracez un histogramme représentant la distribution de ces données.

```{r}
hist(x)
```

## Références

**Manuels**:

E. Gallic, https://egallic.fr/Enseignement/R/Book/avant-propos.html

O. Decourt, Le langage R au Quotidien, Dunod, Paris, 2018

P. Lafaye de Micheaux, R. Drouilhet & B. Liquet, Le logiciel R, Seconde Édition, Springer, Paris, 2014

V. Goulet, Introduction à la programmation en R: https://cran.r-project.org/doc/contrib/Goulet_introduction_programmation_R.pdf

W. N. Venables & D. M. Smith, An Introduction to R, R Core Team, 2021

**Sites WEB**:

R pour Scientifique:
https://stt4230.rbind.io/ 

R for Data Science: https://r4ds.had.co.nz/introduction.html 

Learn R: https://www.codecademy.com/learn/learn-r 

R Basics, Quick and Easy: http://www.sthda.com/english/wiki/r-basics-quick-and-easy

R in 2 Hours (Youtube): https://www.youtube.com/watch?v=_V8eKsto3Ug

R programming tutorial for beginners: https://intellipaat.com/blog/tutorial/r-programming/introduction/

R tutorial for beginners: https://www.statmethods.net/r-tutorial/index.html

STAT545: Data wrangling, exploration, and analysis with R
https://stat545.com/index.html

<!--chapter:end:01-Chap1.Rmd-->

---
title: "MAT104 - Cours 2: Introduction aux Probabilités"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
---

```{r 02-Chap2-1, include = FALSE}
knit_print.data.frame = function(x, ...) {
  res = paste(c("", "", knitr::kable(x)), collapse = "\n")
  knitr::asis_output(res)
}

registerS3method(
  "knit_print", "data.frame", knit_print.data.frame,
  envir = asNamespace("knitr")
)
```

```{r 02-Chap2-2, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```
# Introductions aux Probabilités

## Objectifs d'apprentissage

À la fin de cette section, vous devriez être en mesure de:

1.  Définir la notion de probabilité et faire la distinction entre
    probabilité théorique et probabilité fréquentiste
2.  Spécifier l'espace échantillonnal et sa taille
3.  Spécifier un événement et la fréquence de son occurrence
4.  Définir une permutation et en calculer le nombre
5.  Définir une combinaison et en calculer le nombre
6.  Utiliser correctement la règle d'addition de probabilités
7.  Utiliser correctement la règle de multiplication de probabilités
8.  Utiliser un tableau de contingence pour calculer les probabilités
    marginales, jointes et conditionnelles
9.  Faire la lecture d'un arbre de probabilités
10. Construire un tableau présentant une distribution de fréquences
11. Simuler différentes expériences aléatoires:

-   Lancer d'une pièce de monnaie
-   Lancer de dés (1 et 2)
-   Sélection au hasard d'objets dans une urne
-   Sélection de cartes dans un jeu standard

## Probabilités: Définitions de base

Dans cette section,nous abordons les notions pertinentes au calcul de
probabilités dans diverses situations. Une compréhension ferme de ces
notions est essentielle à l'étude des méthodes d'analyse inférentielles.

Pour cette section, nous utiliserons les librairie suivantes:

```{r 02-Chap2-3,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randtests,ggvenn,DiagrammeR,arrangements,dplyr,tidyr,formatR,kableExtra,isteahMAT104,openintro)
```

```{r 02-Chap2-4,message=FALSE}
library(randtests)
library(ggvenn)
library(arrangements)
library(dplyr)
library(tidyr)
library(formatR)
library(kableExtra)
library(isteahMAT104)
library(openintro)
```

### Expérience aléatoire**,** Événement**, et** Espace Échantillonnal

Afin d'illustrer les notions présentées dans les paragraphes qui
suivent, considérons des situations (ou épreuves) très simples: le
lancer d'une ou de plusieurs pièces de monnaie, de lancers de dés, ou
tirer quelques cartes d'un jeu de cartes standard.

Le lancer d'une pièce de monnaie est une expérience aléatoire qui peut
se traduire par l'un ou l'autre de deux résultats ou **événements**
possibles: la pièce tombe du côté **PILE**, ou elle tombe du côté
**FACE**. L'ensemble des événements possibles lors d'une expérience
aléatoire constitue l'espace échantillonnal. Formellement:

-   **Expérience aléatoire**: épreuve donnant lieu à un événement dont
    la nature est le produit du hasard. On réfère ici à toute action se
    traduisant par un résutat ou événement: lancer d'une pièce, d'un dé,
    administrer un test, etc. De fait, toute action conduisant à des
    observations qui sont le fruit du hasard peut être considérée comme
    étant une expérience aléatoire.

-   **Espace échantillonnal**: ensemble exhaustif des résultats
    possibles d'une expérience aléatoire. Dans le cas du lancer d'une
    pièce de monnaie, on représentera l'espace échantillonnal par :
    **S={Pile, Face}**. Dans le cas du lancer d'un dé, on aurait:
    **S={1, 2, 3, 4, 5, 6}**. Ces espaces échantillonnaux se composent
    d'événements discrets: on peut dénombrer les éléments qu'ils
    contiennent. Dans d'autres cas, l'espace échantillonnal est composé
    d'événements continus dont le nombre est infini. Par exemple: **S =
    {**$-\infty$,...,+$\infty$}

-   **Événement**: le résultat d'une expérience aléatoire. Il s'agit
    d'un élément particulier faisant partie de l'espace échantillonnal,
    donc un résultat parmi l'ensemble des résultats possibles. Étant le
    fruit du hasard, on ne peut pas déterminer à l'avance l'événement
    qui se produira. Par contre, on peut calculer la probabilité qu'un
    événement particulier se produise: **P(E) = ?**

    -   **Événements indépendants**: Deux événements sont dits
        **indépendants** si l'occurrence de l'un n'affecte en rien la
        probabilité d'occurrence de l'autre. Par exemple, en lançant une
        pièce de monnaie deux fois, la probabilité d'obtenir FACE au
        premier lancer n'a aucun impact sur la probabilité qu'au second
        lancer, on obtienne le même résultat. De même, en lançant deux
        dés, la probabilité de voir apparaître un '6' sur le premier dé
        n'a aucun impact sur ce qu'on observera sur le second. De la
        même façon, les résultats d'une loterie sont indépendants d'un
        tirage à l'autre: ce n'est pas parce que vous n'avez jamais
        gagné quoi que ce soit à la loterie nationale, après y avoir
        participé chaque semaine pendant 20 ans que vos chances de
        gagner 'bientôt' sont plus grandes! Dans la vie courante, de
        nombreuses superstitions sont basées sur le fait de nier
        l'indépendance entre des événements réellement indépendants.

    -   **Événements dépendants**: Deux événements sont dits
        **dépendants** si l'occurrence de l'un affecte la probabilité
        d'occurrence de l'autre. Par exemple, le fait de tirer (sans
        remise) le Roi de Cœur d'un jeu de carte standard réduit la
        probabilité d'obtenir un autre Roi du même jeu de carte
        lorsqu'on tire une seconde carte. Lors du tirage de la première
        carte, il y avait 4 Rois dans un jeu de 52 cartes; mais lors du
        second tirage, si un Roi a été obtenu au premier tirage, il n'en
        reste plus que 3 dans le jeu qui ne contient plus que 51 cartes.
        Par contre, si la première carte (Roi de Coeur) est replacée
        dans le jeu avant de tirer la seconde carte, alors les
        événements 'obtenir un roi pour première carte' et 'obtenir un
        roi pour seconde carte' sont indépendants: dans cette expérience
        aléatoire, les mots-clés sont **sans remise**, et **avec
        remise**: une expérience aléatoire sans remise se traduit par
        des événements dépendants, alors qu'une expérience aléatoire
        avec remise résulte en événements indépendants.

    -   **Événements mutuellement exclusifs**: Deux événements sont dits
        **mutuellement exclusifs** s'ils ne peuvent survenir en même
        temps. Par exemple, en tirant une carte d'un jeu standard, les
        événements 'obtenir un Coeur' et 'obtenir une carte noire' sont
        mutuellement exclusifs, car une carte ne peut pas être 'noire'
        et 'Coeur' à la fois.

    -   **Événements non mutuellement exclusifs**: Lorsque deux
        événements ne sont pas mutuellement exclusifs, il peuvent
        survenir simultanément. Par exemple, un événement défini comme
        'un étudiant suit un cours de psychologie' et un second
        événement comme 'un étudiant suit un cours de mathématiques'
        sont deux événements qui peuvent survenir en même temps: le
        programme d'étude peut très bien inclure ces deux cours! Il
        s'agit donc d'événements non mutuellement exclusifs.

-   **Variable aléatoire**: Ce terme reviendra continuellement dans les
    discussions à venir. Une variable aléatoire est une variable dont la
    valeur est le résultat d'une expérience aléatoire, et en
    conséquence, le fruit du hasard. Elle est donc définie sur
    l'ensemble des résultats possibles d'une expérience aléatoire.
    Certaines variables aléatoires sont discrètes, d'autre continues.
    Toutes les variables utilisées dans le cadre d'une recherche
    scientifique sont des variables aléatoires! En calculant la
    probabilité de chaque valeur possible d'une variable aléatoire, on
    obtient une distribution de probabilités. Selon la nature de
    l'expérience aléatoire et des événements qu'elle génère, on obtient
    différentes distributions de probabilités.

## Éléments de calcul de probabilités

En supposant une pièce de monnaie qui est bien équilibrée (non truquée),
on a une chance sur 2 que l'événement observé lors d'un lancer de cette
pièce soit PILE, et une chance sur 2 qu'il soit FACE. On peut donc
exprimer cette situation par: $$P(PILE) = 1/2$$ $$P(FACE)=1/2$$

Ces expressions nous informent que la **probabilité** d'obtenir FACE en
lançant une pièce de monnaie est égale à 0.5; et que la probabilité
d'obtenir PILE en lançant cette pièce est aussi égale à 0.5. Cette
définition est conforme à l'idée qu'une probabilité est le rapport entre
le **nombre d'occurrence d'un événement donné (nE)** et le **nombre
total d'événements possibles (nS)**: $$P(E)=nE/nS$$ Ce qui est implicite
dans cette définition, c'est que pour pouvoir déterminer la probabilité
d'un événement, il faut être capable de compter le nombre de 'succès'
(succès = l'événement se produit), de même que le nombre d'événements
possibles (l'ensemble des événements qui pourraient se produire lors
d'une expérience aléatoire). Ces deux nombres sont toujours au coeur
d'un exercice de calcul de probabilités, quelle que soit sa complexité.
Dans la section suivante, nous présentons les éléments pertinents à
l'obtention de ces nombres.

### Calcul de **nE** et de **nS**

Pour pouvoir calculer une probabilité, il est nécessaire de
comptabiliser les **nE** 'succès' et les **nS** événements possibles. Souvent,
il suffit d'un peu de réflexion pour y parvenir, ou d'un peu d'aide
informatique!.

On sait qu'en lançant une pièce de monnaie, deux résultats sont
possibles: PILE, ou FACE. Donc dans ce cas, l'espace échantillonnal est
**S = {PILE, FACE}**, **nS = 2**, et le résultat d'un lancer étant soit
PILE, soit FACE, **nE = 1** dans chacun des cas.

Si deux pièces de monnaie sont lancées, l'espace échantillonnal
s'agrandit: **S = {PP,PF,FP,FF}: nS = 4** Dans ce cas, l'événement peut
prendre différentes formes: obtenir 2 FACEs, ou 2 PILEs, ou une
combinaison PILE/FACE ou FACE/PILE. Si l'événement étudié est PILE/PILE,
alors nE = 1. Si on recherche une combinaison des deux résultats
possibles, alors nE = 2.

Quel serait l'espace échantillonnal lorsqu'on tire une carte d'un jeu
standard? Un jeu standard contient 52 cartes, et cet ensemble forme
l'espace échantillonnal: pour chacun de 'Pique', 'Coeur', 'Carreau',
'Treffle', on a l'As, 2, 3, 4, 5, 6, 7, 8, 9, 10, Valet, Dame, Roi. Donc
13 cartes par série, pour un total de nS = 52. Si ce que l'on recherche
en tirant une carte est la couleur 'Rouge' (Coeur et Carreau), alors nE
= 26.

L'espace échantillonnal correspondant au lancer d'un dé est: **S = {1,
2, 3, 4, 5, 6}**. Donc nS = 6. Si l'événement recherché est une valeur
supérieure à 4, alors nE = 2... Si deux dés sont lancés et que l'on
considère la somme des deux observations, alors l'espace échantillonnale
se compose de 36 événements possibles:

```{r 02-Chap2-5, echo=FALSE}
row1<-c(2,3,4,5,6,7)
row2<-c(3,4,5,6,7,8)
row3<-c(4,5,6,7,8,9)
row4<-c(5,6,7,8,9,10)
row5<-c(6,7,8,9,10,11)
row6<-c(7,8,9,10,11,12)
matrix(c(row1,row2,row3,row4,row5,row6),nrow=6)
```

Tous les exemples précédents impliquent des situations simples où les événements possibles sont discrets. L'espace échantillonnal est fini. Dans beaucoup d'autres situations, les événements impliquent des valeurs **continues**, et l'espace échantillonnal est **INFINI**! Par exemple, si l'expérience aléatoire consiste pour un individu à effectuer le trajet entre la maison et le bureau, même si le temps requis varie entre 15 et 30 minutes, il existe un nombre infini de valeurs possibles, en supposant qu'il soit possible de mesurer ce temps avec une précision de $1.0\times10^{-1,000,000,000,000}$ de nano-seconde! Des stratégies seront développées plus tard pour permettre l'étude de ces cas.

### Quelques exemples simples:

Une probabilité **théorique** est définie par:
$$P(E)=\frac{Nombre~d'occurrence~de~l'événement~E}{Nombre~d'éléments~composant~l'espace~Échantillonnal}=\frac{nE}{nS}$$
1. En lançant un dé, quelle est la probabilité d'obtenir un 6:
$$P(6)=\frac{nE}{nS}=\frac{1}{6}=0.17$$ 2. Dans un jeu de 52 cartes, on
tire une carte. Quelle est la probabilité d'obtenir un Cœur (E~1~) ou un
As de pique (E~2~) ?
$$P(Coeur~OU~As~de~Pique)=\frac{nE_1}{nS_1}+\frac{nE_2}{nS_2}=\frac{13}{52}+\frac{1}{52}=\frac{14}{52}=0.27$$
3. Dans un jeu de 52 cartes, on tire 2 cartes, séquentiellement, en
replaçant la première carte après avoir pris note de son contenu. Quelle
est la probabilité d'obtenir un Cœur (E~1~) et un As de pique (E~2~)?
$$P(Coeur~ET~As~de~Pique)=\frac{nE_1}{nS_1}\times \frac{nE_2}{nS_2}=\frac{13}{52}\times\frac{1}{52}=\frac{13}{2704}=0.0048$$
Notez ici le relation entre **OU**, impliquant la **somme** des
probabilités des deux événements, et **ET**, qui implique le **produit**
de ces probabilités. Nous venons de rencontrer deux règles importantes
du calcul des probabilités: la règle d'addition, et la règle de
multiplication d'événements indépendants. Nous y reviendrons sous peu.

#### Exemple 1:

Supposons que les mots de passe permettant d'accéder à un fichier
spécifique sont composés de 3 lettres majuscules. Quelle est la
probabilité qu'un mot de passe choisi au hasard ne comporte que des
lettres différentes.

##### **Solution**:

###### Par simulation:

En prenant un échantillon de trois lettres majuscules
parmi les 26 lettres disponibles en s'assurant que l'échantillonnage se fait **avec remise** pour permettre les répétitions de lettres, on peut effectuer des comparaisons logiques de chaque paire de lettres sélectionnées afin de déterminer si elles sont différentes l'une de l'autre. Si c'est le cas, on a un 'succès'. En répétant l'exercice un nombre infini de fois (ie un GRAND nombre de fois!), on obtient la probabilité recherchée. Le code R suivant permet une telle simulation. Exécutez ce code en spécifiant un nombre croissant de réplications (**nrep**). Quelle est la probabilité estimée d'obtenir 3 lettres différentes, dans ces conditions?

```{r 02-Chap2-6, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
nrep <- 100    # Nombre de réplications
ok <- 0        # Initialisation du compteur de 'succès'
for(i in 1:nrep){
  x  <-  sample(LETTERS[1:26],3,replace=TRUE)
  ok <-  ok + (sum((x[1]!=x[2]),(x[1]!=x[3]),(x[2]!=x[3]))==3)
}
 cat("Probabilité = ",ok/nrep)
```
Une manière plus compacte d'obtenir le même résultat pourrait se formuler ainsi:

```{r}
nrep = 100    # Nombre de réplications
x <- replicate(nrep,sample(LETTERS[1:26],3,replace=TRUE))
y <- as.data.frame(x)
ok <- sum(rapply(y,function(x)length(unique(x))==3))
cat("Probabilité = ",ok/nrep)
```
Dans cette dernière solution, la fonction **rapply()** permet d'obtenir l'évaluation de chaque échantillon de 3 lettres (ie détermination du nombre de lettres différentes), de manière récursive pour chaque colonne d'un dataframe (**y**) composé de 3 rangées par **nrep** colonnes. On évite ainsi l'utilisation d'une boucle, souvent plus lente.

Cette probabilité, obtenue en répétant l'expérience aléatoire un grand
nombre de fois et en établissant le rapport entre le nombre de 'succès'
(nE) et le nombre (N) de répétition de l'expérience est souvent
qualifiée de '**fréquentielle**'. Sa précision dépend du nombre de
répétition: plus N est grand, plus on obtient une valeur s'approchant de
la probabilité '**théorique**', que l'on obtient de manière analytique.
Pour $N = \infty$, ces deux probabilités sont égales. Ce fait définit la
'**Loi des Grands Nombres**': lorsque l'expérience est effectuée un très
grand nombre de fois, la probabilité fréquentielle tend à se rapprocher
de la probabilité théorique:
$$\displaystyle \lim_{N \to \infty} \frac{nE}{N}=\frac{nE}{nS}$$

###### Par calcul combinatoire:

1.  Pour obtenir trois lettres différentes dans un mot de passe, il est
    nécessaire que la sélection se fasse **sans remise**. De cette
    manière, on s'assure qu'une lettre donnée ne peut pas apparaître une
    seconde fois dans la séquence de 3 lettres. Le nombre de 'succès'
    (**nE**) s'obtient donc en considérant que 26 lettres sont
    disponibles pour la première position; une fois cette lettre
    sélectionnée, il en reste 25 pour la seconde; puis 24 pour la
    troisième. Le nombre total de mots de passe composés de trois
    lettres différentes est donc:

```{r 02-Chap2-7}
26*25*24
```

Ce résultat correspond au nombre de permutations de 26 objets (ici, des
lettres) pris 3 (longueur des mots de passe) à la fois, comme nous le
verrons plus loin.

2.  On obtient le nombre total de mots de passe composés de trois
    lettres qu'il est possible de former en procédant à une sélection
    **avec remise**. Dans ce cas, chacune des trois lettres sera l'une
    des 26 lettres de l'alphabet, et nS est égal à:

```{r 02-Chap2-8}
26*26*26
```

Finalement, la probabilité d'obtenir un mot de passe composé de trois
lettres différentes est le rapport entre ces deux quantités:
$$(26\cdot25\cdot24)/26^3$$

```{r 02-Chap2-9}
(26*25*24)/26^3
```

Cette probabilité est qualifiée de '**théorique**', par opposition à la
probabilité **fréquentielle** définie dans les paragraphes précédents.

### Exercice 1

1.  Pour chacune des situations suivantes, définissez l'expérience
    aléatoire, l'espace échantillonnal, ce que pourrait être un
    événement, et calculez nE et nS:

  -   Dans un jeu de hasard, on lance 3 pièces de monnaie et on gagne si
    on obtient 3 FACES ou 3 PILES.

  -   Dans une usine, on fabrique 12 instruments par heure. Ces 12
    instruments sont ensuite mis en boîte, à la condition qu'il n'y ait
    pas plus de 2 instruments présentant des défauts de fabrication.

  -   Dans une classe de mathématiques, 35 étudiants sont inscrits. Le
    professeur espère qu'il n'y aura pas plus de 5 étudiants qui
    échoueront le cours.

  -   En période de pandémie, un restaurant ne peut accueillir plus de 5
    clients à la fois. À l'extérieur, 15 personnes attendent pour y
    entrer et on choisit au hasard celles qui sont admises.

2.  En période de pandémie, un restaurant ne peut accueillir plus de 5
    clients à la fois. À l'extérieur, 15 personnes attendent pour y
    entrer et on choisit au hasard celles qui sont admises, et ces 15
    personnes portent toutes des noms différents. Désignons-les par les
    numéros 1 à 15.

  -   Quelle est la valeur de nE, si on veut admettre les personnes 1 à 5?
  -   Quelle est la valeur de nS dans cette situation?
  -   Quelle est la probabilité que les personnes 1 à 5 soient admises?

3.  De combien de façons différentes peut---on répartir un groupe de 7
    personnes sur une rangée de 7 chaises ?

4.  Trois garçons et deux filles prennent place sur un banc public de 5
    places:

  -   De combien de facons différentes, 3 garcons et 2 filles peuvent-ils
    prendre place sur ce banc?
  -   De combien de facons peuvent-ils s'asseoir si les garcons s'assoient
    les uns à côté des autres et s'il en est de méme pour les filles ?
  -   De combien de maniéres différentes peuvent---ils s'asseoir si
    seulement les filles s'assoient l'une a côté de l'autre?

## Permutations de n objets:

Une permutation est une séquence de *n* items prix *k* à la fois, sans
remise. C'est dire que l'ordre de la sélection est pris en compte.
Par exemple, la séquence {A,B,C} est différente de la séquence {B,A,C}
ou de {C,B,A} même si elles sont composées des mêmes éléments.

La fonction **permute()** de la librairie **isteahMAT104** permet de dresser une liste complète de toutes les permutations qu'il est possible de former à partir de *n* objets. On obtient donc une liste de l'ensemble des éléments qui se trouvent dans
l'espace échantillonnal. Pour l'utiliser, il suffit de définir un vecteur contenant les éléments à permuter. Ce vecteur est le seul argument nécessaire. Par exemple:

```{r 02-Chap2-11}
x  <-  permute(1:5)      # Permutations des nombres 1 à 5
head(x,10)
tail(x,10)
permute(letters[1:4]) # Permutations des lettres a, b, c, d
```

ATTENTION: la liste des permutations peut être TRÈS longue: avec 5
éléments, 120 permutations sont possibles; avec 6 éléments, cette liste
grimpe à 720; avec 8 éléments, on a 40,420 permutations! Je vous laisse
tenter votre chance avec des nombres plus élevés d'éléments!

Le nombre de permutations de *n* objets est égal à
$$P(n,n)=n! = n\cdot(n-1)\cdot(n-2)\cdot...\cdot1$$

par exemple: $$P(5,5)=5! = 5\cdot4\cdot3\cdot2\cdot1=120$$ Sous R, la
fonction **factorial()** permet d'obtenir cette quantité:

```{r 02-Chap2-12}
factorial(5)
```

## Permutations de n objets pris k à la fois

Il est très fréquent que l'on doive considérer le nombre de permutations
de *n* éléments, mais pris *k* à la fois, pour *k* \< *n*. La fonction
**permut** qui se trouve dans la librairie **randtests** permet
d'obtenir la liste de ces permutations. Spécifiquement, après avoir
installé cette librairie:

```{r 02-Chap2-13}

library(randtests)
permut(1:6,2)
permut(letters[1:4],2)
```
Obtenir une telle liste peut être intéressant dans certaines situations, mais pour nos besoins, l'ingrédient qui est souvent nécessaire pour les calculs de probabilité,
c'est davantage le NOMBRE de permutations, et non pas la liste de ces
dernières.
  
Formellement, le nombre de permutations de n items pris k à la fois
s'obtient par:

$$P(n,k) =  \frac{n!}{(n-k)!} = n\cdot(n-1)\cdot...\cdot(n-k+1)$$

Sous R, cette donnée peut s'obtenir simplement par la
commande suivante:

```{r 02-Chap2-14}
# perm.n.k  <-  factorial(n)/factorial(n-k)
#
# Exemple: Nombre de permutations de 6 objets pris 2 à la fois
perm.n.k  <-  factorial(6)/factorial(6-2)
perm.n.k
#
# ou
#
perm.n.k  <-  prod(6:(6-2+1))
perm.n.k
```

### Exemple 3:

Supposons que les mots de passe donnant droit à un site WEB se composent
de 6 caractères choisis parmi les 10 nombres et les 26 lettres de
l'alphabet, et que ces lettres peuvent être minuscules ou majuscules.
Dans ces conditions: 
  
a) combien de mots de passe sont possibles?

    Solution:
    1. Le nombre de caractères disponibles est 10 + 26 x 2 = 62
    2. Chaque caractère composant un mot de passe est l'une de ces 62 possibilités...
    3. Donc le nombre total de mots de passe de 6 caractères que l'on peut produire est:

$$62^6 = 62 \times 62 \times 62 \times 62 \times 62 \times 62 = 56,800,235,584$$
        
    Sous R:

```{r 02-Chap2-15}
62^6
```

b)  Quelle est la proportion des mots de passe composés de 6 caractères
    différents?

```{=html}
<!-- -->
```
    Solution:

    - Par simulation: le code suivant est approprié dans ce cas:

```{r 02-Chap2-16, eval = FALSE}
nrep  <-  1000    # Nombre de réplications
ncar  <-  6       # Nombre de caractères pour mots de passe
caract  <-  c(LETTERS[1:26],letters[1:26],as.character(0:9))
succes  <-  0
i  <-  1
repeat{
  x  <-  sample(caract,ncar,replace=TRUE)
  succes=succes+(sum(duplicated(x))==0)   # nE
  i  <-  i+1
  if(i==nrep){break}
}
 cat("Probabilité = ",succes/nrep)
```
De manière alternative (et plus compacte), on obtient un résultat similaire:

```{r}
nrep  <-  1000    # Nombre de réplications
ncar  <-  6       # Nombre de caractères pour mots de passe
caract  <-  c(LETTERS[1:26],letters[1:26],as.character(0:9))
x <- replicate(nrep,sample(caract,ncar,replace=TRUE))
y <- as.data.frame(x)
succes <- sum(rapply(y,function(x)sum(duplicated(x))==0))
cat("Probabilité = ",succes/nrep)
```
    En exécutant ce code avec nrep = 100,000, on obtient:

```{r 02-Chap2-17, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
nrep  <-  100000    # Nombre de réplications
ncar  <-  6         # Nombre de caractères pour mots de passe

caract  <-  c(LETTERS[1:26],letters[1:26],as.character(0:9))
succes  <-  0
i  <-  1
repeat{
  x  <-  sample(caract,ncar,replace=TRUE)
  succes=succes+(sum(duplicated(x))==0)   # nE
  i  <-  i+1
  if(i==nrep){break}
}
 cat("Probabilité = ",succes/nrep)
```

    - Par calcul combinatoire:
      
$$P(62,6) = 62 \times 61 \times ... \times (62 - 6 + 1) / 62^6$$

        Sous R:

```{r 02-Chap2-18}
prod(62:57)/62^6
```

### Exemple 4:

Dans une ligne d'attente se trouvent 6 personnes. En supposant qu'aucune
d'elles ne porte un nom de famille commençant par la même lettre, quelle
est la probabilité que leur position dans la ligne d'attente corresponde
à l'ordre alphabétique de leur nom de famille?

Solution:

1.  Désignons ces personnes par les numéros 1 à 6: **pers = 1:6**
2.  De combien de façon 6 personnes dont les noms de famille débutent
    par des lettres différentes peuvent-elles être alignées dans un
    ordre alphabétique? **Rep: une seule**
3.  Quel est le nombre de permutations de 6 personnes prises 6 à la
    fois? $$P(6,6)=6\cdot5\cdot4\cdot3\cdot2\cdot1=6!=720$$
4.  Donc la probabilité recherchée est: 1/720 = **0.00139**

### Exercice 2

1.  Un comité de 3 personnes doit être formé à partir d'un groupe de 12
    employés d'un grand commerce. Une des 3 personnes sélectionnées sera
    présidente du comité, une autre sera secrétaire, et l'autre agira
    comme conseillère.\

-   Combien de comités différents sont possibles?
-   En supposant que les 12 employés du commerce portent des noms
    différents, combien de comités seraient composé de 'Marie-Lune',
    'Pierre', et 'Harold'?
-   Quelle est la probabilité que ce dernier événement se produise?
-   Si 'Pierre' doit nécessairement être Président en raison de son
    ancienneté dans l'entreprise, dans combien de comités lui
    associerait-on 'Marie-Lune' et 'Harold'?
-   Quelle est la probabilité que ce dernier événement se produise?

## Combinaisons de *n* objets pris *k* à la fois

Les permutations tiennent compte de l'ordre des différents éléments. Par
exemple, une permutation formée des éléments (A, B, C) est différente de
(B, A, C).

Une combinaison est une séquence d'objets dans laquelle l'ordre **n'est
pas** pris en compte. Ainsi, les séquences {A,B,C}, {A,C,B}, {C,A,B},
{C,B,A} sont équivalentes, alors qu'elles sont des permutations
distinctes.

Le nombre de combinations de *n* objets pris *k* à la fois est donné
par:

$$C(n,k) = \frac{n!}{k!(n-k)!} = \frac{P(n,k)}{P(k,k)}$$

Par exemple, le nombre de combinaisons de 5 objets pris 3 à la fois est:

$$C(5,3) = \frac{5!}{3!(5-3)!} = \frac{P(5,3)}{P(3,3)} = 10$$

Reprenant l'exemple 1, le nombre de combinaisons des 26 lettres de
l'alphabet prises 3 à la fois est:

$$C(26,3) =     \frac{26!}{3!(26-3)!} = \frac{26\cdot(26-1)\cdot...\cdot(26-3+1)}{3\cdot2\cdot1} = 2,600$$
Notons que le nombre de permutations de *n* objets pris *k* à la fois,
$P(n,k)$, peut se définir en terme de combinaisons. En effet:
$$P(n,k)=\frac{n!}{(n-k)!} = \frac{k!n!}{k!(n-k)!}=k!C(n,k)$$
### 1.4.1 Exemple 2:

Le nombre de séquences de 6 lettres distinctes formées à partir des
lettres A-J (10 lettres) est égal à
$$C(n,k)=\frac{n!}{k!(n-k)!} = \frac{P(n,k)}{P(k,k)}=\frac{P(n,k)}{k!}$$

$$C(10,6) =\frac{10!}{6!(10-6)!} = \frac{10\cdot9\cdot8\cdot7}{4\cdot3\cdot2\cdot1}  = 210$$

Sous R, on obtient le nombre de combinaisons de *n* objets pris *k* à la
fois à l'aide de la fonction **choose()**:

```{r 02-Chap2-19}
choose(10,6)
```

et le nombre de permutations serait:

```{r 02-Chap2-20}
choose(10,6) * factorial(6)
```

### Exercice 3

1.  De combien de maniéres peut-on former un jury de 3 hommes et 2
    femmes parmi 7 hommes et 5 femmes ?

2.  À l'occasion d'un examen, un étudiant doit répondre à 8 questions
    sur un total de 10.

-   Combien de choix de questions sont possibles ?
-   Combien de choix y a-t-il s'il doit répondre aux 3 premières
    questions ?
-   Combien de choix y a---t-il s'il doit répondre au moins à 4 des 5
    premières questions ?

3.  Combien de plaques d'immatriculation de véhicules peut-on former si
    chaque plaque contient 2 lettres différentes suivies de 3 chiffres
    différents ?

## Calcul de probabilités: Quelques règles élémentaires

Il existe un petit nombre de règles élémentaires que l'on doit connaître
pour obtenir des estimations justes de probabilités. Certaines d'entre
elles ont déjà été utilisées intuitivement dans les exemples antérieurs.

### Règle d'addition - Événements mutuellement exclusifs (ou disjoints):

Lorsque deux événements sont indépendants et mutuellement exclusifs (ou
disjoints), n'ayant aucun élément en commun, la probabilité qu'un **ou**
l'autre de ces événements survienne est donnée par la somme des
probabilités de chacun d'eux. Par exemple, si on note le lieu de
résidence de 100 individus et que 30 d'entre eux sont du département du
Centre (C), 50 d'Artibonite (A) et 20 de l'Ouest (O), quelle est la
probabilité qu'une personne choisie au hasard provienne d'Artibonite ou
du Centre?
$$P(A \cup C)=P(A)+P(C)=\frac{50}{100}+\frac{30}{100}=\frac{80}{100}=0.80$$
### 1.5.2 Règle d'addition générale - Événements non-mutuellement exclusifs (ou compatibles)

Lorsque deux événements peuvent survenir simultanément, n'étant pas
mutuellement exclusifs, la règle d'addition doit être modifiée pour
tenir compte du nombre de cas faisant partie des deux événements.

Par exemple, supposons 100 étudiants de l'ISTEAH devant s'inscrire à au
moins un de deux cours: Physique et/ou Chimie. Sur les 100 étudiants, 50
se sont inscrits au cours de Physique, 40 se sont inscrits au cours de
Chimie, et 10 se sont inscrits aux deux cours. Finalement, 20 étudiants
n'ont pas encore fait leur choix. La situation peut se résumer à l'aide
d'un diagramme de Venn, produit avec la fonction **ggvenn()** se
trouvant dans la librairie portant le même nom:

```{r 02-Chap2-21, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
library(ggvenn)
grid.newpage()  # Nettoyer la page graphique

x <- matrix(FALSE,nrow=100,ncol=2) # Création des données:
                                # Matrice de 100 lignes, par 2 colonnes
                                # x[ ,1] = inscrit en physique
                                # x[ ,2] = inscrit en chimie
x[1:50,1] <- TRUE               # 50 Physique
x[41:50,2] <- TRUE              # 10 Physique & Chimie
x[51:80,2] <- TRUE              # 30 Chimie seulement
ix <- 1:100                     # Identification des étudiants
Physique <- ix[x[,1]==TRUE]     # Qui est inscrit en Physique?
Chimie=ix[x[,2]==TRUE]          # Qui est inscrit en Chimie?
x <- list("Physique" = Physique,
		"Chimie" = Chimie)          # Données soumises à ggvenn
ggvenn(x)                       # Création du diagramme
```

Quelle serait la probabilité qu'un étudiant s'inscrive au cours de
Physique (A) **OU** au cours de Chimie (B)? En examinant le diagramme de
Venn, on peut voir que l'intersection est partie à la fois de A et de B.
Il est donc nécessaire d'en soustraire un exemplaire:
$$P(A \cup C)=P(A)+P(C)-P(A\cap B)=\frac{50}{100}+\frac{30}{100}-\frac{10}{100}=\frac{80}{100}=0.80$$
Supposons maintenant qu'un troisième cours s'ajoute à l'horaire. Cette
fois-ci, nous avons 200 étudiants devant choisir au moins un cours parmi
Statistiques, Physique et Chimie. Sur 200 étudiants,

    - 60 se sont inscrits au cours de Statistiques     (Événement A)
    - 100 se sont inscrits au cours de Physique        (Événement B)
    - 120 se sont inscrits au cours de Chimie          (Événement C)
    - 30 ont choisi les cours de Probabilité et de Physique
    - 20 ont choisi les cours de Probabilité et de Chimie
    - 40 ont choisi les cours de Physique et de Chimie
    - 20 étudiants ambitieux ont choisi les trois cours
    - 10 étudiants n'ont pas encore fait leur choix

Le diagramme de Venn résumant ces données est reproduit ci-dessous.
Comme le diagramme précédent, il est créé à l'aide de la fonction
**ggven()**:

```{r 02-Chap2-22, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
x=matrix(FALSE,nrow=200,ncol=3)
x[1:20,1:3] <- TRUE		  # Trois cours
x[21:30,c(1,3)] <- TRUE	# Stats &Physique	
x[31:60,1] <- TRUE			  # Physique
x[61:100,1:2] <- TRUE		# Physique & Chimie
x[101:130,3] <- TRUE		  # Stats
x[131:190,2] <- TRUE		  # Chimie

ix <- 1:200
Physique <- ix[x[,1]==TRUE]
Chimie <- ix[x[,2]==TRUE]
Stats <- ix[x[,3]==TRUE]
x <- list("Physique" = Physique,"Chimie" = Chimie,"Stats" = Stats)
ggvenn(x,show_percentage=FALSE)

```

L'espace échantillonnal correspond à l'ensemble des étudiants
susceptibles de s'inscrire à au moins un des 3 cours offerts, incluant
ceux qui n'ont pas encore fait leur choix (nS = 200).

### Exercice:

1.  Quelle est donc la probabilité qu'un étudiant s'inscrive à un cours
    de Physique (B), OU à un cours de Chimie (C)?

$$P(B \cup C)=P(B)+P(C)-P(B\cap C)=\frac{100}{200}+\frac{120}{200}-\frac{60}{200}=\frac{160}{200}=0.80$$

2.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique ET de Chimie?

3.  Quelle est la probabilité qu'un étudiant ne s'inscrive pas à un
    cours de Stats?

4.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Chimie, mais pas à un cours de Stats?

5.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique et à un cours de Chimie, mais pas à un cours de Stats?

6.  Quelle est la probabilité qu'un étudiant s'inscrive à 2 cours
    seulement?

7.  Quelle est la probabilité qu'un étudiant ne s'inscrive qu'à un seul
    cours?

8.  Quelle est la probabilité qu'un étudiant s'inscrive aux trois cours?

9.  Quelle est la probabilité qu'un étudiant ne soit encore inscrit à
    aucun cours?

10. Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique, OU à un cours de chimie, mais pas les deux à la fois?

11. Quelle est la probabilité qu'un étudiant s'inscrive soit au cours de
    Stats uniquement, soit au cours de Physique ET au cours de Chimie
    (mais pas au cours de Stats)?

### Règle de multiplication: événements indépendants

Lorsque deux événements sont indépendants, la probabilité qu'ils se
produisent en même temps est donnée par:

$$P(A \cap B)=P(A)\times P(B)$$ Par exemple, quelle est la probabilité
qu'un étudiant s'inscrive au cours de Physique (A)  **ET** au cours de
Chimie (B)?

$$P(A \cap B)=P(A)\times P(B)=\frac{100}{200}\times\frac{120}{200}=0.30$$
En effet, 60/200 étudiants se sont inscrits aux deux cours (Physique et
Chimie), ce qui représente 30% de l'effectif. Un examen du diagramme de
Venn illustre clairement cette donnée.

### Règle de multiplication générale: Événements dépendants

De la même manière que la règle d'addition énoncée ci-dessus doit
réfléter la nature mutuellement/non mutuellement exclusive des
événements, la règle de multiplication doit tenir compte de la
dépendance des événements en cause, si cette dépendance existe. Ceci
requiert la notion de **probabilité conditionnelle**, définie comme la
probabilité qu'un événement survienne, **étant donné** qu'un autre
événement est déjà survenu.

Reprenons le diagramme de Venn représentant les inscriptions de 100
étudiants de l'ISTEAH aux cours de Physique et de Chimie:

```{r 02-Chap2-23, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
library(ggvenn)
grid.newpage()                  # Nettoyer la page graphique

x=matrix(FALSE,nrow=100,ncol=2) # Création des données
x[1:50,1]=TRUE                  # 50 Physique
x[41:50,2]=TRUE                 # 10 Physique & Chimie
x[51:80,2]=TRUE                 # 30 Chimie seulement
ix=1:100                        # Identification des étudiants
Physique=ix[x[,1]==TRUE]        # Qui est inscrit en Physique?
Chimie=ix[x[,2]==TRUE]          # Qui est inscrit en Chimie?
x=list("Physique" = Physique,
		"Chimie" = Chimie)          # Données soumises à ggvenn
ggvenn(x,show_percentage = FALSE)  
```

De ce diagramme, on constate que la probabilité qu'un étudiant
s'inscrive au cours de Physique est P(A) = 50/100 = 0.5. De même, on
constate que 10 étudiants se sont inscrits aux 2 cours:
$P(A\cap B) = 10/100=0.10$. Finalement, sachant qu'un étudiant s'est
inscrit au cours de Physique, quelle est la probabilité qu'il s'inscrive
aussi au cours de Chimie? Ici, l'espace échantillonnal se rétrécit! Il
s'agit maintenant non pas de l'ensemble des étudiants, mais seulement
des étudiants inscrits en Physique! Donc la probabilité doit tenir
compte de ce fait. On trouve 10 étudiants inscrits au cours de Chimie,
sur les 50 qui sont déjà inscrits au cours de Physique. En conséquence,
la probabilité recherchée est:
$$P(B|A)=\frac{P(A \cap B)}{P(A)}=\frac{10}{50}=0.20$$ On lit: la
probabilité que B survienne, étant donné que A s'est produit, et égale à
la probabilité que A et B surviennent, divisé par la probabilité
**marginale** de A. Il s'agit ici de la **probabilité conditionnelle**.

De la même manière, on pourrait obtenir la probabilité que A survienne,
étant donné que B s'est déjà produit:

$$P(A|B)=\frac{P(A \cap B)}{P(B)}=\frac{10}{40}=0.25$$ On note ici que
$P(A|B)\neq P(B|A)$.

De ces considérations, on peut donc énoncer la règle de multipliction
générale:
$$P(A|B)=\frac{P(A \cap B)}{P(B)}~~~~\to ~~~~P(A \cap B)=P(B)\times P(A|B) $$
Cette définition suggère également une stratégie pour vérifier
l'indépendance de deux événements:

$$Événements~indépendants:~~~~~~ P(A \cap B)=P(A)\times P(B)$$
$$Événements~dépendants:~~~~~~~~~~~ P(A \cap B)=P(A)\times P(A|B)$$

Si les événements sont indépendants, alors $P(B)=P(A|B)$. Il suffit donc
de vérifier cette égalité pour déterminer cette indépendance. De manière
alternative, on peut simplement vérifier: $P(A \cap B)=P(A)\times P(B)$.
Si ces deux termes sont égaux, les événements sont indépendants. Sinon,
ils sont dépendants.

Par exemple, supposons une urne contenant 6 balles numérotées de 1 à 6,
et définissons les événements suivants:\
- A = nombre pair = {2, 4, 6}\
- B = Nombre impair = {1, 3, 5}\
- C = Multiple de 3 = {3, 6}

Le diagramme de Venn illustrant cette situation est:

```{r 02-Chap2-24}
library(ggvenn)
grid.newpage()  # Nettoyer la page graphique

x=list("A" = c(2,4,6),
		"B" = c(1,3,5),
		"C" = c(3,6))
ggvenn(x,show_elements=TRUE,text_size=6)  # Création du diagramme
```

1.  Dans ce contexte, peut-on affirmer que les événements A et B sont
    dépendants? Pour répondre à cette question, vérifions l'égalité
    suivante: $$ P(A\cap B)=P(A)\times P(B)$$
    $$ P(A \cap B)=0~~~~~~~P(A)=3/6~~~~~~~~P(B)=3/6~~~~~~~~P(A) \times P(B)=0.25$$
    Puisque l'égalité n'est pas réalisée, on doit conclure que les
    événements A et B sont dépendants.

2.  Dans ce contexte, peut-on affirmer que les événements A et C sont
    dépendants? Pour répondre à cette question, vérifions l'égalité
    suivante: $$ P(A\cap C)=P(A)\times P(C)$$
    $$ P(A \cap C)=1/6~~~~P(A)=3/6~~~~~P(C)=2/6~~~~~P(A) \times P(C)=3/6\times 2/6=1/6 $$
    Puisque l'égalité se réalise, on peut affirmer que les événements A
    et C sont indépendants.

### Exercice:

1.  Donnez suite à l'exemple précédent: est-ce que les événements B et C
    sont indépendants?

2.  Reprenons l'exemple portant sur les inscriptions aux cours de Stats,
    de Chimie et de Physique, à l'ISTEAH. Le diagramme de Venn est
    reproduit ci-dessous:

```{r 02-Chap2-25, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
x=matrix(FALSE,nrow=200,ncol=3)
x[1:20,1:3]=TRUE		# Trois cours
x[21:30,c(1,3)]=TRUE	# Stats &Physique	
x[31:60,1]=TRUE			# Physique
x[61:100,1:2]=TRUE		# Physique & Chimie
x[101:130,3]=TRUE		# Stats
x[131:190,2]=TRUE		# Chimie

ix=1:200
Physique=ix[x[,1]==TRUE]
Chimie=ix[x[,2]==TRUE]
Stats=ix[x[,3]==TRUE]
x=list("Physique" = Physique,
		"Chimie" = Chimie,
		"Stats" = Stats)
ggvenn(x,show_percentage=FALSE,text_size=6)
```

a.  Est-ce que l'inscription à un cours de Physique est indépendante
    d'une inscription à un cours de Chimie?

b.  Est-ce que l'inscription à un cours de Physique est indépendante
    d'une inscription à un cours de Stats?

c.  Est-ce que l'inscription à un cours de Chimie est indépendante d'une
    inscription à un cours de Stats?

## Tableaux de contingence

L'ensemble des notions qui ont été présentées dans cette section peuvent
aisément se réfléter dans un tableau de contingence. Par exemple, les
inscriptions des étudiants aux cours de Physique et de Chimie peuvent
être résumées dans le tableau suivant:

```{r 02-Chap2-26, echo=FALSE}
B.1=c(40,30,70)
B.0=c(20,10,30)
TotR=c(60,40,100)
x=data.frame(B.1,B.0,TotR)
row.names(x)=c("A.1","A.0","TotC")
kable(x, booktabs=TRUE, caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)

```

Dans ce tableau, A représente l'inscription au cours de Physique (A.1 =
Oui, A.0 = Non), B l'inscription au cours de Chimie (B.1 = Oui, B.0 =
Non).

D'un tel tableau, il est facile d'obtenir les différentes probabilités
qui pourraient nous intéresser:

**Probabilités Marginales**: 
$$P(A)=\frac{TotR}{N}=\frac{60}{100}$$
$$P(B)=\frac{TotC}{N}=\frac{70}{100}$$ 
**Probabilités conjointes**:

$$P(A_{1} \cap B_{1})=\frac{f_{11}}{N}=\frac{40}{100}$$ 
$$P(A_{1} \cap B_{0})=\frac{f_{10}}{N}=\frac{20}{100}$$ 
$$P(A_{0} \cap B_{1})=\frac{f_{01}}{N}=\frac{30}{100}$$ 
$$P(A_{0} \cap B_{0})=\frac{f_{00}}{N}=\frac{10}{100}$$ 

**Probabilités conditionnelles**: 

$$P(A_{1}|B_1)=\frac{f_{11}}{Tot_{B_1}}=\frac{40}{70}$$
$$P(A_{0}|B_1)=\frac{f_{01}}{Tot_{B_1}}=\frac{30}{70}$$
$$P(A_{1}|B_0)=\frac{f_{10}}{Tot_{B_0}}=\frac{20}{30}$$
$$P(A_{0}|B_0)=\frac{f_{00}}{Tot_{B_0}}=\frac{10}{30}$$
$$P(B_{1}|A_1)=\frac{f_{11}}{Tot_{A1}}=\frac{40}{60}$$ 
$$P(B_{0}|A_1)=\frac{f_{01}}{Tot_{A1}}=\frac{20}{60}$$ 
$$P(B_{1}|A_0)=\frac{f_{10}}{Tot_{A0}}=\frac{30}{40}$$ 
$$P(B_{0}|A_0)=\frac{f_{00}}{Tot_{A0}}=\frac{10}{40}$$ 

Notons ici qu'en aucun cas nous nous sommes éloignés de la définition formelle d'une
probabilité: rapport entre le nombre de 'succès' et la taille de l'espace échantillonnal...

Pour résumer le tout, considérons le tableau de contingence suivant, composé de deux rangées et trois colonnes. Le premier tableau contient simplement les fréquences observées dans chacune des cellules ($f_{rc}$), ainsi que les totaux marginaux ($T_R$ et $T_C$). Le nombre total d'observation se trouve dans la case inférieure droite:


```{r}
x=matrix(c(20,30,40,10,20,30),nrow=2,byrow=TRUE)
tbl <- cbind(x,rowSums(x))
tbl <- rbind(tbl,c(colSums(x),sum(x)))
rownames(tbl) <- c("A1","A2","Tc")
colnames(tbl) <- c("B1","B2","B3","Tr")
kable(tbl, booktabs=TRUE,caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)
``` 
Les probabilités marginales  $[P(R),P(C)]$ sont les rapports entre les totaux marginaux $[T_R,T_C]$ et le nombre total d'observations ($N$):

```{r}
df <- tbl
df[1:2, 4] <- tbl[1:2,4]/tbl[3,4]
df[3, 1:3 ] <- tbl[3,1:3]/tbl[3,4]
df <- as.data.frame(round(df,4))
df %>% 
  format_cells(1:2,4,"bold") %>%
  format_cells(3,1:3,"bold") %>%
  knitr::kable(booktabs=TRUE,caption="Probabilités Marginales") 

```

On obtient les probabilités conjointes $P(R_i\cap C_j)$ en divisant chacune des fréquences observées dans les cellules par le nombre total d'observations:


```{r}
df[1:2,1:3] <- tbl[1:2,1:3]/tbl[3,4]
round(df,4) %>% 
  format_cells(1:2,1:3,"bold") %>%
  knitr::kable(booktabs=TRUE,caption="Probabilités Conjointes") 

``` 
Finalement, on obtient les probabilités conditionnelles en divisant les fréquences observées dans les différentes cellules par le total marginal des rangées, ou des colonnes. On obtient évidemment le même résultat en calculant le rapport entre les probabilités conjointes et la probabilité marginale appropriée:

```{r}
dfr <- df[1:2,1:3]/df[1:2,4]
round(dfr,4) %>% 
  format_cells(1:2,1:3,"bold") %>%
  knitr::kable(booktabs=TRUE,caption="Probabilités Conditionnelles (R|C)") 

m <- df[1:2,1:3]
v <- df[3,1:3]
dfc <- m / v[col(m)]
round(dfc,4) %>% 
  format_cells(1:2,1:3,"bold") %>%
  knitr::kable(booktabs=TRUE,caption="Probabilités Conditionnelles (C|R)") 

```


## L'arbre de probabilités

À partir d'un tableau de contingence tel que le précédent, il est
possible de produire une représentation prenant la forme d'une
arborescence fournissant l'ensemble des probabilités discutées
jusqu'ici. Pour produire une telle arborescence, on utilisera avantageusement la fonction **treeDiag()** de la librairie **openintro**. Les principaux arguments de cette fonction sont:
  - main : un vecteur alphanumérique contenant l'étiquette permettant d'identifier les deux variables. Exemple: main = c("A","B")
  - p1 : vecteur des probabilités associées à la branche principale
  - p2 : vecteur des probabilités associées à la branche secondaire
  - out1 : vecteur alphanumérique contenant les étiquettes identifiant les événements survenant sous la branche principale
  - out2 : vecteur alphanumérique contenant les étiquettes identifiant les événements survenant sous la branche secondaire
  - ShoSol : valeur booléenne indiquant si une solution est désirée (calcul des probabilités conjointes)
  - digits : nombre de décimales à afficher
  
Afin d'illustrer l'utilisation de cette fonction, considérons les données résumées dans le tableau de contingence défini au paragraphe précédent:

```{r}
x=matrix(c(20,30,40,10,20,30),nrow=2,byrow=TRUE)
tbl <- cbind(x,rowSums(x))
tbl <- rbind(tbl,c(colSums(x),sum(x)))
rownames(tbl) <- c("A1","A2","Tc")
colnames(tbl) <- c("B1","B2","B3","Tr")
kable(tbl, booktabs=TRUE,caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)

```
et pour lequel on a calculé les probabilités marginales P(R) et P(C), de même que les probabilités conditionnelles P(R|C):

```{r}
pconj <- tbl[1:2,1:3]/tbl[3,4]
pmargr <- rowSums(tbl[1:2,1:3])/tbl[3,4]
pmargc <- colSums(tbl[1:2,1:3])/tbl[3,4]
pcond <- pconj / pmarg[col(pconj)]
pcond <- rbind(pcond,pmargc)
pcond <- cbind(pcond,c(pmargr,1))
rownames(pcond) <- c("A1","A2","P(C)")
colnames(pcond) <- c("B1","B2","B3","P(R)")


kable(round(pcond,4), booktabs=TRUE,caption="P(R|C)") %>%
  kable_styling(full_width = FALSE)
```

À partir de ce dernier tableau, on obtient l'arbre des probabilitésÈ

```{r}
library(openintro)
p1 <- round(pcond[1,1:3],3)
p2 <- round(pcond[2,1:3 ],3)
treeDiag(c("Event A","Event B"), c(.6,.4),
         list(p1,p2), c("A1","A2"),c("B1", "B2","B3"),
         showWork = TRUE)
```
Les probabilités affichées dans les deux premiers noeuds sont les
probabilités marginales [P(A1)=0.6, P(A2)=0.4]. Les noeuds
suivants contiennent les probabilités conditionnelles, et les résultats affichés à la droite sont les probabilités conjointes, produits des
probabilités marginales et des probabilités conditionnelles.

## Distribution de fréquences et probabilités

En recherche scientifique, il est fréquent de condenser un ensemble
d'observations (événements) dans un tableau affichant chaque valeur
possible et le nombre d'occurrence correspondant. Par exemple,
considérons le tableau suivant, résumant l'âge d'un groupe d'élèves du
Lycée:

```{r 02-Chap2-30, echo=FALSE}
agef = data.frame(age = 12:17,freq = c(6,3,10,4,2,5))
kable(agef, booktabs=TRUE, caption="Répartition des Effectifs/Age") %>%
  kable_styling(full_width = FALSE)
```

Dès à présent, vous devriez être en mesure de répondre aux questions
suivantes:

1.  Quelle est la probabilité, en choisissant au hasard un de ces
    élèves, qu'il soit âgé

```{=html}
<!-- -->
```
a.  de 13 ans?\
    - Le nombre total d'observations est nS = 30.\
    - Le nombre d'observations correspondant à âge = 13 est nE = 3.\
    - Donc P(âge=13)= 3/30 = 0.10\
b.  de 15 ans ou moins?
c.  d'au moins 16 ans?
d.  d'au plus 13 ans?
e.  de 12 ou de 17 ans?

Ce type d'exercice sera récurrent à chacune des étapes de votre
formation!

### En bref...

L'ensemble des sections précédente résume l'essentiel de l'approche
classique de probabilités. Nous avons présentés les principales méthodes
nous permettant d'obtenir la taille de l'espace échantillonnal afin de
pouvoir calculer les probabilités associées à différents sous-ensembles
de cet espace. Dans les sections suivantes, l'approche fréquentiste sera
mise à profit pour obtenir de telles probabilités par simulation
informatique.

## Simulation d'un lancer de pièces de monnaie

Dans la section précédente, nous avons solutionné quelques problèmes de
calcul de probabilités en calculant le rapport entre le nombre de fois
qu'un événement survient et le nombre total d'expériences aléatoires
effectuées. On se souviendra qu'une probabilité obtenue de cette manière
représente le résultat que l'on s'attend d'obtenir en effectuant
l'expérience aléatoire un nombre infini de fois. Dire qu'on a une
probabilité de 0.25 d'obtenir 2 FACEs en lançant 2 pièces de monnaie
signifie que si on lance les 2 pièces de monnaie un nombre infini de
fois, à la limite, on obtiendra deux faces une fois sur 4 lancers.

Évidemment qu'il serait fastidieux d'effectuer une expérience aléatoire
un nombre infini de fois, mais si on remplace le terme **infini** par
les termes **'un très grand nombre** de fois', on se retrouve dans un
contexte réalisable... si on a accès à un ordinateur pour se charger du
travail!

La fonction **CoinToss()** permet de simuler le lancer d'une pièce de monnaie. Les
arguments de la foncton sont *n* = Nombre de pièces à lancer, et *p* =
la probabilité d'obtenir un des deux résultats possibles ('PILE' ou
'FACE'). Par défaut, une seule pièce est lancée et la probabilité du
résultat désiré est 0.5. On peut changer ces arguments selon le besoin:

```{r 02-Chap2-32}

x=CoinToss()      # n=1, p=0.5
x

CoinToss(10)      # lancer de 10 pièces, p=0.5

x=CoinToss(5,0.7) # Lancer de 5 pièces, p=0.7 (pièce trafiquée!)
x

```

Pour simuler le lancer d'un certain nombre de pièces un grand nombre de
fois, il suffira d'introduire les arguments appropriés dans la fonction
CoinToss(), et d'examiner les événements qui en résultent sur le nombre d'essais désirés. 

## Exercice 4

1.  Avant d'exécuter CoinToss() pour simuler le lancer d'une seule
    pièce de monnaie, écrivez sur une feuille de papier votre prédiction
    quant au résultat: Pile, ou Face. Activez la fonction, et inscrivez
    à droite de votre prédiction '1' si votre prédiction est correcte,
    et '0' si elle ne s'est pas réalisée. Répétez l'exercice 20 fois!
2.  Combien de vos prédictions se sont avérées correctes? Combien de
    prédictions justes faudrait-il pour que vous puissiez vous
    considérer comme 'devin'?
3.  Simulez le lancer de 20 pièces de monnaie, en supposant que la
    probabilité associée au résultat est 0.5. Comparez les résultats
    avec ceux que vous avez obtenus en (2). Est-ce que les résultats
    sont différents? Comment expliquez-vous cette différence, si elle
    existe?

```{r 02-Chap2-33}

set.seed(1876534)
x = CoinToss(20)
x
```

5.  Combien de côtés 'PILE' avez-vous obtenus? Combien de côtés 'FACE'
    sont apparus? Est-ce que ces résultats sont compatibles avec l'idée
    que les probabilités P(PILE) et P(FACE) sont égales?
6.  Relancez la simulation: est-ce que les résultats sont différents du
    premier essai?
7.  S'il y a différence, comment l'expliqueriez-vous?
8.  Essayons de représenter graphiquement ce résultat à l'aide d'un
    diagramme en secteur. Le code R qui nous permet de le faire est:

```{r 02-Chap2-34}
slices=c(sum(x=="Face"),sum(x=="Pile")) # Définition des secteurs
lbls=c("Face","Pile")                   # Définition des étiquettes
pie(slices, labels = lbls, main="Répartition des résultats Pile/Face")
```

9.  Répétez l'exercice en simulant le lancer de 5, 10, 15, 50 et 100
    pièces. Obtenez le diagramme en secteurs pour chaque simulation.
    Quelle conclusion tirez-vous concernant l'impact de l'augmentation
    de la taille de l'échantillon (nombre de pièces de monnaie))?

## La distribution des probabilités

Si une pièce de monnaie n'est pas trafiquée, on peut accepter le fait
que la probabilité d'obtenir le côté 'FACE' lors d'un lancer de cette
pièce est égale à 0.5. On a une chance sur deux d'obtenir ce côté. Si on
lance plusieurs pièces, n = 5 par exemple, chaque pièce a une chance
égale de tomber du côté 'FACE', et les résultats possibles pour une
telle expérience seraient l'un ou l'autre de 0, 1, 2,3, 4 ou 5 FACES...
Vérifions cela. Le code suivant répète 100 fois le lancer de 5 pièce de
monnaie, et on comptabilise à chaque essai le nombre de FACES
apparaissant:

```{r 02-Chap2-35, tidy=TRUE, tidy.opts=list(width.cutoff=75)}

res = vector()    # Initialisation du vecteur des résultats
nt = 100          # Nombre d'essais
No_Pieces = 5     # Nombre de pièces
set.seed(1876534)
for(i in 1:nt){
  x = CoinToss(No_Pieces,0.5)
  res[i] = sum(x=="Face")
}
fr = as.matrix(table(res))
pr = fr[,1] / nt  # Fréquence relative (Probabilités)
tbl = data.frame(sort(unique(res)),fr,pr)
colnames(tbl) = c("#Faces","Fréq","Prop")
hist(res+.001,xlab="Nombre de Faces",ylab="Fréquences",main="Distribution du nombre de Faces\n en lançant 5 pièces de monnaie")
```

Ajoutons un élément à notre discussion: un tableau résumant la
distribution que nous venons d'établir.

```{r 02-Chap2-36, echo=FALSE}
kable(tbl, booktabs=TRUE, caption="Distribution du nombre de Faces",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)

```

Reprenons l'expérience, mais en la répétant 100,000 fois! Pour permettre
le changement rapide du nombre d'essais, une variable a été créée. Il
vous suffira de changer sa valeur pour spécifier le nombre d'essais à
effectuer:

```{r 02-Chap2-37, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
set.seed(150752)
res = vector()      # Initialisation du vecteur des résultats
No_essais = 100000  # Spécification du nombre d'essais à effectuer
No_Pieces = 5       # Nombre de Pièces
for(i in 1:No_essais){
  x = CoinToss(No_Pieces,0.5)
  res[i] = sum(x=="Face")
}
fr = as.matrix(table(res))
pr = fr[,1] / No_essais
x = sort(unique(res))
tbl = data.frame(x,fr,pr)
espm=sum(x*pr)
colnames(tbl) = c("#Faces","Fréq","Prop")
hist(res+.001,xlab="Nombre de Faces",ylab="Fréquences",main="Distribution de probabilités: 5 pièces")

```

```{r 02-Chap2-38, echo=FALSE}
kable(tbl, booktabs=TRUE, caption="Distribution du nombre de Faces",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
```

Pour répondre à une question telle que: '**Combien de fois obtenons-nous
4 FACES ou PLUS, en lançant 5 pièces de monnaie non trafiquées**?', on
pourrait simplement additionner les fréquences correspondant à x = 4 et
x = 5. Ou on pourrait directement obtenir ce nombre à partir de
l'ensemble des résultats obtenus. On utilisera ici l'opérateur logique
'**\>=**', qui accorde une valeur '1' ou 'TRUE' si x (le nombre de
FACES) est supérieur ou égal à l'argument de droite (4):

```{r 02-Chap2-39}
sum(res>=4) 

```

Connaissant le nombre de résultats qui nous intéressent (x = 4 ou x =
5), on peut aisément calculer la probabilité recherchée:

```{r 02-Chap2-40}

sum(res>=4)/length(res)
```

La distribution de probabilités que nous sommes maintenant en mesure de
définir possède certaines caractéristiques importantes. D'une part, elle
a une forme: les différents événements (axe horizontal) ne sont pas tous
équiprobables. La forme peut être symétrique ou non, selon le cas.

D'autre part, elle a une certaine étendue, indiquant une certaine
dispersion: dans certains cas, les valeurs possibles sont rapprochées
l'une de l'autre, alors que dans d'autres cas, elles sont éloignées.

Et finalement, elle a un centre, situé entre les deux valeurs extrêmes.
Ce centre nous permettrait de répondre à une question telle que: 'En
lançant 5 pièces de monnaie, combien de **FACEs** puis-je m'attendre à
obtenir, de manière générale. La forme nous indique que les résultats
(nombre de FACEs) ne sont pas équiprobables. Quelle serait, alors, le
résultats le plus probable? Le terme désignant cette caractéristique
est: **espérance mathématique**. Mathématiquement, combien de FACEs
peut-on espérer obtenir en lançant 5 pièces de monnaie? On sait que ce
nombre est compris entre 0 et 5. On peut obtenir la probabilité de
chacun de ces résultats. L'espérance mathématique est alors la somme du
produit de la probabilité d'obtenir un résultat particulier et la valeur
de ce résultat. En termes plus formels: $$E(X)=\sum_{X=0}^{5} XP(X)$$

Si on considère la distribution de probabilités obtenue par simulation
plus haut:

```{r 02-Chap2-41, echo=FALSE}
kable(tbl, booktabs=TRUE, caption="Distribution du nombre de Faces",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)

cat("Expérance Mathématique = ",espm)
```

L'espérance mathématique de cette distribution, indiquée ci-dessus, est
obtenue par:
$$(0\cdot0.03222)+(1\cdot0.1573)+(2\cdot0.31181)+(3\cdot0.31423)+(4\cdot0.15401)+(5\cdot0.03043)$$
Essentiellement, on peut s'attendre, en lançant 5 pièces de monnaie un
nombre infini de fois, obtenir 2.5 faces en moyenne. En examinant le
graphique correspondant à cette simulation, on note en effet que les
deux valeurs les plus fréquentes (probables) sont 2 et 3 FACEs.

## La Loi des Grands Nombres

Pour fins d'illustration, le code R suivant répète la simulation pour un
nombre de lancers de 5 pièces de monnaie variant entre 100 et 100,000,
par étape de 100. Le temps d'exécution de l'exercice est un peu long
(quelques minutes), mais permet d'illustrer la Loi des Grands Nombres.
L'événement d'intérêt est 'Nombre de FACEs supérieur à 3':

```{r 02-Chap2-42, eval=FALSE}

pr = vector()               # Vecteur des probabilités
for (j in 100*c(1:1000)){   # Nombres d'essais : 100 - 100,000 / 100
  succes=0
  for (i in 1:j) {
      x = CoinToss(5, 0.5)
     succes = succes + (sum(x == "Face")>3)   # Nombre de Faces supérieur à 3?
     }
  pr[j/100] = succes/j  # Probabilités empirique
}
plot(pr,type="l",xlab="Essais (blocs de 100)",ylab="Probabilité")

```

Le résultat est résumé dans le graphique suivant, dans lequel
l'augmentation de la précision en fonction du nombre d'essais est
manifeste.

![Loi des Grands Nombres: p(x\>3) pour lancer de 5
pièces](./images/Loi_Grands_Nombres.png)

Cette simulation illustre la Loi des Grands Nombres, sur laquelle se
fonde de nombreux concepts statistiques.

### Que se passe-t'il lorsque le nombre de pièces augmente?

Nous sommes donc en mesure de construire une distribution de
probabilités lorsque l'expérience aléatoire consiste à lancer 5 pièces
de monnaie et en compabilisant le nombre de pièces tombant du côté FACE.
Il serait intéressant de voir à quoi ressemble une distribution
similaire, mais lorsque l'expérience aléatoire consiste à lancer 100
pièces! Le code R suivant se charge du travail:

```{r 02-Chap2-43, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
res = vector()      # Initialisation du vecteur des résultats
No_essais = 100000  # Spécification du nombre d'essais à effectuer
No_Pieces = 100     # Nombre de Pièces
set.seed(150752)
for(i in 1:No_essais){
  x = CoinToss(No_Pieces,0.5)
  res[i] = sum(x=="Face")
}
fr = as.matrix(table(res))
pr = fr[,1] / No_essais
x = sort(unique(res))
tbl = data.frame(x,fr,pr)
espm=sum(x*pr)
colnames(tbl) = c("#Faces","Fréq","Prop")
hist(res+.001,xlab="Nombre de Faces",ylab="Fréquences",main="Distribution de probabilités: 100 pièces")

```

La forme de la distribution que l'on obtient dans ce cas est très
caractéristique. Il s'agit d'une forme omniprésente dans l'univers
statistique, et elle reviendra dans notre champs visuel fréquemment dans
les chapitres suivants. La distribution est visiblement symétrique et
s'étend en théorie de 0 à 100 FACES. Cependant, les probabilités
associées aux nombres faibles et aux nombres élevés de FACEs sont très
faibles de sorte que même après 100,000 lancers, aucun de ces événements
n'est apparu. En conséquence, l'étendue de la distribution est plus
restreinte.

De la même manière qu'on l'a fait dans les exercices précédents, on peut
calculer les probabilités associées à différents événements à partir de
cette distribution.

## Exercice 5

1.  Déterminez la distribution de fréquence des résultats (nombre de
    FACES), lorsque le nombre de pièces de monnaie est n = 100, et que
    les pièces ne sont pas trafiquées (soit p = 0.5)\
2.  Comment pourriez-vous qualifier la distribution obtenue?
3.  Quelle est la probabilité d'obtenir 43 faces ou moins?
4.  Quelle est la probabilité d'obtenir entre 45 et 65 faces?
5.  Obtenez la proportion des essais où on obtient plus de 65 FACES, en
    utilisant le tableau de fréquences.
6.  Obtenez cette proportion en comptabilisant cette proportion
    directement des résultats de chaque essai.
7.  Pour quelle proportion des essais obtient-on entre 65 et 85 FACES?
8.  Sous quelle valeur de x (nombre de FACES) trouve-t'on 75% de la
    distribution?\
9.  Au-dessus de quelle valeur de x trouve-t'on 10% de la distribution?
10. Quelles sont les limites à l'intérieur desquelles on retrouve les
    95% centraux des résultats de lancers de ces 100 pièces de monnaie?

## Le lancer de dés

Le même cheminement que nous avons parcouru avec des pièces de monnaie
peut être fait en considérant le lancer de dés, trafiqués ou non... La fonction **DiceRoll()** permet de simuler le lancer d'un dé. Par défaut, le dé est lancé 1
seule fois (n=1) et il n'est pas trafiqué (chacune des 6 faces a une
chance égale d'apparaître: **p=rep(1,6)**). Essayons-le une première
fois, en simulant le lancer de 5 dés non trafiqués:

```{r 02-Chap2-45}

DiceRoll(5)

```

Comme nous l'avons fait dans le cas de pièces de monnaie, nous pouvons
examiner comment les résultats varient d'une expérience à l'autre. Le
code suivant nous permettra d'effectuer autant de lancers d'un dé que
nécessaire, de manière à établir la distribution de fréquences des
valeurs possibles dans cette situation. En lançant le dé 60 fois, on
devrait s'attendre à obtenir 10 fois chacune des valeurs de 1 à 6, en
supposant que le dé n'est pas truqué:

```{r 02-Chap2-46, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
res = vector()       # Initialisation du vecteur des résultats
No_essais = 60       # Spécification du nombre d'essais à effectuer
res= DiceRoll(No_essais)
ft = as.matrix(table(res))
fr = ft/sum(ft)
f = data.frame(sort(unique(res)),ft,fr)
colnames(f)=c("Face","Fréq","Prop")

barplot(height=f$Fréq,names=f$Face,xlab="Observations",ylab="Fréquences",main="Répartition de résultats des lancers d'un dé non truqué")

kable(f, booktabs=TRUE, caption="Résultats du lancer d'un dé",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)

```

## Exercice 6

1.  Déterminez la distribution de fréquence des résultats observés en
    lançant un dé à 100 reprises, supposant que le dé n'est pas truqué.

2.  Comment pourriez-vous qualifier la distribution obtenue? A première
    vue, seriez-vous porté à croire que le dé est truqué?

3.  Répétez l'expérience en lançant le dé 300 fois, puis 1000 fois, et
    enfin 10,000 fois. Avez-vous des raisons de croire que le dé est
    truqué?

    -   Pour cette portion de l'exercice, vous pouvez indiquer les
        nombres d'essais désirés dans la premièe ligne du code suivant:

```{r 02-Chap2-48, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
for (i in c(60,300,600,1000,6000,60000)){
  res = vector()       # Initialisation du vecteur des résultats
  res= DiceRoll(i)
  ft = as.matrix(table(res))
  ft = data.frame(sort(unique(res)),ft,ft/sum(ft))
  colnames(ft)=c("Face","Fréq","Prop")
  titl = paste("Répartition de résultats des lancers d'un dé\n (",i,"lancers)")
  barplot(height=ft$Fréq,names=ft$Face,xlab="Observations",ylab="Fréquences",main=titl)
}

```

4.  Quelle conclusion générale pourriez-vous tirer de ces expériences?
5.  Exécutez le code R suivant afin de définir un dé trafiqué de manière
    à ce que la probabilité d'obtenir une des faces du dé (1-6) soit
    différente de celle typique d'un dé non truqué, et pour obtenir un
    échantillon initial de 30 lancers de ce dé.

```{r 02-Chap2-49, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
No_essais = 30
p6=sample(c(rep(1.6667,5),1.6667+runif(1,-.7,.7)),6)
res = DiceRoll(No_essais,p=p6)
ft = as.matrix(table(res))
ft = data.frame(sort(unique(res)),ft,ft/sum(ft))
colnames(ft)=c("Face","Fréq","Prop")
titl = paste("Répartition de résultats des lancers d'un dé truqué\n pour ",No_essais,"lancers")
barplot(height=ft$Fréq,names=ft$Face,xlab="Observations",ylab="Fréquences",main=titl)

kable(ft, booktabs=TRUE, caption="Résultats du lancer d'un dé truqué",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
```

-   Êtes-vous en mesure de conclure sans doute raisonnable que le dé a
    été trafiqué, sur la base de ces données?
-   Répétez l'expérience en modifiant le nombre d'essais graduellement:
    60, 120, 300, 1,000, 3,000... À partir de quel moment auriez-vous
    confiance en affirmant que le dé est truqué, sur la base des
    observations?
-   Pour quelle face le dé est-il trafiqué, et quelle est la probabilité
    d'obtenir ce résultat, à 5 décimales près?

## La somme de deux dés

Considérons maintenant le lancer de deux dés. On s'intéresse à la somme
des deux faces observées lors du lancer. On sait que la plus petite
somme est 1+1 = 2, et que la plus grande est 6+6 = 12. Donc l'ensemble
des résultats occupe l'étendue 2 à 12.

Le code R suivant permet de simuler le lancer de deux dés:

```{r 02-Chap2-51}
dice.roll <- sample(1:6, size = 2, replace = TRUE)
dice.roll
sum(dice.roll)

```

Tout ce que ce jeu de commandes fait, c'est de choisir 2 (**size=2**)
nombres parmi 1-6 (**1:6**), avec replacement (**replace=TRUE**), et de
calculer la somme des deux valeurs obtenues (**sum()**\*). On sait que
cette somme sera comprise entre 2 et 12.

## Exercice 7

1.  Pourriez-vous écrire un code R permettant de simuler le lancer de 10
    dés non-truqués et de calculer la somme des valeurs observées?
    Inscrivez votre code dans l'espace ci-dessous. Puis appliquez ce
    code dans la console.

| 
| 
| 

2.  Pourriez-vous écrire un code R permettant de lancer un seul dé à 20
    reprises? Utilisez l'espace ci-dessous pour ce faire, puis appliquez
    ce code dans la console.

| 
| 
| 

Pour pouvoir répéter à volonté le code précédent sans que l'on ait à
ré-inscrire ce code dans la console, on pourra créer une fonction.
Jusqu'à maintenant, nous avons utilisé des fonctions pré-définies, mais
il est bon de pouvoir écrire ses propres fonctions selon les besoins du
moment. La définition d'une fonction permettant de lancer deux dés non
truqués pourrait ressembler à ceci:

```{r 02-Chap2-52}
two.dice <- function(){
  dice <- sample(1:6, size = 2, replace = TRUE)
  return(sum(dice))
}

```

Cette fonction, **two.dice** n'a aucun argument: les parenthèses suivant
**function** sont vides. Par contre, en l'exécutant (**two.dice()**),
on obtiendra la somme des deux dés: **return(sum(dice))**

Mettons notre fonction au travail:

```{r 02-Chap2-53}

two.dice()

two.dice()

two.dice()

two.dice

```

Tout semble fonctionner! Sauf pour le dernier essai, où on a oublié
d'inscrire les parenthèses, que l'on doit inclure dans l'exécution de la
fonction, même en l'absence d'argument. Sinon, on peut le voir, le
contenu de la fonction sera reproduit.

Pour répéter le lancer de deux dés *n* fois, on peut simplement utiliser
la fonction **replicate**, qui nécessite deux arguments: le nombre de
fois que l'on veut répéter l'opération, et l'opération à répéter:

```{r 02-Chap2-54}

replicate(20, two.dice())

```

À présent, nous avons tout ce qui est nécessaire pour effectuer une
simulation du lancer de deux dés. Simulons le lancer de 60 paires de
dés, calculant la somme des dés pour chaque paire, pour finalement
afficher le tableau de la distribution résultant de l'opération:

```{r 02-Chap2-55}
x = replicate(20, two.dice())
x
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)),ft,ft/sum(ft))
colnames(ft)=c("Somme","Fréq","Prop")
kable(ft, booktabs=TRUE, caption="Résultats du lancer d'un dé truqué",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
```

La distribution de fréquences résumée dans le tableau nous informe sur
la probabilité d'obtenir chacun des résultats possibles, de 2 à 12.
Graphiquement, on obtient:

```{r 02-Chap2-56, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
titl = paste("Répartition des résultats de lancers de deux dés\n (",50, "lancers)")
  barplot(height=ft$Fréq,names=ft$Somme,xlab="Observations",ylab="Fréquences",main=titl)

```

De ce diagramme, on constate que l'ensemble des sommes de deux dés
possibles couvre bien l'étendue 2 à 12. on note aussi que la probabilité
ne semble pas égale pour chacune des sommes possibles: il semble plus
probable, en lançant deux dés, d'obtenir des valeurs telles que 6, 7 ou
8 que des valeurs plus élevées ou plus faibles.

Augmentons le nombre de lancers de nos dés. Pour n = 500:

```{r 02-Chap2-57, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
No_essais = 500   # Définition du nombre d'essais
x = replicate(No_essais, two.dice())
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)),ft,ft/sum(ft))
colnames(ft)=c("Somme","Fréq","Prop")
kable(ft, booktabs=TRUE, caption="Résultats du lancer de 2 dés",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)

titl = paste("Répartition des résultats de lancers de deux dés\n  (",No_essais,"lancers)")
  barplot(height=ft$Fréq,names=ft$Face,xlab="Observations",ylab="Fréquences",main=titl, names.arg=ft$Somme)

```

Il semble que plus on augmente la taille de l'échantillon, plus on
s'approche d'une distribution de forme triangulaire: les résultats ne
sont pas equi-probables. Les petites sommes, et les grandes sommes sont
moins probables que les sommes intermédiaires.

À ce stade, vous devriez avoir saisi qu'une probabilité peut être
définie comme étant le nombre de 'succès' sur un très grand nombre
d'essais. En mathématiques, on dira que la probabilité d'un 'succès' est
le nombre de 'succès' obtenus pour un nombre **infini** d'expériences
aléatoires. Pour nos besoins, examinons la distribution des sommes de
deux dés pour 1,000,000 d'essais:

```{r 02-Chap2-58, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
set.seed(1234567)
No_essais = 100000   # Définition du nombre d'essais
x = replicate(No_essais, two.dice())
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)),ft,ft/sum(ft))
colnames(ft)=c("Somme","Fréq","Prop")
kable(ft, booktabs=TRUE, caption="Résultats du lancer de deux dés",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
cat("Espérance Mathématique = ",sum(ft$Somme*ft$Prop))
```

On s'attend donc, en lançant deux dés un nombre infini de fois, à
obtenir une somme égale à 7.

```{r 02-Chap2-59, echo=FALSE}
titl = paste("Répartition des résultats de lancers de deux dés\n  (",No_essais,"lancers)")
  barplot(height=ft$Fréq,names=ft$Face,xlab="Observations",ylab="Fréquences",main=titl, names.arg=ft$Somme)
```

On note ici que la distribution est très clairement définie. Elle est
presque parfaitement symétrique, et les proportions observées pour
chaque somme possible correspond presque parfaitement aux probabilités
que plus tard on pourra obtenir de manière analytique (avec
des'formules' qu'il n'est pas utile de présenter ici!).

Comme on l'a fait dans l'exercice 4, on peut obtenir les probabilités
associées à différentes zones de cette distribution. On a déjà vu
qu'ayant un accès direct à l'ensemble des données, il suffit d'une
opération de comparaison, ou opération logique pour obtenir réponses à
nos questions. Par exemple, quelle est la probabilité d'obtenir une
somme inférieure à 5:

```{r 02-Chap2-60}
sum(x < 5) / length(x)

```

De la simulation précédente, le million de résultats se trouve dans la
variable x. Le nombre d'essais est donné par la fonction **length(x)**,
et le rapport de la somme des x inférieurs à 5, divisé par le nombre
total d'essais nous donne la probabilité recherchée.

## Exercice 8

**NOTES**: - lorsqu'une question implique une condition **OU** une
autre, l'opérateur est la barre verticale '\|':

```{r 02-Chap2-61}
a = 5
b = 10
(a < 7) | (b > 12)  # Première condition VRAIE, seconde FAUSSE. Ensemble VRAI
```

-   lorsqu'une question implique une condition **ET** une autre,
    l'opérateur est '&' :

```{r 02-Chap2-62}
a = 5
b = 10
(a < 7) & (b < 12)  # Les deux conditions sont VRAIES. Ensemble VRAI

```

-   La distribution utilisée ici est la suivante:

```{r 02-Chap2-63, tidy=TRUE, tidy.opts=list(width.cutoff=75)}
No_essais = 500   # Définition du nombre d'essais
x = replicate(No_essais, two.dice())
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)),ft,ft/sum(ft))
colnames(ft)=c("Somme","Fréq","Prop")
kable(ft, booktabs=TRUE, caption="Résultats du lancer de 2 dés",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)

titl = paste("Répartition des résultats de lancers de deux dés\n  (",No_essais,"lancers)")
  barplot(height=ft$Fréq,names=ft$Face,xlab="Observations",ylab="Fréquences",main=titl, names.arg=ft$Somme)

```

1.  En vous basant sur la dernière distribution obtenue suite à 500
    lancers de deux dés, quelle serait la probabilité d'obtenir une
    somme supérieure à 10?

2.  Toujours en vous basant sur la distribution obtenue, quelle serait
    la probabilité d'obtenir une somme supérieure à 10 ou inférieure à
    5?

3.  Quelle serait la probabilité d'obtenir une somme comprise entre 6 et
    8?

4.  Quelle est la probabilité d'obtenir au plus une somme égale à 6?

5.  Quelle est la probabilité d'obtenir au moins une somme égale à 6?

## Exercice 9

1.  En utilisant la dernière distribution obtenue (1,000,000 d'essais),
    quelle serait la probabilité d'obtenir une somme supérieure à 10?

2.  Toujours en vous basant sur cette distribution, quelle serait la
    probabilité d'obtenir une somme supérieure à 10 ou inférieure à 5?

3.  Quelle serait la probabilité d'obtenir une somme comprise entre 6 et
    8?

4.  Quelle est la probabilité d'obtenir une somme inférieure ou égale à
    6?

5.  Quelle est la probabilité d'obtenir une somme supérieure ou égale à
    6?

6.  Comparez ces trois probabilités à celles obtenues dans l'exercice 3.
    Quel ensemble de probabilités vous semble le plus précis?

7.  Considérez une modification de la fonction two.dice() qui retourne
    la valeur des deux dés, plutôt que leur somme:

```{r 02-Chap2-64}
two.dice2 <- function(){
  dice <- sample(1:6, size = 2, replace = TRUE)
  return(dice)
}

```

En utilisant cette fonction et la fonction replicate(), obtenez la
probabilité d'obtenir un double, soit (1,1), (2,2), ..., (6,6))?

8.  Quelle serait la probabilité qu'au moins un des deux dés soit un
    '6'?

9.  Il existe un vieux jeu de hasard italien appelé **Passadieci**, dans
    lequel le but est d'obtenir au moins 11 lorsque trois dés à six
    faces sont lancés. La fonction suivante vous permettra d'effectuer
    autant d'expériences aléatoires (lancers de 3 dés) que vous le
    désirez et d'obtenir la somme de ces trois dés:

```{r 02-Chap2-65, eval=FALSE}
three.dice = function(){
  res = sample(1:6,3,replace = TRUE)
  return(sum(dice))
}

```

a.  Utilisez cette fonction et la fonction **replicate()** que nous
    avons utilisé plus haut pour simuler 100,000 réplications de ce jeu
    et sauvegardez les résultats dans un vecteur appelé **passadieci**.

-   Utilisez ce vecteur pour répondre aux questions suivantes :

    b.  Quelle est la probabilité de gagner à ce jeu (ie obtenir une
        somme supérieure ou égale à 11) ?
    c.  Qu'est-ce qui est le plus probable en lançant trois dés :
        obtenir 11 ou obtenir 12 ?
    d.  Quelle est la probabilité d'obtenir une somme inférieure ou
        égale à 7 ou supérieure ou égale à 15, toujours en lançant nos 3
        dés?
    e.  Tracez un graphique pour illustrer l'ensemble de la distribution
        des probabilités.

10. Parmi les deux situations suivantes, laquelle est la plus probable?

    1.  obtenir au moins un 6 en lançant 4 fois un seul dé à six faces.\
    2.  obtenir au moins une paire de 6 en lançant 24 fois deux dés à
        six faces.

```{=html}
<!-- -->
```
a.  Vous devez donc effectuer deux simulations de manière à déterminer

b.  la probabilité d'obtenir au moins un '6' en lançant 4 dés. La
    fonction suivante pourrait s'avérer utile:

```{r 02-Chap2-66, eval=FALSE}
SituationA <- function(){
  rolls <- sample(1:6, size = 4, replace = TRUE)
  res1 <- sum(rolls == 6) > 0
  return(res1)
}

```

c.  La probabilité d'obtenir au moins une paire de 6 en lançant 24 fois
    deux dés. La fonction suivante devrait pouvoir faire le travail:

```{r 02-Chap2-67, eval=FALSE}
SituationB <- function(){
  d1 <- sample(1:6, size = 24, replace = TRUE)  # Deux dés sont lancés 
  d2 <- sample(1:6, size = 24, replace = TRUE)  # 24 fois
  res2 <- sum((d1 == d2) & (d1 == 6)) > 0       # Condition recherchée
  return(res2)
}

```

d.  Pour répondre à la question, il suffira d'utiliser la fonction
    **replicate()** pour chacune de ces deux fonctions pour établir la
    distribution de probabilités correspondant aux deux situations. On
    pourra alors déterminer laquelle des situations est la plus
    probable! Vous prenez la relève?

## Urnes: Plus que 2 catégories

Lorsqu'on aborde l'étude des probabilités, l'utilisation d'exemples
impliquant une urne contenant des balles de différentes couleurs est
fréquente. Supposons une urne contenant 12 balles rouges, 18 balles
bleues et 10 balles vertes, pour un total de 40 balles. On peut dès lors
définir plusieurs problèmes dont les solutions ne seront jamais en
contradiction avec la définition que nous avons donnée d'une
probabilité.

-   Quelle est la probabilité, en tirant de l'urne 5 balles, d'obtenir
    uniquement des balles rouges?

    -   Solution:

        -   Puisqu'il y a 12 balles rouges dans l'urne, le nombre de
            combinaisons résultant en 5 balles rouges sera:
            $$C(12,5) =   \frac{12!}{5!(12-5)!} = \frac{12\cdot...\cdot8}{5\cdot4\cdot3\cdot2\cdot1} = nE =792$$

        -   Le nombre total de combinaisons de 5 balles tirées de
            l'ensemble des 40 balles que contient l'urne est:
            $$C(40,5) =     \frac{40!}{5!(40-5)!} = nS = 658,008$$

        -   La probabilité recherchée est donc:
            $$\frac{nE}{nS} = \frac{792}{658,008} = 0.0012$$

Sous R, on peut définir des fonctions susceptibles de nous fournir une
visualisation de ce type de problème. Par exemple, supposons une urne
avec une boule de chacune des couleurs rouge, bleue et noire. Si vous
choisissez deux boules **avec remise**, il y a $3^2$ permutations :

    {rouge, rouge}, {rouge, bleu}, {rouge, noir}, 
    {bleu, rouge},  {bleu, bleu},  {bleu, noir}, 
    {noir, rouge},  {noir, bleu},  {noir, noir}. 

Sous R, on peut obtenir ce résultat en utilisant la fonction
**permutations()**, incluse dans la librairie **arrangements**. Cette
fonction a 3 arguments: le vecteur contenant les éléments à permuter, le
nombre d'éléments à sélectionner, et un indicateur logique indiquant la
remise ou non:

```{r 02-Chap2-68}

  library(arrangements)             # Chargement de la librairie
  
  x <- c("rouge", "bleu", "vert")   # Définition de l'urne
  # Choisir 2 boules avec remise, et dresser la liste des permutations
  permutations(x,2,replace=TRUE)
```

À partir de cette liste, il suffit de définir l'événement recherché, en
déterminer la fréquence, pour ensuite obtenir sa probabilité. Par
exemple, quelle est la probabilité d'obtenir une boule rouge et une
boule verte?

```{r 02-Chap2-69, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
library(arrangements)
x <- c('rouge', 'bleu', 'vert')
res = permutations(x,2,replace=TRUE)
prob = sum((res[,1]=="rouge" & res[,2]=="vert")|(res[,1]=="vert" & res[,2]=="rouge")) / length(x)^2
cat("Probabilité = ",prob)
```

La fonction **polyUrn()**, de la librairie **isteahMAT104** permet d'effectuer ce type de calculs de manière plus générale, par simulation. Les arguments sont: *n* = nombre de balles à sélectionner, *r* = nombre de balles rouges, *b* = nombre de balles bleues,
et r = nombre de balles vertes; et le dernier argument, repl a une
valeur logique déterminant si l'échantillonnage se fait avec ou sans
remise. La valeur retournée est un vecteur composé des nombres de balles
rouges, vertes et bleueus, respectivement, dans un échantillon de n
balles.

Pour illustrer, exécutons cette fonction en utilisant l'exemple
précédent, pour tirer 5 balles rouges en sélectionnant 5 balles au hasard, d'une urne contenant 10R, 18B et 10G
balles:

```{r 02-Chap2-71}
x = polyUrn(5, r=12, b=18, g=10)
x

```

Le premier élément du vecteur sortant est le nombre de balles rouges; le
second élément est le nombre de balles bleues; et le dernier élément est
le nombre de balles vertes. On peut utiliser cette fonction de manière
répétitive un grand nombre de fois pour permettre une estimation des
probabilités recherchées.

```{r 02-Chap2-72, tidy=TRUE, tidy.opts=list(arrow=TRUE, indent=2)}
ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 5         # Nombre de balles tirées lors d'une expérience
r = 12        # Nombre de balles rouges
b = 18        # Nombre de balles bleues
g = 10        # Nombre de balles vertes
E = c(5,0,0)  # Événement recherché (r = 5, b = 0, g = 0)
repl = FALSE
succes = 0    # Initialisation des variables servant au comptage
i = 0         #
repeat{
  draw = polyUrn(n,r,b,g,repl)    # Exécution d'une expérience
  i = i + 1
  if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
  if (i == ndraw){break}                  # Exercice complété?
}
cat("Probabilité = ",succes/ndraw)
```

Ce résultat n'est pas très éloigné de ce qu'on a obtenu en utilisant une
approche classique!

Relancez le code R précédent en variant le nombre d'expériences. Quelle
probabiité obtenez-vous en effectuant 1000, 1,0000, 100,000, 1 million,
ou même 10 millions d'expériences, si vous en avez le temps! Quelle
conclusion tirez-vous de cet exercice?

## Exercice 10

Pour les exercices suivants, utilisez l'approche classique et la
simulation pour obtenir les réponses:

1.  Sous le balcon de Juliette, Roméo cueille des fleurs pour lui lancer
    un bouquet. Le parterre de roses (la fleur) comporte 4 fleurs
    rouges, 3 fleurs jaunes et 5 fleurs roses. Roméo procédant dans
    l'obscurité, il ne peut pas contrôler la couleur des fleurs qui est
    donc considérée comme aléatoire.

-   Si Roméo cueille trois fleurs, quelle est la probabilité qu'il
    obtienne trois fleurs de couleur rose ?

-   Si Roméo cueille trois fleurs, quelle est la probabilité qu'il
    obtienne une fleur de chaque couleur ?

-   Si Roméo cueille quatre fleurs, quelle est la probabilité qu'il
    n'obtienne aucune fleur jaune ?

-   Si Roméo cueille deux fleurs, quelle est la probabilité qu'il
    obtienne des fleurs de couleurs différentes ?

2.  En utilisant la fonction polyUrn() et le code défini plus haut,
    étudiez une situation dans laquelle l'urne contient 15 balles
    rouges, 7 balles bleues et 20 balles vertes. Répétez les exercices
    en variant le nombre d'expériences aléatoires.

**NOTE**: Pour ces exercices, il vous sera nécessaire d'éditer le code
de manière à définir le contexte de l'expérience (nombre de balles à
échantillonner, nombre d'expériences, nombres de balles pour chaque
couleur), et l'événement recherché. Cela se fait aux lignes suivantes:

```{r 02-Chap2-73, eval=FALSE}

ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 5         # Nombre de balles tirées lors d'une expérience
r = 12        # Nombre de balles rouges
b = 18        # Nombre de balles bleues
g = 10        # Nombre de balles vertes
E = c(5,0,0)  # Événement recherché (r = 5, b = 0, g = 0)

```

-   Quelle est la probabilité, en tirant 10 balles, que cet échantillon
    soit composé de 3 balles rouges, 4 balles bleues et 3 balles vertes?
-   Quelle est la probabilité, en tirant 10 balles, que cet échantillon
    soit composé de 2 balles rouges, d'aucune balle bleue et de 8 balles
    vertes?
-   Quelle est la probabilité qu'un échantillon de 12 balles ne soit
    composé que de balles vertes?

3.  De la même manière qu'en (2), refaites l'examen des probabilités
    obtenues en (1), sous le balcon de Juliette...

## Simulation de jeux de cartes

Les jeux de cartes sont, avec les lancers de pièces de monnaie et de
dés, des outils privilégiés pour l'apprentissage des principes de base
des probabilités. Pour faciliter leur utilisation dans cette
présentation, utilisons la fonction **BlackJack()** de la librairie **isteahMAT104**. Cette fonction permet de générer un certain nombre de mains (n = 100 par défaut) de 3 cartes. La sortie est la somme des 3 cartes calculée de la manière suivante:

-   Cartes 2 - 9: valeur nominale
-   Cartes 10, Valet, Reine et Roi: 10
-   As: Si la somme des deux autres cartes est supérieure à 11, l'as
    vaut 1; dans le cas contraire, il vaut 11.

Les librairies **dplyr** et **tidyr** sont requises au préalable.

Quelle est la probabilité d'obtenir une somme égale à 21 en tirant des
mains 3 cartes sélectionnées au hasard, d'un jeu de cartes standard, et
la somme étant calculée suivant la description donnée plus haut? Pour
répondre à cette question, tirons 1,000 mains de 3 cartes:

```{r 02-Chap2-75}
library(dplyr)
library(tidyr)

n = 1000  # Nombre de mains à tirer
x = BlackJack(n)
cat("Probabilité = ",sum(x==21)/n)
    
```

## Exercice 11:

1.  Utilisez la fonction **BlackJack()** afin de déterminer les
    probabilités:

```{=html}
<!-- -->
```
a.  d'obtenir une somme supérieure à 18.
b.  d'obtenir une somme inférieure à 7.
c.  une somme comprise entre 10 et 14.

## Deux Définitions d'une Probabilité

Dans ce document, deux définitions distinctes du terme PROBABILITÉ ont
été mises de l'avant:

1.  **Définition classique**: Nombre d'événements favorables (résultats
    d'une épreuve dont on veut déterminer la fréquence relative), **nE**
    sur le nombre total d'événements possibles (nombre d'éléments dans
    l'espace échantillonnal), **nS**
2.  **Définition fréquentiste**: En répétant une épreuve un GRAND nombre
    de fois (N) et en notant le résultat de l'épreuve à chaque occasion,
    on obtient une approximation de P(E) en divisant le nombre de
    résultats favorables (nE) par le nombre d'épreuves (N). Cette
    définition se fonde sur la loi des Grands Nombres:
    $$P(E)=\displaystyle \lim_{N \to \infty} \frac{nE}{N}$$ Lorsque le
    nombre d'expériences aléatoires tend vers l'infini, la probabilité
    d'un événement donné est égale au nombre d'occurrence d'un événement
    particulier, divisé par le nombre d'expérience.

Cette dernière définition nous conviendra dans un très grand nombre de
situations qu'on examinera dans le futur. Mais il faut aussi mentionner
une troisième définition qui réfère à une approche distincte de l'étude
des probabilités et de l'inférence statistique...

3.  **Probabilité subjective**: Lorsqu'on ne dispose que peu ou pas de
    données concernant les résultats d'une expérience aléatoire, on
    pourra proposer une probabilité subjective, sur la base d'intuition,
    d'informations partielles ou de toute autre information. On obtient
    une probabilité exprimant notre de degré de croyance qu'un événement
    particulier va se produire. Cette probabilité peut varier d'un
    individu à l'autre, selon les informations dont chacun dispose. Par
    exemple, d'après la pression atmosphérique présente, le degré
    d'humidité présent, et la présence de cumulo-nimbus dans le ciel, il
    y a 60% de chance qu'il y ait de la pluie avant la fin de la
    journée... La probabilité n'est plus basée sur le nombre
    d'événements possibles, mais sur l'information dont on dispose pour
    l'évaluer. Et cette évaluation change en fonction de nouvelles
    informations s'ajoutant aux anciennes. Cette définition de la
    probabilité est propre aux méthodes inférentielles Bayesiennes.

## Regards vers le futur!

Jusqu'à maintenant, nous avons considéré des expériences aléatoires dont
les résultats étaient des événements discrets: un certain nombre de
pièces de monnaie tombant sur le côté FACE, une valeur comprise entre 1
et 6 observée en lançant un dé, etc. Dans plusieurs cas, il était facile
d'obtenir la probabilité qu'un événement survienne: la probabilité
d'obtenir FACE en lançant une pièce de monnaie est 0.5, en lançant 2
pièces de monnaie, la probabilité d'obtenir 2 FACES est 0.5x0.5 = 0.25
(on applique ici la règle de multiplication des probabilités
d'événements indépendants); en lançant un dé, la probabilité d'obtenir
un 6 est 1/6, et la probabilité d'obtenir un 5 OU un 6 est 1/6+1/6 = 2/6
(on applique ici la règle d'addition des probabilités d'événements
mutuellement exclusifs). Nous avons aussi utilisé la simulation afin
d'obtenir des distributions de probabiités à partir desquelles on
pouvait sans trop de difficulté obtenir les probabilités recherchées.
Une troisième approche existe, pour obtenir des réponses analogues: la
définition de **distributions théoriques**.

Dans le cas des situations que nous avons étudiées jusqu'à maintenant,
situations impliquant des événements discrets dont la probabilité
d'occurrence est invariante d'un essai à l'autre, la distribution
théorique appropriée est la distribution **binômiale**. Cette
distribution permet d'obtenir les probabilités de tout événement de ce
type, de manière analytique. Sous condition que certains postulats sont
respectés, les probabilités sont en effet données par l'expression
suivante:
$$f(x|n,p) = \frac{n!}{x!(n-x)!}\cdot p^x \cdot (1-p)^{n-x} = {{n}\choose{x}} \cdot p^x \cdot (1-p)^{n-x}$$
Croyez-le ou non, cette expression est simplement un rapport entre le
nombre de 'succès' divisé par le nombre total d'événements possibles.
Donc aucune différence par rapport aux définitions présentées
antérieurement. En reprenant l'exemple du lancer de 5 pièces de monnaie,
le résultat des opérations décrites par cette équation se lisent: 'En
lançant un nombre infini de fois *n*=5 pièces de monnaie non truquées
(*p*=0.5), la probabilité d'obtenir *x*=3 FACEs est égale à...':
$$f(3|5,0.5) = \frac{5!}{3!(5-3)!}\cdot 0.5^3 \cdot (1-0.5)^{5-3} = 0.3125 $$
Ce résultat est presqu'identique à la probabilité obtenue par simulation
dans la section 1.2.

Sous R, on peut obtenir cette probabilité en utilisant la fonction
**dbinom()**:

```{r 02-Chap2-76}
dbinom(x=3,size=5,prob=0.5) # P(3 FACES en 5 lancers, p=0.5)

```

Les arguments de cette fonction sont: x= Événement recherché; size =
Nombre de pièces lancées; et prob = probabilité d'un succès.

La fonction **rbinom(n,size,prob)** est également disponible pour
générer des données binômiales aléatoires:

```{r 02-Chap2-77}
rbinom(20,5,0.5)
```

La distribution binômiale n'est qu'une des nombreuses distributions de
probabilités applicables en fonction des différents types de variables
aléatoires. Ces distributions sont dérivées de manière analytique plutôt
que par simulation, et représentent les probabilités théoriques que l'on
obtiendrait pour un nombre infini d'exécutions d'une expérience
aléatoire. Elles seront présentées dans les chapitres suivants, selon
les besoins.

## CONCLUSION

Dans ce document, nous avons introduit les concepts fondamentaux des
probabilités et de leurs distributions. Dans ce contexte, deux approches
ont été mises en relief: une approche classique, pertinente lorsque
l'espace échantillonnal est fini, de sorte que le nombre d'événements
possibles puisse être formellement obtenu; et une approche fréquentiste
nécessitant des algorithmes de simulation informatique. Tout au long des
documents à suivre, ces notions seront mises en application afin
d'étudier une vaste gamme de situations.

Nous avons aussi introduit la **distribution BINÔMIALE**, qui est
appropriée pour représenter des données discrètes obtenues lors
d'expériences aléatoires dans lesquelles les résultats se présentent
sous deux formes: '**SUCCÈS** ou **ÉCHEC**: en lançant un dé, on obtient
un '6' ou on obtient 'autre chose'; en lançant une pièce de monnaie, on
obtient **PILE** ou **FACE**. Cette distribution peut servir de prélude
à plusieurs autres distributions de probabilité qui deviendront
nécessaires pour traiter des situations plus complexes.


## Solutionnaire des exercices

### Exercice 1

Pour chacune des situations suivantes, définissez l'expérience
aléatoire, l'espace échantillonnal, ce que pourrait être un événement,
et calculez nE et nS:

1.  Dans un jeu de hasard, on lance 3 pièces de monnaie et on gagne si
    on obtient 3 FACES ou 3 PILES.

-   Solution:

    -   L'expérience aléatoire est le **lancer de 3 pièces**
    -   L'espace échantillonnal se compose de tous les événements
        possibles lorsque 3 pièces de monnaie sont lancées: chaque pièce
        peut tomber sur PILE ou FACE. Donc le nombre d'éléments
        composant l'espace échantillonnal est égal à
        $nS=2\times 2\times 2 = 2^3 = 8$
    -   Les événements d'intérêt sont: PPP et FFF, et ces deux
        événements sont uniques à l'intérieur de **S**. Donc nE = 2.

2.  Dans une usine, on fabrique 12 instruments par heure. Ces 12
    instruments sont ensuite mis en boîte, à la condition qu'il n'y ait
    pas plus de 2 instruments présentant des défauts de fabrication.

-   Solution:

    -   L'expérience aléatoire est la constitution d'un ensemble de 12
        instruments, chacun d'eux pouvant être défectueux ou non.
    -   L'espace échantillonnal se compose de tous les ensembles de 12
        instruments qu'il est possible de former, chacun d'eux pouvant
        être défectueux ou non. Le nombre d'éléments composant cet
        espace est égal à $nS=2^12^=4096$.
    -   Les événements d'intérêt sont les ensembles de 12 instruments
        comptant 0, 1 ou 2 instruments défectueux. Nous verrons plus
        loin que nE serait égal au nombre de combinaisons de 12 objets
        pris 0, puis 1, puis 2 à la fois:
        $nE=\binom{12}{0}+\binom{12}{1}+\binom{12}{2}=1+12+66=79$

3.  Dans une classe de mathématiques, 35 étudiants sont inscrits. Le
    professeur espère qu'il n'y aura pas plus de 5 étudiants qui
    échoueront le cours. On suppose ici que pour chacun des étudiants,
    les chances d'échouer sont de 1/2 (ce qui n'est pas réaliste, mais
    supposons-le pour les besoins de l'exemple!)

-   Solution:

    -   L'expérience aléatoire est 'être inscrit à un cours de
        mathématiques'.
    -   L'espace échantillonnal se compose de nS = 35 étudiants, chacun
        pouvant échouer ou réussir le cours. Dans ce cas,
        $nS = 2^35^=34,359,738,370$
    -   Les événements d'intérêt sont: observer soit 0, 1, 2, 3, 4 ou 5
        échecs. Dans ce cas,
        $nE = \binom{35}{0}+\binom{35}{1}+\binom{35}{2}+\binom{35}{3}+\binom{35}{4}+\binom{35}{5} =1+35+595+6545+52360+324632=384,168$

4.  En période de pandémie, un restaurant ne peut accueillir plus de 5
    clients à la fois. À l'extérieur, 15 personnes attendent pour y
    entrer et on choisit au hasard celles qui sont admises.

-   Solution:

    -   L'expérience aléatoire est 'sélectionner 5 personnes attendant à
        l'extérieur'.
    -   L'espace échantillonnal se compose des 15 personnes attendant à
        l'extérieur. Donc nE = 15.
    -   L'événement d'intérêt correspond aux 5 personnes sélectionnées.
        Dans ce cas, $nE = \binom{15}{5}=3,003$

5.  En période de pandémie, un restaurant ne peut accueillir plus de 5
    clients à la fois. À l'extérieur, 15 personnes attendent pour y
    entrer et on choisit au hasard celles qui sont admisesc Ces 15
    personnes portent toutes des noms différents. Désignons-les par les
    numéros 1 à 15.

-   Quelle est la valeur de nE, si on veut admettre les personnes 1 à 5?

    -   Solution:

        -   Parmi l'ensemble des sélections possibles , il n'y a qu'une
            manière de sélectionner les clients 1 à 5. nE = 1

-   Quelle est la valeur de nS dans cette situation?

    -   Solution:

        -   De l'exercice 4, on a trouvé nS = 3,003

-   Quelle est la probabilité que les personnes 1 à 5 soient admises?

    -   Solution:

        -   $P(E)=\frac{nE}{nS}=\frac{1}{3003}=0.000333$

6.  De combien de façons différentes peut---on répartir un groupe de 7
    personnes sur une rangée de 7 chaises ?

-   Solution:

    -   7 personnes peuvent prendre la première chaise; il reste 6
        personnes pour la deuxième, 5 pour la troisième, etc. Donc il y
        a 7! = 5040 manière d'assoir 7 personnes dans une rangée de 7
        chaises.

7.  Trois garçons et deux filles prennent place sur un banc public de 5
    places:

-   De combien de facons différentes, 3 garcons et 2 filles peuvent-ils
    prendre place sur ce banc?

    -   Solution:

        -   Cinq personnes peuvent prendre place de 5! = 120 manière,
            sur un banc de 5 places.

-   De combien de facons peuvent-ils s'asseoir si les garcons s'assoient
    les uns à côté des autres et s'il en est de méme pour les filles ?

    -   Solution:

        -   Si les garçons et les filles sont groupés, on obtient soit
            'GGGFF' ou 'FFGGG'. Donc il y a 2 façons d'obtenir cet
            arrangement.

-   De combien de maniéres différentes peuvent---ils s'asseoir si
    seulement les filles s'assoient l'une a côté de l'autre?

    -   Solution:

        -   Dans ce cas, les arrangements possibles sont: 'GFFGG',
            'GGFFG'. Donc deux arrangements possibles.

### Exercice 2

1.  Un comité de 3 personnes doit être formé à partir d'un groupe de 12
    employés d'un grand commerce. Une des 3 personnes sélectionnées sera
    présidente du comité, une autre sera secrétaire, et l'autre agira
    comme conseillère.

```{=html}
<!-- -->
```
a.  Combien de comités différents sont possibles?

```{=html}
<!-- -->
```
    - Solution:
      - Puisqu'un rôle particulier est donné à chaque personne composant un comité, on doit tenir compte de l'ordre. Le nombre de PERMUTATIONS de 12 personnes prises 3 à la fois est donc:
      $$P(12,3) =   \frac{12!}{(12-3)!} = 12\cdot11\cdot10=1320$$
      

b.  En supposant que les 12 employés du commerce portent des noms
    différents, combien de comités seraient composé de 'Marie-Lune',
    'Pierre', et 'Harold'?\
    - Solution:

    -   Ces trois personnes forment un comité, et chacune d'elles prend
        un des 3 rôles. Le nombre de permutations de trois personnes
        prises 3 à la fois est égal à 6: 3 personnes peuvent occuper le
        poste de président. Une fois ce poste comblé, il reste 2
        personnes pour les postes de secrétaire et de conseiller. Et
        finalement, la troisième personne prend le dernier poste:
        P(3,3)=6

c.  Quelle est la probabilité que ce dernier événement se produise?\
    - Solution:

    -   $P(E)=\frac{6}{1320}=0.004545$

d.  Si 'Pierre' doit nécessairement être Président en raison de son
    ancienneté dans l'entreprise, dans combien de comités lui
    associerait-on 'Marie-Lune' et 'Harold'?\
    - Solution:

    -   Si Pierre est président, deux personnes sont sélectionnées parmi
        les 11 qui restent disponibles. Le nombre de permutations de 11
        personnes prises 2 à la fois est:
        $$P(11,2) =   \frac{11!}{(11-2)!} = 11\cdot10=110$$ De ces 110
        possibilités, les événements 'Marie-Lune est secrétaire' et
        'Harold est conseiller', et 'Marie-Lune est conseillère' et
        'Harold est secrétaire' sont les deux seuls événements
        d'intérêt.

e, Quelle est la probabilité que ce dernier événement se produise?\
- Solution: - $P(E)=\frac{2}{110}=0.01818$

### Exercice 3

1.  De combien de maniéres peut-on former un jury de 3 hommes et 2
    femmes parmi 7 hommes et 5 femmes ?

-   Solution: $$C(7,3) = \frac{7!}{3!(7-3)!} = 35$$\
    $$C(5,2) = \frac{5!}{2!(5-2)!} = 10$$\
    Le nombre total est le produit: $35\times10=350$ Sous R:

```{r}
choose(7,3)*choose(5,2)
```

2.  À l'occasion d'un examen, un étudiant doit répondre à 8 questions
    sur un total de 10.

```{=html}
<!-- -->
```
a.  Combien de choix de questions sont possibles ?

-   Solution:

    -   Le nombre de combinaisons de 10 questions prises 8 à la fois:

    ```{r}
    choose(10,8)
    ```

b.  Combien de choix y a-t-il s'il doit répondre aux 3 premières
    questions ?

-   Solution:

    -   Si les 3 premières questions sont obligatoires, il reste 5
        questions à choisir parmi les 7 qui restent:

    ```{r}
    choose(7,5)
    ```

c.  Combien de choix y a---t-il s'il doit répondre au moins à 4 des 5
    premières questions ?

-   Solution:

    -   Ici, on doit considérer le choix de 4 questions parmi les 5
        premières. Une fois ces 4 questions sélectionnées, il en reste 6
        à partir desquelles on doit en choisir 4. Donc, le nombre
        d'examens différents est:

    ```{r}
    choose(5,4)*choose(6,4)
    ```

3.  Combien de plaques d'immatriculation de véhicules peut-on former si
    chaque plaque contient 2 lettres différentes suivies de 3 chiffres
    différents ?

-   Solution:

    -   On doit choisir 2 lettres parmi les 26 que contient l'alphabet,
        et 3 chiffres parmi les entiers de 0 à 9. Donc, le nombre de
        plaques d'immatriculation possible est:

    ```{r}
    choose(26,2)*choose(10,3)
    ```

### Exercice 4

Considérez le diagramme suivant pour les questions suivantes:

![Diagramme de Venn](./images/Diag_Venn.png) a. Quelle est donc la
probabilité qu'un étudiant s'inscrive à un cours de Physique (B), OU à
un cours de Chimie (C)? - Solution: - Les événements ne sont pas
mutuellement exclusifs. Donc\
$$P(B\cup C)=P(B)+P(C)−P(B\cap C)=100/200+120/200−60/200=160/200=0.80$$

b.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique ET de Chimie?\

-   Solution:

    -   60 étudiants s'inscrivent à ces 2 cours: 60/200 = 0.30

c.  Quelle est la probabilité qu'un étudiant ne s'inscrive pas à un
    cours de Stats?

-   Solution:

    -   70 étudiants s'inscrivent au cours de Physique mais pas au cours
        de Stats; 100 étudiants s'inscrivent au cours de Chimie mais pas
        au cours de Stats; et 40 étudiants s'inscrivent aux cours de
        Physique et de Chimie, mais pas au cours de Stats. Donc on a:
        $$70/200+100/200−40/200=130/200=0.65$$

d.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Chimie, mais pas à un cours de Stats?

-   Solution:

    -   120 étudiants sont inscrits au cours de Chimie, mais 20 de
        ceux-ci sont aussi inscrits au cours de Stats. Donc la
        probabilité recherchée est 100/200 = 0.5

e.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique et à un cours de Chimie, mais pas à un cours de Stats?

-   Solution:

    -   Cette question est une simple reformulation de la question (c)

f.  Quelle est la probabilité qu'un étudiant s'inscrive à 2 cours
    seulement?

-   Solution:

    -   30 étudiants sont inscrits aux cours de Physiques et de Stats,
        mais de ce nombre, 20 sont aussi inscrits au cours de Chimie
    -   20 étudiants sont inscrits aux cours de Chimie et de Stats, mais
        ces étudiants sont aussi inscrits au cours de Physique
    -   60 étudiants sont inscrits aux cours de Physique et Chimie, mais
        20 de ces étudiants sont aussi inscrits au cours de Stats.
    -   Donc la probabilité recherchée est
        (30-10)+(20-20)+(60-20)=50/200 = 0.25

g.  Quelle est la probabilité qu'un étudiant ne s'inscrive qu'à un seul
    cours?

-   Solution:

    -   Du diagramme on note: (30+60+30)/200 = 120/200 = 0.60

h.  Quelle est la probabilité qu'un étudiant s'inscrive aux trois cours?

-   Solution:

    -   20 étudiants sont inscrits aux trois cours: 20/200 = 0.10

i.  Quelle est la probabilité qu'un étudiant ne soit encore inscrit à
    aucun cours?

-   Solution:

    -   10 étudiants ne sont inscrits à aucun cours (énoncé du
        problème). Donc la probabilité recherchée est 10/200 = 0.05

j.  Quelle est la probabilité qu'un étudiant s'inscrive à un cours de
    Physique, OU à un cours de chimie, mais pas les deux à la fois?

-   Solution:

    -   40 étudiants sont inscrits au cours de Physique, mais pas au
        cours de Chimie;
    -   60 étudiants sont inscrits au cours de Chimie, mais pas au cours
        de Physique;
    -   Donc (40+60)/200 = 0.50

k.  Quelle est la probabilité qu'un étudiant s'inscrive soit au cours de
    Stats uniquement, soit au cours de Physique ET au cours de Chimie
    (mais pas au cours de Stats)?

-   Solution:

    -   P(Stats uniquement) = 30/200 = 0.15
    -   P(Physique et Chimie et pas Stats) = 40/200 = 0.20
    -   Probabilité recherchée = 0.15 + 0.20 = 0.35

l.  Est-ce que l'inscription à un cours de Physique est indépendante
    d'une inscription à un cours de Chimie?
    $$ P(P \cap C)=0.20~~~~P(P)=0.5~~~~P(C)=0.60~~~~~P(P) \times P(C)=0.30$$
    Ces deux événements ne sont pas indépendants!

m.  Est-ce que l'inscription à un cours de Physique est indépendante
    d'une inscription à un cours de Stats?
    $$ P(P \cap S)=0.15~~~~P(P)=0.50~~~~P(S)=0.30~~~~~P(P) \times P(C)=0.15$$
    Ces deux événements sont indépendants

n.  Est-ce que l'inscription à un cours de Chimie est indépendante d'une
    inscription à un cours de Stats?

$$ P(C \cap S)=0.10~~~~P(C)=0.60~~~~P(S)=0.30~~~~~P(C) \times P(S)=0.18$$
Ces deux événements ne sont pas indépendants

### Exercice 5

1.  Déterminez la distribution de fréquence des résultats (nombre de
    FACES), lorsque le nombre de pièces de monnaie est n = 100, et que
    les pièces ne sont pas trafiquées (soit p = 0.5).

-   Solution:

    -   Par simulation, créons la distribution de probabilités du lancer
        de 100 pièces de monnaie. Répétons l'échantillonnage 10,000
        fois:

```{r echo=FALSE}
CoinToss = function(n=1,p=0.5){
  coin = c("Pile","Face")
  x = sample(coin,n,c(p,1-p),replace=TRUE)
  return(x)
}
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
res = vector()    # Initialisation du vecteur des résultats
No_essais = 10000 # Spécification du nombre d'essais à effectuer
for(i in 1:No_essais){
  x = CoinToss(100,0.5)
  res[i] = sum(x=="Face")
}
fr = as.matrix(table(res))
pr = fr[,1] / No_essais
x = sort(unique(res))
tbl = data.frame(x,fr,pr)
espm=sum(x*pr)
colnames(tbl) = c("#Faces","Fréq","Prop")
hist(res+.001,xlab="Nombre de Faces",ylab="Fréquences",main="Distribution de probabilités: 5 pièces")
kable(tbl, booktabs=TRUE, caption="Distribution du nombre de Faces",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
cat("Espérance Mathématique = ",espm)
```

```{r echo=FALSE}
kable(tbl, booktabs=TRUE, caption="Distribution du nombre de Faces",row.names=FALSE) %>%
  kable_styling(full_width = FALSE)
```

2.  Comment pourriez-vous qualifier la distribution obtenue?

-   Elle est symétrique
-   les valeurs inférieures à 30 et supérieure à 70 sont très
    improbables
-   Le centre de la distribution semble se trouver autour de 50 FACEs,
    et de fait, pour cette simulation, l'espérance mathématique est
    égale à:

```{r edho=FALSE}
espm
```

3.  Obtenez la proportion des essais où on obtient plus de 65 FACES, en
    utilisant le tableau de fréquences.

-   Solution:

```{r}
sum(pr[(x>=65)])/length(x)
```

4.  Obtenez cette proportion en comptabilisant cette proportion
    directement des résultats de chaque essai.

-   Solution:

    -   La variable contenant l'ensemble des résultats obtenus à chaque
        essai est **res**. Le code R suivant cherche les valeurs de
        **res** supérieures à 65:

    ```{r}
    cat("La probabilité d'obtenir 65+ FACEs = ",sum(res>=65)/length(res))
    ```

5.  Pour quelle proportion des essais obtient-on entre 65 et 85 FACES?

-   Solution:

```{r}
cat("La probabilité d'obtenir entre 65 et 85 FACEs = ",sum((res>=65)&(res<=85))/length(res))
```

6.  Sous quelle valeur de x (nombre de FACES) trouve-t'on 75% de la
    distribution?\

-   Solution:

    -   La position de la valeur de x sous laquelle on trouve 75% de la
        distribution est:

    ```{r}
    rc75 = round(0.75*length(res),0)
    ```

    -   Donc, en ordonnant les scores en ordre croissant, et en trouvant
        le score occupant cette position, on trouve:

    ```{r}
    sort(res)[rc75]

    ```

7.  Au-dessus de quelle valeur de x trouve-t'on 10% de la distribution?

-   Solution:

    -   De la même manière, en inversant la requête, on cherche la
        valeur de X sous laquelle se trouve 90% de la distribution:

```{r}
sort(res)[length(res)*0.9]
```

8.  Quelles sont les limites à l'intérieur desquelles on retrouve les
    95% centraux des résultats de lancers de ces 100 pièces de monnaie?

```{r}
sx = sort(res)
liminf = length(res)*.025
limsup = length(res)*0.975
cat("[",sx[liminf]," < X < ",sx[limsup],"] = 95%")
```

### Exercice 6

1.  Déterminez la distribution de fréquence des résultats observés en
    lançant un dé à 100 reprises, supposant que le dé n'est pas truqué.

-   Solution:

    -   Le code R permettant la simulation du lancer d'un dé est
        reproduit ci-dessous:

```{r echo=FALSE}
DiceRoll = function(n=1,p=rep(1,6)){
  dice = 1:6
  x = sample(dice,n,p,replace=TRUE)
  return(x)
}
```

```{r}
res = vector()  # Initialisation du vecteur des résultats
No_essais = 100  # Spécification du nombre d'essais à effectuer
res = DiceRoll(No_essais)
ft = as.matrix(table(res))
fr = ft/sum(ft)
f = data.frame(sort(unique(res)), ft, fr)
colnames(f) = c("Face", "Fréq", "Prop")

barplot(height = f$Fréq, names = f$Face, xlab = "Observations", 
    ylab = "Fréquences", main = "Répartition de résultats des lancers d'un dé non truqué")
```

2.  Comment pourriez-vous qualifier la distribution obtenue? A première
    vue, seriez-vous porté à croire que le dé est truqué?

-   Solution:

    -   À première vue, il semble que la probabilité d'obtenir un ou
        l'autre des résultats possibles (1-6) varie. S'il existe une
        différence suffisamment grande dans ces probabilités, on pourra
        peut-être croire que le dé est truqué. Mais le nombre de
        réplications est relativement faible!

3.  Répétez l'expérience en lançant le dé 300 fois, puis 1000 fois, et
    enfin 10,000 fois. Avez-vous des raisons de croire que le dé est
    truqué?

```{r}
for (i in c(300, 1000, 10000)) {
    res = vector()  # Initialisation du vecteur des résultats
    res = DiceRoll(i)
    ft = as.matrix(table(res))
    ft = data.frame(sort(unique(res)), ft, ft/sum(ft))
    colnames(ft) = c("Face", "Fréq", "Prop")
    titl = paste("Répartition de résultats des lancers d'un dé\n (", 
        i, "lancers)")
    barplot(height = ft$Fréq, names = ft$Face, xlab = "Observations", 
        ylab = "Fréquences", main = titl)
}
```

    - Avec un grand nombre de réplications, les différences entre les probabilités d'obtenir une face particulière s'estompent. Il devient de plus en plus difficile de croire que le dé est truqué.

4.  Quelle conclusion générale pourriez-vous tirer de ces expériences?\

-   Solution:

    -   La précision des estimations de probabilités augmente avec le
        nombre de réplications.

5.  Exécutez le code R suivant afin de définir un dé trafiqué de manière
    à ce que la probabilité d'obtenir une des faces du dé (1-6) soit
    différente de celle typique d'un dé non truqué, et pour obtenir un
    échantillon initial de 30 lancers de ce dé.

```{r}
No_essais = 30
p6 = sample(c(rep(1.6667, 5), 1.6667 + runif(1, -0.7, 0.7)),6)
res = DiceRoll(No_essais, p = p6)
ft = as.matrix(table(res))
ft = data.frame(sort(unique(res)), ft, ft/sum(ft))
colnames(ft) = c("Face", "Fréq", "Prop")
titl = paste("Répartition de résultats des lancers d'un dé truqué\n pour ", 
    No_essais, "lancers")
barplot(height = ft$Fréq, names = ft$Face, xlab = "Observations", 
    ylab = "Fréquences", main = titl)
```

6.  Êtes-vous en mesure de conclure sans doute raisonnable que le dé a
    été trafiqué, sur la base de ces données?\

-   Solution:

    -   Les probabilités semblent différer pour les différentes faces du
        dé. Mais le nombre d'essais est trop faible pour permettre une
        conclusion définitive...

7.  Répétez l'expérience en modifiant le nombre d'essais graduellement:
    60, 120, 300, 1,000, 3,000... À partir de quel moment auriez-vous
    confiance en affirmant que le dé est truqué, sur la base des
    observations?\

-   Soluton:

    -   Le code R est modifié afin de permettre la simulation impliquant
        différents nombres d'essais. Au besoin, vous pouvez ajouter des
        nombres d'essais au départ de la boucle **for()**:

```{r}
p6 = sample(c(rep(1.6667, 5), 1.6667 + runif(1, -0.7, 0.7)),6)
res=vector()
for(No_essais in c(60,120,300,1000,3000)){
  res = DiceRoll(No_essais, p = p6)
  ft = as.matrix(table(res))
  ft = data.frame(sort(unique(res)), ft, ft/sum(ft))
  colnames(ft) = c("Face", "Fréq", "Prop")
  titl = paste("Répartition de résultats des lancers d'un dé truqué\n pour ",No_essais, "lancers")
  barplot(height = ft$Fréq, names = ft$Face, xlab = "Observations",ylab = "Fréquences", main = titl)
}
```

8.  Pour quelle face le dé est-il trafiqué, et quelle est la probabilité
    d'obtenir ce résultat, à 5 décimales près? Il est possible que vous
    deviez utiliser un grand nombre d'essais pour le déterminer... Pour
    les données actuelles, le vecteur **p6** contient les valeurs:

```{r}
p6
```

Ces valeurs sont proportionnelles aux probabilités d'obtenir 1, 2, 3, 4,
5 ou 6, respectivement, en lançant le dé. En relançant l'exercice, vous
obtiendrez des résultats différents, la face truquée et les probabilités
varieront d'une fois à l'autre. Expérimentez!

### Exercice 7

1.  Pourriez-vous écrire un code R permettant de simuler le lancer de 10
    dés non-truqués et de calculer la somme des valeurs observées?
    Inscrivez votre code dans l'espace ci-dessous. Puis appliquez ce
    code dans la console.

-   Solution:

    -   Le code R pourrait ressembler à ceci (plusieurs façons
        existent!)

    ```{r}
    dice.roll <- sample(1:6, size = 10, replace = TRUE)
    dice.roll
    sum(dice.roll)
    ```

2.  Pourriez-vous écrire un code R permettant de lancer un seul dé à 20
    reprises? Utilisez l'espace ci-dessous pour ce faire, puis appliquez
    ce code dans la console.

-   Solution:

    -   Votre code R pourrait ressembler à ceci:

    ```{r}
    replicate(20,sample(1:6,size=1,replace=TRUE))
    ```

### Exercice 8

Puisque les données varient aléatoirement d'une expérience à l'autre,
relançons la simulation nécessaire à cet exercice:

```{r echo=FALSE}
two.dice <- function(){
  dice <- sample(1:6, size = 2, replace = TRUE)
  return(sum(dice))
}
```

```{r}
No_essais = 500  # Définition du nombre d'essais
x = replicate(No_essais, two.dice())
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)), ft, ft/sum(ft))
colnames(ft) = c("Somme", "Fréq", "Prop")
kable(ft, booktabs = TRUE, caption = "Résultats du lancer de 2 dés", 
    row.names = FALSE) %>%
    kable_styling(full_width = FALSE)
```

**Les données à examiner se trouvent dans la variable x**

1.  En vous basant sur la dernière distribution obtenue suite à 500
    lancers de deux dés, quelle serait la probabilité d'obtenir une
    somme supérieure à 10?

```{r}
sum(x>10)/length(x)
```

2.  Toujours en vous basant sur la distribution obtenue, quelle serait
    la probabilité d'obtenir une somme supérieure à 10 ou inférieure à
    5?

```{r}
sum((x>10)|(x<5))/length(x)
```

3.  Quelle serait la probabilité d'obtenir une somme comprise entre 6 et
    8?

```{r}
sum((x<=8)&(x>=6))/length(x)
```

4.  Quelle est la probabilité d'obtenir au plus une somme égale à 6?

```{r}
sum(x<=6)/length(x)
```

5.  Quelle est la probabilité d'obtenir au moins une somme égale à 6?

```{r}
sum(x>=6)/length(x)
```

###  Exercice 9

Relançons la simulation en utilisant 500,000 réplications (si vous avez
un peu plus de temps, essayez 1 millions de réplications).

```{r}
No_essais = 500000  # Définition du nombre d'essais
x = replicate(No_essais, two.dice())
ft = as.matrix(table(x))
ft = data.frame(sort(unique(x)), ft, ft/sum(ft))
colnames(ft) = c("Somme", "Fréq", "Prop")
kable(ft, booktabs = TRUE, caption = "Résultats du lancer de deux dés", 
    row.names = FALSE) %>%
    kable_styling(full_width = FALSE)
```

1.  Dans cette distribution, quelle est la probabilité d'obtenir une
    somme supérieure à 10?

-   Solution:

```{r}
sum(x>10)/length(x)
```

2.  Quelle serait la probabilité d'obtenir une somme supérieure à 10 ou
    inférieure à 5?

-   Solution:

```{r}
sum((x>10)|(x<5))/length(x)
```

3.  Quelle serait la probabilité d'obtenir une somme comprise entre 6 et
    8?

-   Solution:

```{r}
sum((x>=6)&(x<=8))/length(x)
```

4.  Quelle est la probabilité d'obtenir une somme inférieure ou égale à
    6?

-   Solution:

```{r}
sum(x<=6)/length(x)
```

5.  Quelle est la probabilité d'obtenir une somme supérieure ou égale à
    6?

-   Solution:

```{r}
sum(x>=6)/length(x)
```

6.  Comparez ces trois probabilités à celles obtenues dans l'exercice 3.
    Quel ensemble de probabilités vous semble le plus précis?

-   Solution:

    -   La simulation impliquant le plus grand nombre de réplications
        sera toujours le plus précis.

7.  Considérez une modification de la fonction two.dice() qui retourne
    la valeur des deux dés, plutôt que leur somme:

```{r}
two.dice2 <- function(){
  dice <- sample(1:6, size = 2, replace = TRUE)
  return(dice)
}
```

-   En utilisant cette fonction et la fonction replicate(), obtenez la
    probabilité d'obtenir un double, soit (1,1), (2,2), ..., (6,6))?

    -   Solution:

    ```{r}
    nrep = 10000
    x = replicate(nrep,two.dice2())
    cat("La probabilité d'obtenir un double = ",sum(x[1,]==x[2,])/nrep)
    ```

8.  Quelle serait la probabilité qu'au moins un des deux dés soit un
    '6'?

-   Solution:

```{r}
cat("La probabilité d'obtenir au moins un 6 = ",sum((x[1,]==6)|(x[2,]==6)|((x[1,]==6)&(x[2,]==6)))/nrep)
```

9.  Il existe un vieux jeu de hasard italien appelé Passadieci, dans
    lequel le but est d'obtenir au moins 11 lorsque trois dés à six
    faces sont lancés. La fonction suivante vous permettra d'effectuer
    autant d'expériences aléatoires (lancers de 3 dés) que vous le
    désirez et d'obtenir la somme de ces trois dés:

```{r }
three.dice = function(){
  res = sample(1:6,3,replace = TRUE)
  return(sum(res))
}
```

a.  Utilisez cette fonction et la fonction replicate() pour simuler
    100,000 réplications de ce jeu et sauvegardez les résultats dans un
    vecteur appelé passadieci.

-   Solution:

```{r}
passadieci = replicate(100000,three.dice())
```

-   Utilisez ce vecteur pour répondre aux questions suivantes :

b.  Quelle est la probabilité de gagner à ce jeu (ie obtenir une somme
    supérieure ou égale à 11) ?

```{r}
sum(passadieci>=11)/length(passadieci)
```

c.  Qu'est-ce qui est le plus probable en lançant trois dés : obtenir 11
    ou obtenir 12 ?

```{r}
s11 = sum(passadieci==11)/length(passadieci)
s12 = sum(passadieci==12)/length(passadieci)
if(s11<s12){cat("Obtenir 12 est plus probable")
}else{cat("Obtenir 11 est plus probable")}
```

d.  Quelle est la probabilité d'obtenir une somme inférieure ou égale à
    7 ou supérieure ou égale à 15, toujours en lançant nos 3 dés?

```{r}
  sum((passadieci<=7)|(passadieci>=15))/length(passadieci)
```

e.  Tracez un graphique pour illustrer l'ensemble de la distribution des
    probabilités.

```{r}
hist(passadieci)
```

10. Parmi les deux situations suivantes, laquelle est la plus probable?

    1.  obtenir au moins un 6 en lançant 4 fois un seul dé à six faces.
    2.  obtenir au moins une paire de 6 en lançant 24 fois deux dés à
        six faces.

    -   Vous devez donc effectuer deux simulations de manière à
        déterminer la probabilité d'obtenir au moins un '6' en lançant 4
        dés. La fonction suivante pourrait s'avérer utile:

```{r}
SituationA <- function(){
  rolls <- sample(1:6, size = 4, replace = TRUE)
  res1 <- sum(rolls == 6) > 0
  return(res1)
}
```

    - la probabilité d’obtenir au moins une paire de 6 en lançant 24 fois deux dés. La fonction suivante devrait pouvoir faire le travail:

```{r}
SituationB <- function(){
  d1 <- sample(1:6, size = 24, replace = TRUE)  # Deux dés sont lancés 
  d2 <- sample(1:6, size = 24, replace = TRUE)  # 24 fois
  res2 <- sum((d1 == d2) & (d1 == 6)) > 0       # Condition recherchée
  return(res2)
}
```

    - Pour répondre à la question, il suffira d’utiliser la fonction replicate() pour chacune de ces deux fonctions pour établir la distribution de probabilités correspondant aux deux situations. On pourra alors déterminer laquelle des situations est la plus probable!

-   Solution:

```{r}
x = replicate(10000,SituationA())
p1 = sum(x)/length(x)
y = replicate(10000,SituationB())
p2 = sum(y)/length(y)
cat("Probabilité de la situation A = ",p1)
cat("Probabilité de la situation B = ",p2)
if(p1<p2){cat("La situation B est plus probable!")
  }else{cat("La situation A est plus probable!")}
```

### Exercice 10

Pour les exercices suivants, utilisez l'approche classique et la
simulation pour obtenir les réponses:

1.  Sous le balcon de Juliette, Roméo cueille des fleurs pour lui lancer
    un bouquet. Le parterre de roses (la fleur) comporte 4 fleurs
    rouges, 3 fleurs jaunes et 5 fleurs roses. Roméo procédant dans
    l'obscurité, il ne peut pas contrôler la couleur des fleurs qui est
    donc considérée comme aléatoire.

```{=html}
<!-- -->
```
a.  Si Roméo cueille trois fleurs, quelle est la probabilité qu'il
    obtienne trois fleurs de couleur rose ?

-   Solution:

    -   Classique: $$nS = \binom{12}{3}=\frac{12!}{3!(12-3)!}=220$$
        $$nE = \binom{5}{3}=\frac{5!}{3!(5-3)!}=10$$
        $$P(3 Roses)=\frac{10}{220}=0.045$$

    -   Simulation:

        -   Utilisons la fonction PolyUrn(), modifié pour un contexte
            bucolique:

        ```{r}
        polyUrn = function(n=20, R=5, j=7, r=10, repl=FALSE){
          # n: size of draws
          # R: Nombre de fleurs rouges
          # j: Nombre de fleurs jaunes
          # r: Nombre de fleurs roses
          urn = c(rep("Rouge", R), rep("Jaune", j), rep("Rose", r))
          draw = sample(urn, size=n, replace=repl)
          x = c(sum(draw=="Rouge"), sum(draw == "Jaune"), sum(draw == "Rose"))
          return(x)
        }
        ```

        Le code R suivant procède en échantillonnant à 10,000 reprises
        l'espace échantillonnal:

```{r}
ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 3         # Nombre de fleurs formant le bouquet
R = 4         # Nombre de fleurs rouges
j = 3         # Nombre de fleurs jaunes
r = 5         # Nombre de fleurs roses
E = c(0,0,3)  # Événement recherché (R = 0, j = 0, r = 3)
repl = FALSE
succes = 0    # Initialisation des variables servant au comptage
i = 0         #
repeat{
  draw = polyUrn(n,R,j,r,repl)    # Exécution d'une expérience
  i = i + 1
  if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
  if (i == ndraw){break}                  # Exercice complété?
}
cat("Probabilité = ",succes/ndraw)
```

b.  Si Roméo cueille trois fleurs, quelle est la probabilité qu'il
    obtienne une fleur de chaque couleur ? - Solution:

    -   Classique: $$nS = 220$$
        $$nE = \binom{4}{1}+\binom{3}{1}+\binom{5}{3}=(3)(4)(5)=60$$
        $$P(E)=\frac{60}{220}=0.273$$

    -   Simulation:

    ```{r}
    ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
    n = 3         # Nombre de fleurs formant le bouquet
    R = 4         # Nombre de fleurs rouges
    j = 3         # Nombre de fleurs jaunes
    r = 5         # Nombre de fleurs roses
    E = c(1,1,1)  # Événement recherché (R = 1, j = 1, r = 1)
    repl = FALSE
    succes = 0    # Initialisation des variables servant au comptage
    i = 0         #
    repeat{
      draw = polyUrn(n,R,j,r,repl)    # Exécution d'une expérience
      i = i + 1
      if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
      if (i == ndraw){break}                  # Exercice complété?
    }
    cat("Probabilité = ",succes/ndraw)
    ```

c.  Si Roméo cueille quatre fleurs, quelle est la probabilité qu'il
    n'obtienne aucune fleur jaune ?

-   Solution:

    -   Classique: $$nS = \binom{12}{4}=\frac{12!}{4!(12-3)!}=495$$
        $$nE = \binom{9}{4}=126$$ $$P(E)=\frac{126}{495}=0.255$$
    -   Simulation:

    ```{r}
    ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
    n = 4         # Nombre de fleurs formant le bouquet
    R = 4         # Nombre de fleurs rouges
    j = 3         # Nombre de fleurs jaunes
    r = 5         # Nombre de fleurs roses
    E = c(1,0,1)  # Événement recherché (R = 1, j = 1, r = 1)
    repl = FALSE
    succes = 0    # Initialisation des variables servant au comptage
    i = 0         #
    repeat{
      draw = polyUrn(n,R,j,r,repl)    # Exécution d'une expérience
      i = i + 1
      if(draw[2]==0){succes=succes+1}     # Est-ce un 'succès'?
      if (i == ndraw){break}              # Exercice complété?
    }
    cat("Probabilité = ",succes/ndraw)
    ```

d.  Si Roméo cueille deux fleurs, quelle est la probabilité qu'il
    obtienne des fleurs de couleurs différentes ?

-   Solution:

    -   Classique: $$nS = \binom{12}{2}=\frac{12!}{2!(12-2)!}=66$$
        $$nE = \binom{4}{1}\times\binom{3}{1}+ \binom{4}{1}\times\binom{5}{1}+\binom{3}{1}\times\binom{5}{1}=47$$
        $$P(E)=\frac{47}{66}=0.712$$

    -   Simulation:

    ```{r}
    ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
    n = 2         # Nombre de fleurs formant le bouquet
    R = 4         # Nombre de fleurs rouges
    j = 3         # Nombre de fleurs jaunes
    r = 5         # Nombre de fleurs roses
    E = c(1,0,1)  # Événement recherché (R = 1, j = 1, r = 1)
    repl = FALSE
    succes = 0    # Initialisation des variables servant au comptage
    i = 0         #
    repeat{
      draw = polyUrn(n,R,j,r,repl)    # Exécution d'une expérience
      i = i + 1
      succes=succes+!is.element(2,draw)   # Est-ce un 'succès'?
      if (i == ndraw){break}              # Exercice complété?
    }
    cat("Probabilité = ",succes/ndraw)
    ```

2.  En utilisant la fonction polyUrn() et le code défini plus haut,
    étudiez une situation dans laquelle l'urne contient 15 balles
    rouges, 7 balles bleues et 20 balles vertes. Répétez les exercices
    en variant le nombre d'expériences aléatoires.

a.  Quelle est la probabilité, en tirant 10 balles, que cet échantillon
    soit composé de 3 balles rouges, 4 balles bleues et 3 balles
    vertes? - Solution:

    -   Simulation: Le code R permettant de simuler cette situation est
        reproduit ci-dessous:

```{r}
ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 10        # Nombre de balles tirées lors d'une expérience
r = 15        # Nombre de balles rouges
b = 7         # Nombre de balles bleues
g = 20        # Nombre de balles vertes
E = c(3,4,3)  # Événement recherché (r = 3, b = 4, g = 3)
repl = FALSE
succes = 0    # Initialisation des variables servant au comptage
i = 0         #
repeat{
  draw = polyUrn(n,r,b,g,repl)    # Exécution d'une expérience
  i = i + 1
  if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
  if (i == ndraw){break}                  # Exercice complété?
}
cat("Probabilité = ",succes/ndraw)
```

      - Approche classique:  

      $$nS = \binom{42}{10}=\frac{42!}{10!(42-10)!}=$$

```{r}
nS = choose(42,10)
cat("nS = ",nS)
```

      $$nE = \binom{15}{3}\times\binom{7}{4}\times \binom{20}{3}=$$

```{r}
nE = choose(15,3)*choose(7,4)*choose(20,3)
cat("nE = ",nE)

cat("P(E) = nE/nS =",nE/nS)
```

b.  Quelle est la probabilité, en tirant 10 balles, que cet échantillon
    soit composé de 2 balles rouges, d'aucune balle bleue et d2 8 balles
    vertes?

    -   Solution:
    -   Simulation: Le code R permettant de simuler cette situation est
        reproduit ci-dessous:

```{r}
ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 10        # Nombre de balles tirées lors d'une expérience
r = 15        # Nombre de balles rouges
b = 7         # Nombre de balles bleues
g = 20        # Nombre de balles vertes
E = c(2,0,8)  # Événement recherché (r = 8, b = 2, g = 0)
repl = FALSE
succes = 0    # Initialisation des variables servant au comptage
i = 0         #
repeat{
  draw = polyUrn(n,r,b,g,repl)    # Exécution d'une expérience
  i = i + 1
  if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
  if (i == ndraw){break}                  # Exercice complété?
}
cat("Probabilité = ",succes/ndraw)
```

    - Solution: Approche classique:  

      $$nS = \binom{42}{10}=\frac{42!}{10!(42-10)!}=$$

```{r}
nS = choose(42,10)
cat("nS = ",nS)
```

      $$nE = \binom{15}{2}\times \binom{20}{8}=$$

```{r}
nE = choose(15,2)*choose(20,8)
cat("nE = ",nE)

cat("P(E) = nE/nS =",nE/nS)
```

      $$P(E)=\frac{47}{66}=0.712$$
       

c.  Quelle est la probabilité qu'un échantillon de 12 balles ne soit
    composé que de balles vertes?

    -   Solution:
    -   Simulation: Le code R permettant de simuler cette situation est
        reproduit ci-dessous:

```{r}
ndraw=100000  # Nombre d'expériences aléatoires (tirage de balles)
n = 12        # Nombre de balles tirées lors d'une expérience
r = 15        # Nombre de balles rouges
b = 7         # Nombre de balles bleues
g = 20        # Nombre de balles vertes
E = c(0,0,12)  # Événement recherché (r = 8, b = 2, g = 0)
repl = FALSE
succes = 0    # Initialisation des variables servant au comptage
i = 0         #
repeat{
  draw = polyUrn(n,r,b,g,repl)    # Exécution d'une expérience
  i = i + 1
  if(sum(draw==E)==3){succes=succes+1}    # Est-ce un 'succès'?
  if (i == ndraw){break}                  # Exercice complété?
}
cat("Probabilité = ",succes/ndraw)
```

    - Solution: Approche classique:  

      $$nS = \binom{42}{12}=\frac{42!}{12!(42-12)!}=$$

```{r}
nS = choose(42,12)
cat("nS = ",nS)
```

      $$nE = \binom{20}{12}=$$

```{r}
nE = choose(20,12)
cat("nE = ",nE) 

cat("P(E) = nE/nS =",nE/nS)
```

### Exercice 11

Utilisez la fonction BlackJack() afin de déterminer les probabilités:

```{r echo=FALSE}

library(dplyr)
library(tidyr)

BlackJack = function(sampling_number=30){
  card_suits <- c("Pique", "Coeur", "Carreau", "Trèfle")
  card_names <- c("As", "Roi", "Reine", "Valet", "10", "9", "8", "7", "6", "5", "4", "3", "2")
  card_values <- c(1, 10, 10, 10, 10:2)
  n_suits <- length(card_suits)
  card_deck <- expand.grid(card_names, card_suits)
  card_deck <- card_deck %>%
              unite("Cards", c(1, 2), sep = "_", remove = TRUE) %>%
              mutate(Values = rep(card_values, n_suits))
  sample_sums = rep(NA,sampling_number)
  cards = rep(NA,sampling_number)
  for(i in 1:sampling_number){
    cards[i] <- list(sample(card_deck$Cards, size = 3, replace = FALSE))
    Card_val = card_deck$Values[card_deck$Cards %in% cards[[i]]]
    sample_sums[i] <- sum(Card_val)+10*(sum(Card_val==1)>0 & sum(Card_val)<=11)
  }
  return(sample_sums)
}
```

1.  d'obtenir une somme supérieure à 18.

-   Solution:

```{r}
nrep = 10000
x = BlackJack(nrep)
cat("P(x>18) = ",sum(x>18)/nrep)
```

2.  d'obtenir une somme inférieure à 7.

-   Solution:

```{r}
nrep = 10000
x = BlackJack(nrep)
cat("P(x<7) = ",sum(x<7)/nrep)
```

3.  une somme comprise entre 10 et 14.

-   Solution:

```{r}
nrep = 10000
x = BlackJack(nrep)
cat("P[(x>10)&(x<14)) = ",sum((x>10)&(x<14))/nrep)
```

<!--chapter:end:02-Chap2.Rmd-->

---
title: "MAT104 - Cours 4 - Statistiques Descriptives"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
---


```{r 03-Chap3-1, include = FALSE}
knit_print.data.frame = function(x, ...) {
  res = paste(c("", "", knitr::kable(x)), collapse = "\n")
  knitr::asis_output(res)
}

registerS3method(
  "knit_print", "data.frame", knit_print.data.frame,
  envir = asNamespace("knitr")
)
```

```{r 03-Chap3-2, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```

```{r 03-Chap3-3, include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r 03-Chap3-4,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(wakefield,moments,DescTools,plyr,dplyr,tidyr,formatR,kableExtra,psych,agrmt,mnonr,lattice,modeest,multimode)
```
# Statistiques Descriptives

## Objectifs d'apprentissage

À la fin de l'étude de ce chapitre, vous devriez être en mesure de:

-   pour chacun des types de données (qualitatives et quantitatives):

    -   Générer des ensembles de données aléatoires
    -   Produire des graphiques appropriés
    -   Obtenir les indices de tendance centrale appropriés
    -   Obtenir les indices de dispersion appropriés
    -   Décrire la forme de la distribution, qualitativement et numériquement

## Introduction

Dans les sections précédentes, nous avons rencontré des distributions de probabilités permettant de résumer le 'comportement' d'une variable donnée en indiquant pour chacune des valeurs possibles, la probabilité d'occurrence.

Dans un contexte de recherche, on obtient des observations pour plusieurs variables et il est d'usage, dans un premier temps suivant la collecte des données, d'étudier les principales caractéristiques de l'ensemble des données. Cet exercice peut n'impliquer qu'une variable à la fois (analyse univariée), des paires de variables (analyse bivariée), ou un certain nombre de variables (analyse multivariée). Dans cette section, nous présenterons les différents indices permettant de quantifier ces caractéristiques.

Pour cette section, nous utiliserons les librairies suivantes:

```{r 03-Chap3-5,message=FALSE}

library(moments)
library(psych)
library(DescTools)
library(wakefield)
library(plyr)
library(dplyr)
library(kableExtra)
library(agrmt)
library(mnonr)
library(multimode)
library(modeest)
```

## Données qualitatives

Les données qualitatives peuvent prendre l'une ou l'autre de deux formes: nominales et ordinales. Dans le premier cas, les attributs de la variable sont des catégories mutuellement exclusives entre lesquelles aucun ordre n'existe. Tous les arrangements des différents attributs (différentes catégories) sont parfaitement équivalentes. La seule opération qu'il est possible d'effectuer sur ce type de données est la comptabilisation des effectifs pour chacune des catégories.

Dans le second cas, la variable possède différents attributs, comme pour les variables nominales, mais ces attributs sont ordonnés: un attribut a quelque chose de plus que le précédent, et quelque chose de moins que le suivant. Par contre, la distance qui sépare ces attributs n'est pas spécifié. La seule information dont on dispose est qu'une donnée est plus grande ou plus petite qu'une autre.

Ces deux types de données sont fréquents dans la littérature scientifique. Dans les paragraphes suivants, nous examinons diverses méthodes permettant d'en fournir une description.

### Données nominales: Génération

Dans un contexte de recherche, les données qualitatives (échelle nominale ou ordinale) sont très fréquentes. Leur examen est plus simple que dans le cas de données quantitatives, mais l'intérêt de cet examen demeure.

La génération de données nominales aléatoires est simple. Il suffit de définir les catégories de cette variable, de spécifier les probabilités d'occurrence de chacune d'elles, et d'utiliser la fonction **sample()** pour compléter le travail. Supposons un ensemble de 500 observations nominales réparties en 5 catégories nommées A, B, C, D et E, dont les probabilités d'occurrence sont respectivement .3,.1,.2,.15, et .25. La commande suivante est tout de dont nous avons besoin pour accomplir cette tâche:

```{r 03-Chap3-6}
x = sample(LETTERS[1:5], 500, replace=TRUE,prob=c(.3,.1,.2,.15,.25))
head(x,10)
```

La librairie **wakefield** permet de générer des données tant qualitatives que quantitatives, pour une vaste variété de types de variables. Par exemple, générons des données concernant le niveau d'éducation:

```{r 03-Chap3-7, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(wakefield)
ed = education(20,x = c("Aucune", "Maternelle","Fondam1", "Fondam2","Lycée", "Baccaul","Maitrise", "Doctorat"),prob = c(0.013, 0.05, 0.261, 0.246, 0.165, 0.064, 0.09, 0.075),name = "Education")
ed

```

Il en va de même pour le statut marital:

```{r 03-Chap3-8, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
mar = marital(20,c("Marié", "Divorcé", "Veuf", "Séparé", "Célibataire"), prob = NULL,name = "Marital")
mar

```

La librairie **wakefield** offre des simulateurs de données pour près de 100 variables distinctes. On en trouvera la liste en cliquant [ICI](https://rdrr.io/cran/wakefield/man/)

### Données nominales: Tableau et représentation graphique

Afin de résumer un ensemble de données nominales, la production d'un tableau affichant les effectifs de chaque catégorie est de mise. Sous R, on peut utiliser la fonction **table()** à cette fin:

```{r 03-Chap3-9}
tbl = table(x)
tbl
```

Par la suite, on peut donner une forme plus efficace pour les besoins de communication::

```{r 03-Chap3-10}
tblm=as.matrix(tbl) # Conversion du tableau en matrice
kable(tblm, booktabs=TRUE, caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)
```

La fonction **as.matrix()** définit une matrice de données correspondant au type d'argument requis par la fonction **kable()**.

Un diagramme en barres est une représentation graphique appropriée dans le cas d'une variable nominale. La fonction **barplot()** permet la production d'un tel diagramme:

```{r 03-Chap3-11}
barplot(tbl,main="Répartition d'une variable nominale",xlab="Catégories",ylab="Fréquences")
```

De manière alternative, un diagramme en secteurs peut représenter ce type de données.

```{r 03-Chap3-12, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
slices <- tbl
lbls <- LETTERS[1:5]
pie(slices, labels = paste(lbls," (",slices,")"), main="Diagramme en secteurs\n (Données brutes)")
```

Pour ce type de graphique, par contre, on a souvent avantage à utiliser les proportions ou les pourcentages occupées par chaque secteur, plutôt que les fréquences absolues:

```{r 03-Chap3-13}
slices <- tbl
lbls <- LETTERS[1:5]
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls," (", pct,"%)",sep="") 
pie(slices, labels = lbls, main="Diagramme en secteurs\n (Pourcentages)")
```

### Données nominales: Tendance centrale

Dans le cas de données nominales, la seule mesure de tendance centrale qu'il est possible d'utiliser est le **mode**. Il s'agit simplement de la catégorie dont l'effectif est le plus élevé. On parlera de **catégorie modale**. L'examen du tableau de fréquence en permet la détermination. 

La fonction suivante permet de déterminer le mode d'un ensemble de données nominales:

```{r 03-Chap3-14}
ModeCat = function(x) {
   uniqx <- unique(x)
   m = uniqx[which.max(tabulate(match(x, uniqx)))]
   fm = sum(x==m)
   return(c(m,fm))
}
```
Cette fonction retourne un vecteur composé de deux items: la catégorie et la fréquence modale. Par exemple:

```{r 03-Chap3-15}
x=round(sample(1:10,200,replace=TRUE))
mo = ModeCat(x)
table(x)
cat("Catégorie modale = ",mo[1])
cat("Fréquence Modale = ",mo[2])
```
Il arrive parfois que la répartition d'un ensemble de données nominales présente plus qu'un mode. Dans un tel cas, une description des données ne peut omettre la détermination de l'ensemble des modes. Certaines fonctions nous permettent d'examiner ces situations. Par exemple, la fonction **Mode()**, de la librairie **DescTools** pourrait s'avérer utile.

Supposons une variable nominale comportant 7 catégories, numérotées de 1 à 7. Supposons également que la répartition des données est unimodale. Le code R suivant permet de générer une telle variable, pour un total de N = 50 observations:

```{r 03-Chap3-16}
library(DescTools)
x = sample(1:7,50,replace=TRUE,prob=c(1,2,3,4,3,2,1))
tbl = table(x)
tbl
mode = Mode(x)
nomode = length(mode)+1
distmode = c("Uniforme (sans mode)","Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}else if(nomode<1){nomode=0}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode])
#
# Représentation graphique
clrs =c(rep("grey",7))
clrs[mode] = "red"
barplot(tbl, col = clrs)
legend("topright", "Mode", fill = "red")

```

Une répartition sera bimodale, ou multimodale, si l'effectif le plus élevé apparait dans 2+ catégories. Par exemple:

```{r 03-Chap3-17, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
x = c(rep(1,2),rep(2,8),rep(3,3),rep(4,4),rep(5,8),rep(6,2),rep(7,5))
tbl = table(x)
tbl
mode = Mode(x)
nomode = length(mode)+1
distmode = c("Uniforme (sans mode)","Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}else if(nomode<1){nomode=0}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode])
#
# Représentation graphique
clrs =c(rep("grey",7))
clrs[mode] = "red"
barplot(tbl, col = clrs)
legend("topright", "Mode", fill = "red")
```

La fonction **Mode()** utilisée ci-dessus permet donc de mettre en évidence la présence de plusieurs modes dans une distribution. Par contre, ne seront considérés comme modes uniquement les valeurs dont les fréquences sont identiques, comme c'était le cas dans le dernier exemple. En pratique, une distribution telle que la suivante serait très probablement considérée comme **bimodale**, même si les fréquences modales ne sont pas identiques. :

```{r 03-Chap3-18, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
x = c(rep(1,2),rep(2,8),rep(3,3),rep(4,4),rep(5,10),rep(6,2),rep(7,4))
tbl = table(x)
tbl
barplot(tbl)
```

La fonction **modes()**, de la librairie **agrmt** déterminera l'existence de catégories modales dont les fréquences diffèrent. L'argument **tolerance** permet d'ajuster le degré de 'ressemblance' à partir duquel on considère que les fréquences sont égales. Pour les données que nous venons de générer,supposant qu'on considère qu'une différence de 2 unités n'est pas suffisamment grande pour qu'on considère deux fréquences comme étant différentes (tolerance=2), on obtient:

```{r 03-Chap3-19}
library(agrmt)
md = modes(tbl,tolerance=2)
md
cat("Fréquence(s) Modale(s) = ",tbl[md$mode])
clrs =c(rep("grey",7))
clrs[md$mode] = "red"
barplot(tbl, col = clrs)
legend("topright", "Mode", fill = "red")
```

On détecte `r length(md$mode)` modes, respectivement aux positions `r length(md$positions)`, et ces modes `r if(md$contiguous==FALSE){"ne sont pas"}else{"sont "} ` adjacents. Les fréquences modales sont respectivement `r tbl[md$positions]`.


Finalement, la fonction **secondModes()**, toujours de la librairie **agrmt**, permet de mettre en évidence les deux principaux modes. Pour l'exemple précédent, on a:

```{r 03-Chap3-20}
tbl
md = secondModes(tbl)
paste("Premier mode: Catégorie ",md$positions[[1]])
paste("Second  mode: Catégorie ",md$positions[[2]])
clrs =c(rep("grey",7))
clrs[md$positions[[1]]] = "red"
clrs[md$positions[[2]]] = "red"
barplot(tbl, col = clrs)
legend("topright", "Mode", fill = "red")
```

#### Exercice 1

1.  Chargez la librairie **wakefield**:

```{r 03-Chap3-21}
library(wakefield)
```

2.  Utilisez la fonction **employment()** pour générer le statut d'emploi de 500 personnes. Les proportions pour chaque catégories sont estimées à 50%, 15%, 10%, 15% et 10%, respectivement. La variable qui sera générée portera le nom: EMPLOI. Éditez le code R ci-dessous pour obtenir ce résultat

```{r 03-Chap3-22, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
n = 500
x = employment(n,x = c("Plein Temps", "Temps Partiel", "Sans Emploi", "Retraité", "Étudiant"), prob = c(0.6, 0.1, 0.1, 0.1, 0.1), name = "Variable")
```

3.  Obtenez un diagramme en barres pour représenter ces données. Notez que vous devez obtenir le tableau de fréquences au préalable.

4.  Construisez un diagramme en secteur pour représenter les proportions des différents statuts d'emploi.

5.  Déterminez le mode de cette distribution, en utilisant la fonction **Mode()** de la librairie **DescTools**. Dans ce cas, le mode est déterminé à partir des données originales (**x**). Puis, refaites le travail en utilisant la fonction **modes()** de la librairie **agrmt**. Dans ce cas, le mode est calculé à partir du tableau de fréquences.

### Données **ordinales**: Génération et tendance centrale

Dans le cas de données ordinales, la tendance centrale peut être évaluée à l'aide du mode, tel que défini dans la section précédente, ou en obtenant la médiane, qui est située exactement au centre de la séquence ordonnée des scores. Elle correspond à la donnée au-dessous de laquelle on retrouve 50% de la distribution. Elle coupe la distribution en deux parties égales.

Sous R, la fonction **median()** retourne la médiane du vecteur correspondant à son argument. Le code R suivant permettra d'illustrer la procédure. La fonction **r_sample_ordered()** de la librairie **wakefield** permet de générer des données ordinales: un ensemble de réponses à un item de type Likert (1 = Tout-à-fait pour, ... , 7 = Tout-à-fait contre). La médiane est ensuite obtenue, après s'être assuré que les données sont perçues comme étant numériques par la fonction median()::

```{r 03-Chap3-23}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = NULL, name = "Réponse")
head(ord,10)
ord = as.numeric(ord)
cat("La médiane est égale à ",median(ord))
```

Rappelons que la médiane n'a de sens que pour les données dont le niveau de mesure est ordinal ou supérieur (d'intervalle ou de rapport).

La médiane correspond au second quartile de la distribution: le score sous lequel on retrouve 50% de la distribution. Le premier et le troisième quartile sont les scores sous lesquels on retrouve 25% et 75% de la distribution, respectivement. Sous R, on peut obtenir ces valeurs à l'aide de la fonction **quantile()**. Les arguments sont d'abord la variable à étudier, et la ou les proportions se trouvant sous le ou les scores recherchés:

```{r 03-Chap3-24}
quantile(ord,c(0.25,0.5,0.75))
```

De manière alternative, la fonction **summary** retourne tant les quartiles que le minimum et le maximum d'une ensemble de données:

```{r 03-Chap3-25}
summary(ord)
```

Ces différents indices sont les ingrédients d'un diagramme en boîte:

```{r 03-Chap3-26}
boxplot(ord, horizontal=TRUE)
```

En effet, la ligne centrale représente la médiane, la bordure inférieure de la boîte représente le premier quartile, la bordure supérieur représente le troisième quartile, et les valeurs extrêmes se trouvent à chacune des extrémités du graphique.

NOTE: Il est fréquent que des données nominales prennent la forme de catégories telles que "totalement d'accord", "d'accord", "sans avis", "désaccord", et "totalement d'accord". Les fonctions présentées ci-dessus ne peuvent fonctionner pour de telles données. Il faudra d'abord attribuer une valeur numérique à chacune de ces catégories: 1 à 5, pour cet exemple. Ces valeurs numériques réflètent l'ordre des différentes catégories. La fonction **mapvalues()**, qui se trouve dans la librairie **plyr** permet d'effectuer ce recodage:

```{r 03-Chap3-27, eval = FALSE}
library(plyr)
xr = mapvalues(x,from = c("Cat1","Cat2",...,"Catk"), to = c(1:k))
```

Un exemple sera donné plus loin.

#### Différentes options pour obtenir la médiane

Sous R, on dispose très fréquemment de différentes alternatives pour arriver au même résultat. Le code R suivant illustre ce point en affichant quelques manières d'obtenir la médiane d'un ensemble de données (ordinales, ou quantitatives):

```{r 03-Chap3-28}
x = sort(round(rnorm(20,50,10),2))        # Données
pos = (length(x)+1)/2                     # Position de la médiane
md = 0.5*(x[floor(pos)]+x[ceiling(pos)])  # Calcul de la médiane
# ... ou,,,
if((length(x)%%2)==0){                    # n pair?
  md = 0.5*(x[floor(pos)]+x[ceiling(pos)])# Moyenne des 2 x centraux
  } else{
    md = x[pos]}
cat("Médiane =",md)

# Alternative 1: fonction median()...
md = median(x)
cat("Médiane =",md)

# Alternative 2: fonction summary()...
summary(x)

# Alternative 3: fonction quantile()...
md = quantile(0.5)

# Alternative 4: librairie (psych), fonction describe()
library(psych)
round(t(describe(x)),3)  # t(): transposition du tableau
```

#### Exercice 2

1.  Utilisez la fonction **likert_7()** de la librairie **wakefield** afin de générer 500 données ordinales dont les attributs sont les 7 points suivants:

-   "Parfaitement d'accord"
-   "D'accord"
-   "Un peu d'accord"
-   "Neutre"
-   "Un peu en désaccord"
-   "En désaccord"
-   "Parfaitement en désaccord"

Faites en sorte que les proportions des "accords" soient plus élevées que les proportions de "désaccord" Le nom de la variable générée sera: OPINION

La syntaxe générale de cette fonction est:

```{r 03-Chap3-29, eval=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
x = likert_7(n,x = c("Strongly Agree", "Agree", "Somewhat Agree", "Neutral", "Somewhat Disagree","Disagree", "Strongly Disagree"),prob = c(rep(1,7)),
name = "Likert")

```

Lorsque les catégories ordonnées sont alphanumériques, il est nécessaire de les recoder pour qu'elles soient identifiées par des valeurs numériques. Par exemple, un code R tel que le suivant permettra d'effectuer cette opération. La fonction **as.numeric()** assure que le résultat sera numérique et approprié pour les opérations suivantes.

```{r 03-Chap3-30, eval=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
categor = c("Strongly Agree", "Agree", "Somewhat Agree", "Neutral", "Somewhat Disagree","Disagree", "Strongly Disagree")
xrec = as.numeric(mapvalues(x, from = categor, to = c(1:7)))
```

2.  Obtenez des représentations graphiques de cet ensemble de données: diagramme en barres et diagramme en boîte.

3.  Calculez la médiane de cet ensemble de données (essayez plusieurs méthodes)

### Mesure de dispersion: données nominales

Un indice de variation que l'on peut obtenir dans le cas d'une variable nominale est le **rapport de variation (RV)**. Il s'agit simplement du rapport entre le nombre de cas occupant la catégorie modale et le nombre de cas qui se trouvent hors de cette catégorie. Formellement, $$RV=1-\frac{f_{mode}}{N}$$

Lorsque le mode n'est pas unique dans la distribution, RV tient compte de l'ensemble des effectifs des catégories modales:

$$RV=1-\frac{n_{mode}\times f_{mode}}{N}$$

En terme de probabilité, le rapport de variation correspond à la probabilité, en choisissant une observation au hasard, que cette dernière n'appartienne pas à la catégorie modale.

Si la distribution est bi- ou multimodale, le calcul du rapport de variation impliquera la somme des fréquences de chacune des catégories modales.

Considérons à nouveau une variable telle que celle définie plus haut. Le rapport de variation dans ce cas serait:

```{r 03-Chap3-31}
set.seed(150752)  # S'assurer de reproduire les résultats
categ = c("Cat 1","Cat 2","Cat 3","Cat 4","Cat 5","Cat 6","Cat 7")
x = sample(categ,50,replace=TRUE,prob=c(1,2,3,4,3,2,1))
tbl = table(x)
N = sum(tbl)
tbl
mode = Mode(x)            # Détermination du/des mode(s)
nomode = length(mode)
distmode = c("Uniforme (sans mode)","Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}else if(nomode<1){nomode=0}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode+1])
VarRat = 1-nomode*max(tbl)/N           # Rapport de Variation
cat("Rapport de Variation = ",VarRat)

```

On interprète le rapport de variation de la manière suivante:

1.  S'il n'y a pas de catégorie modale, alors la variation est maximale: RV = 1.0\
2.  Plus la fréquence modale est élevée, moins il y a de dispersion entre les autres catégories. À la limite, si tous les cas se trouvaient dans la catégorie modale, il n'y aurait aucune dispersion, et RV = 0.

NOTE: Cet indice n'est pas vraiment présentée dans un rapport de recherche. Il est présenté ici simplement pour illustrer que même dans le cas d'une variable nominale, les trois caractéristiques fondamentales d'un ensemble de données sont présentes!

Quelques autres mesures de dispersion pour données nominales se fondent sur les écarts des fréquences non-modales par rapport à la fréquence modale. Parmi ces mesures, on retrouve:

1.  Déviation Modale Moyenne (DMM):\
    $$DMM=1-\sum_{i = 1}^{k}\frac{{f_{mode}-f_i}}{N(k-1)}$$

2.  Indice de Variation Qualitative (IVQ):\
    $$IVQ=\left(\frac{k}{k-1}\right)\left[1-\sum_{i = 1}^{k}p_i^2\right]$$

3.  Indice d'Entropie Relative (IER):\
    $$IER=-\sum_{i = 1}^{k}\frac{\left[ p_i \times log(p_i)\right]} {log(k)} $$

4.  Écart-Type du Mode (ETM): $$ETM=1-\sqrt{\frac{\sum\limits_{i=1}^{k}(f_{mode}-f_i)^2}{N^2(k-1)}} $$

Pour les données nominales générées ci-dessus, on obtient:

```{r 03-Chap3-32}
k = length(tbl)
N = sum(tbl)
xs = sort(tbl,descending=FALSE)
DMM = 1-(sum(xs[k]-xs[1:k-1]))/(N*(k-1)) 
cat("DMM = ",DMM)

IVQ = (k/(k-1))*(1-sum((tbl/N)**2))
cat("IQV = ",IVQ)

IER = -sum((tbl/sum(tbl))*log(tbl/sum(tbl)))/log(k)
cat("IER = ",IER)

ETM = 1-sqrt(1/((k-1)*N^2)*sum((xs[k]-xs)^2))
cat("ETM = ",ETM)
```

En sélectionnant une observation au hasard, le rapport de variation indique qu'on a 70% des chances d'obtenir une donnée ne provenant pas de la catégorie modale: une grande proportion des données se répartissent dans les autres catégories. Cette grande dispersion est confirmée par les indices fondés sur les écarts par rapport au mode, pour lesquels on obtient des valeurs se rapprochant de 1.0.

Pour fins de comparaison, considérons des ensembles de données de niveaux de dispersion très élevé et très faibles.

#### Dispersion élevée (distribution presqu'uniforme)

Lorsqu'une distribution tend vers l'uniformité, la dispersion est élevée. Le code R suivant illustre cette situation:

```{r Nomin_Disp1, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
x=c(rep("Cat 1",8),rep("Cat 2",8),rep("Cat 3",8),rep("Cat 4",9),  rep("Cat 5",8),rep("Cat 6",8),rep("Cat 7",8))

tbl = table(x)
N = sum(tbl)
tbl
mode = Mode(x)            
nomode = length(mode)
distmode = c("Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode])
VarRat = 1-length(mode)*max(tbl)/N           # Rapport de Variation
cat("Rapport de Variation = ",VarRat)

k = length(tbl)
N = sum(tbl)
xs = sort(tbl,descending=FALSE)
DMM = 1-(sum(xs[k]-xs[1:k-1]))/(N*(k-1)) 
cat("DMM = ",DMM)

IVQ = (k/(k-1))*(1-sum((tbl/N)**2))
cat("IQV = ",IVQ)

IER = -sum((tbl/sum(tbl))*log(tbl/sum(tbl)))/log(k)
cat("IER = ",IER)

ETM = 1-sqrt(1/((k-1)*N^2)*sum((xs[k]-xs)^2))
cat("ETM = ",ETM)


```

Le Rapport de Variation indique qu'en choisissant une observation au hasard, la probabilité que cette dernière n'appartienne pas à la catégorie modale est élevée: 0.842. On peut donc conclure à une dispersion élevée des observations: elles ne sont pas concentrées dans un petit nombre de catégories. Ce niveau de dispersion est confirmé par l'ensemble des mesures de dispersion fondées sur l'écart par rapport au mode.

#### Dispersion 'presque' nulle (distribution presque constante)

Examinons un cas où un mode existe, mais les fréquences observées pour toutes les autres catégories non-modales sont très faibles: la catégorie modale englobe la presque totalité des effectifs:

```{r Nomin_Disp, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Unimodale, dispersion 'presque' nulle
x=c(rep("Cat 1",1),rep("Cat 2",1),rep("Cat 3",1),rep("Cat 4",40),  rep("Cat 5",1),rep("Cat 6",1),rep("Cat 7",1))
tbl = table(x)
N = sum(tbl)
tbl
mode = Mode(x)            # Détermination du/des mode(s)
nomode = length(mode)
distmode = c("Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode])
VarRat = 1-length(mode)*max(tbl)/N           # Rapport de Variation
cat("Rapport de Variation = ",VarRat)

k = length(tbl)
N = sum(tbl)
xs = sort(tbl,descending=FALSE)
DMM = 1-(sum(xs[k]-xs[1:k-1]))/(N*(k-1)) 
cat("DMM = ",DMM)

IVQ = (k/(k-1))*(1-sum((tbl/N)**2))
cat("IQV = ",IVQ)

IER = -sum((tbl/sum(tbl))*log(tbl/sum(tbl)))/log(k)
cat("IER = ",IER)

ETM = 1-sqrt(1/((k-1)*N^2)*sum((xs[k]-xs)^2))
cat("ETM = ",ETM)
```

Le rapport de variation indique que la probabilité, en sélectionnant une observation au hasard, d'obtenir une donnée qui ne se trouve pas dans la catégorie modale est faible: 0.13. Les autres indices de dispersion confirment un niveau de dispersion très faible.

#### Exercice 3

1.  Reprenez la variable EMPLOI générée antérieurement:

```{r 03-Chap3-33, eval=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
n = 500
x = employment(n,x = c("Plein Temps", "Temps Partiel", "Sans Emploi", "Retraité", "Étudiant"), prob = c(0.5, 0.15, 0.1, 0.15, 0.1), name = "Variable")
```

2.  Examinez les indices de dispersion pour cette variable nominale. Comment qualifieriez-vous cette distribution? (NOTE: la librairie **DescTools** est nécessaire ici!)

### Mesure de dispersion: données ordinales

#### Mesures fondées sur les fractiles

Dans le cas de données ordinales (les catégories d'une variable sont ordonnées), l'**écart inter-quartile** (EIQ) est un indicateur du degré de dispersion. Cet indice n'est que la différence entre le troisième et le premier quartile d'une distribution. On l'obtient en utilisant la fonction **IQR()**:

```{r 03-Chap3-34}
IQR(ord)
```

ou en utilisant la fonction **quantile()**, ou la fonction **summary()**:

```{r 03-Chap3-35}
cat("EIQ = ",quantile(ord,0.75)-quantile(ord,0.25))
st = summary(ord)
st
cat("EIQ = ",summary(ord)[5]-summary(ord)[2])
  
```

Essentiellement, cet indice correspond à l'étendue des 50% centraux de la distribution.

Il est également possible d'obtenir des indices de dispersion fondées sur d'autres fractiles. Par exemple on pourrait calculer l'écart inter-décile, en évaluant la distance séparant le premier décile du neuvième; ou celle séparant le 2ième décile du 8ième:

```{r 03-Chap3-36}
deciles = quantile(ord, c(0.1,0.2,0.8,0.9))
cat("Écart Inter-Déciles (d9-d1)",deciles[4]-deciles[1])
cat("Écart Inter-Déciles (d8-d2)",deciles[3]-deciles[2])
```

Ces options seraient aussi défendables que l'écart inter-quartile, qui demeure malgré tout plus souvent utilisé.

#### Mesure du consensus (ou de dissension)

Un angle sous lequel on peut examiner la dispersion d'une variable ordinale consiste à considérer le degré de consensus se traduisant par des fréquences élevées dans une catégorie particulière. On pourrait également considérer le degré de dissension se traduisant par des fréquences élevées dans les catégories opposées. Cette conceptualisation n'est pas très éloignée de ce qui a été fait dans le cas de variables nominales dans la section précédente. Mais étant donné que les attributs de la variable sont ordonnées, on a avantage à tenir compte des informations portées par ce ordre.

Une variable obtenue à l'aide d'une échelle de Likert représente un exemple idéal pour illustrer la procédure. Une échelle de Likert est très souvent utilisée dans les questionnaires, dans un contexte de sondage, par exemple. Les attributs des variables qui en résultent sont de nature ordinale. Par exemple, à une question telle que : "Concernant le projet de loi XYZ, je suis..."

    1 = Très défavorable
    2 = Défavorable
    3 = Neutre
    4 = Favorable
    5 = Très favorable 

Si une très grande majorité des répondants optent pour les choix 4 et 5, on dira que le degré de consensus est élevé. Sinon, si les réponses se répartissent à peu près également dans toutes les catégories, on pourra conclure que le degré de consensus est faible. Si les deux extrémités de l'échelle présentent des fréquences élevées, on pourra parler de dissension. L'objectif, ici, est de quantifier le consensus.

Une mesure développée à cette fin (Tastle & Wierman, 2007) est la suivante:

$$Cns(x)=1+\sum\limits_{i=1}^{k}p_i log_2 \left(1-\frac{\left| f_i-\mu_x\right|}{d_x}\right)$$ $$d_x=X_{max}-X_{min}$$ $$\mu_x=\sum\limits_{i=1}^{k}p_iX_i$$

Cns(x) prend une valeur comprise entre 0, lorsque les données se répartissent également dans les deux extrémités (dissension), et 1, lorsque toutes les réponses se trouvent dans une catégorie particulière (consensus). Si les données se répartissent également dans les différentes catégories, Cns(x) est approximativent égal à 0.5. Le code R suivant simule une telle variable et en calcule le degré de consensus:

```{r Consensus}
set.seed(150752)
categ = 1:5                       # Échelle ordinale, 5 points
x = sample(categ,80,replace=TRUE)
tbl = table(x)
prop = tbl/sum(tbl)             # Proportions de réponses dans chaque catégorie
mu = sum(categ*prop)              # 'Moyenne' de la distribution
tbl
cns = 1+sum(prop*log2(1-(abs(categ-mu)/(max(categ)-min(categ))))) # Consensus
cat("Consensus = ",cns)
```

La fonction **consensus()** de la librairie **agrmt** permet également ce calcul. Son argument est le vecteur des fréquences observées dans chacune des catégories ordonnées:

```{r 03-Chap3-37}
library(agrmt)
consensus(table(x))
```

Lorsqu'il y a consensus, une grande majorité des réponses se trouvent dans une catégorie particulière, et on obtient, par exemple:

```{r 03-Chap3-38}
fx = c(1,1,20,1,1)
consensus(fx)
```

Lorsque le consensus est 'parfait', on obtient:

```{r 03-Chap3-39}
fx = c(0,0,20,0,0)
consensus(fx)
```

Il y a également consensus lorsque les catégories contenant une forte proportion des réponses sont adjacentes l'une à l'autre:

```{r 03-Chap3-40}
fx = c(0,0,0,20,20)
consensus(fx)
```

Mais lorsque les réponses se concentrent dans les deux extrémités de l'échelle, on obtient:

```{r 03-Chap3-41}
fx = c(20,0,0,0,20)
consensus(fx)
```

Le consensus est donc une mesure sensible aux polarisations. Dans ce dernier cas, on pourra parler de 'dissension'.

#### Exercice 4

1.  Reprenez la variable **ord** générée antérieurement:

```{r 03-Chap3-42}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = NULL, name = "Réponse")
ord = as.numeric(ord)

```

2.  Obtenez un diagramme en barres pour cette variable. Quelle est votre perception initiale concernant la dispersion de cette variable?

3.  Obtenez un diagramme en boîte pour ces données. Votre appréciation de la dispersion change-t-elle?

4.  Calculez l'intervalle inter-quartile et prenez-en note pour les exercices suivants.

5.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu. Prenez-en note pour les exercices suivants.

6.  Générez une nouvelle variable ordinale en utilisant le même processus générateur, mais en altérant les probabilités associés à chaque attribut possible:

```{r Ex_6}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = c(10,8,2,0,2,8,10), name = "Réponse")
ord = as.numeric(ord)

```

a.  Obtenez la médiane de cette variable
b.  Calculez l'écart inter-quartile pour ces données
c.  Obtenez un diagramme en barres pour cette variable
d.  Quelle est votre perception de la dispersion de cette variable?
e.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu.

<!-- -->

7.  Générez une nouvelle variable ordinale en utilisant le même processus générateur, mais en altérant les probabilités associés à chaque attribut possible:

```{r Ex_7}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = c(1,0,1,0,4,8,10), name = "Réponse")
ord = as.numeric(ord)

```

a.  Obtenez la médiane de cette variable
b.  Calculez l'écart inter-quartile pour ces données
c.  Obtenez un diagramme en barres pour cette variable
d.  Obtenez un diagramme en boîte pour cette variable
e.  Quelle est votre perception de la dispersion de cette variable?
f.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu.

<!-- -->

8.  Faites l'expérience de différentes distributions en modifiant, dans le code R utilisé ci-dessus, les probabilités des différents attributs. Le vecteur définissant **prob=** correspond aux poids accordés aux attributs 1 à 7. Les probabilités sont simplement les poids relatifs: **prob/sum(prob)**. Expérimentez différentes configurations pour apprécier le fonctionnement de l'indice de consensus. Comparez les intervalles inter-quartiles obtenus dans chaque cas, avec celui obtenu en (4). Comparez de la même manière les indices de consensus.

## Variables quantitatives

Dans le cas d'une variable quantitative, les attributs sont non seulement ordonnés, mais l'intervalle qui les sépare est constant et connu. Par exemple, l'année de la naissance d'un individu représente une période de temps couvrant 365 jours (oublions les années bisextiles pour l'instant!), une période fixe, connue. La distance qui sépare deux individus de 20 et 25 ans est la même que la distance qui sépare des individus de 65 et 70 ans. Par contre, deux situations se présentent: 1. La valeur nulle ('0') est un attribut comme toutes les autres valeurs que peut prendre une variable, mais n'indique pas l'absence de cet attribtu. L'An 0 dans l'histoire de l'Humanité n'indique pas le début des temps! Une température de 0 degré Celsius n'indique pas l'absence de température! On dira de cette situation qu'il n'y a pas de zéro absolu. Les variables de ce type sont dites d'**intervalle**.\
2. La valeur nulle ('0') est absolu et indique l'absence de l'attribut. S'il n'a pas plu au cours du mois de juin au Cap-Haitien, le nombre de milli-mètres de pluie est égal à 0. Une valeur négative n'est pas possible. Si une balance indique un poids nul, c'est qu'il n'y a rien sur le plateau de cette balance. Dans ce cas, on dira d'une telle variable qu'elle est **de rapport**.

Dans ces deux cas, les données peuvent être discrètes ou continues, et un très grand nombre de valeurs sont généralement possibles, en pratique ou en théorie. Dans plusieurs cas, le nombre de valeurs qu'il est possible d'observer pour ce type de variables est infini. Un calcul de probabilités tel que présenté dans les chapitres précédents ne serait donc pas applicable.

### Génération de données

Sous R, plusieurs fonctions permettent la génération de données aléatoires. Nous en utiliserons quelques unes pour illustrer les procédures descriptives que l'on peut appliquer sur un ensemble de données quantitatives.

#### Distribution Uniforme

Dans une distribution uniforme, les valeurs comprises entre un minimum et un maximum sont équiprobables: à la limite, lorsqu'un nombre infini de valeurs sont tirés d'une telle distribution, la distribution prend une forme rectangulaire.

Sous R, la fonction **runif()** permet de générer des données aléatoires provenant d'une telle distribution. Un seul argument est requis: le nombre de données à générer. Pour spécifier une valeur minimale et une valeur maximale, les deux derniers arguments sont disponibles. Par défaut, les données générées sont comprises entre 0 et 1:

```{r 03-Chap3-43}
n = 5000
linf = 10
lsup = 20
x = runif(n,linf,lsup)
head(x,10)
hist(x,main="Distribution Uniforme")
```

#### Distribution Gaussienne

En Sciences, il est fréquent de rencontrer des données quantitatives dont la distribution prend une forme rappelant une cloche. Plusieurs exemples suivront dans la suite de nos présentations. Des données de ce type peuvent être générées à l'aide de la fonction **rnorm()**, qui nécessite au moins un argument: le nombre de données à générer. Si aucun autre argument n'est fourni, le centre de la distribution (sa moyenne) se trouvera aux environs de **0**, et son indice de dispersion (son écart-type) avoisinera **1.0**. Pour spécifier d'autres combinaisons de moyenne et d'écart-type il suffit de spécifier ces arguments. On obtient alors:

```{r 03-Chap3-44}
n = 5000
moyenne = 100
ecart.type = 15
x = rnorm(n,moyenne,ecart.type)
head(x,10)
hist(x,main="Distribution Gaussienne")
```

Le résultat est une variable quantitative mesurée sur 5000 sujets. La moyenne arithmétique de cette variable se situe autour de 100, et son écart-type se trouve autour de 15.

#### Distribution Exponentielle

Il semble que la durée d'un appel au Service à la Clientèle de Apple est d'en moyenne, 17 minutes. On note également qu'un très grand nombre d'appels ont une durée inférieure à 17 minutes et très peu ont des durées supérieures. Ce type d'observations suit souvent la distribution exponentielle.

On peut simuler ce type de données à l'aide de la fonction **rexp()**. Cette fonction nécessite un argument obligatoire, n = nombre de données à générer. Un second argument correspond à la réciproque du taux moyen de la variable générée. Par défaut, ce taux est égal à 1. Dans le cas de l'exemple ci-dessus, le taux moyen est 17, de sorte que le second argument de la fonction sera 1/17 = 0.0588. Le code R suivant produira un ensemble de 500 données suivant la distribution exponentielle, avec un taux moyen de 17:

```{r 03-Chap3-45}
x = rexp(5000,0.0588)
head(x)
hist(x,main="Distribution Exponentielle")
```

### Données quantitatives: Forme de la distribution

La forme d'une distribution est probablement la première caractéristique que l'on saisi en examinant un diagramme résumant un ensemble de données. Dans une analyse descriptive de données, différents types de graphiques et/ou des tableaux de fréquences sont généralement efficaces pour nous permettre de qualifier cette forme.

#### Tableaux de fréquences

Un premier exercice qu'il est généralement utile de compléter sur un ensemble de données consiste à condenser, ou organiser les données de manière à pouvoir en déceler les principales caractéristiques. Lorsque les données sont de nature quantitatives, on doit fréquemment les regrouper dans des intervalles de classe. Cette opération implique nécessairement une perte d'information dont l'ampleur est inversement proportionnelles au nombre de classes utilisées. Plusieurs 'règles' permettent de déterminer ce nombre, pour que l'efficacité de la représentation soit maximale. Une telle règle, dûe à Freedman & Diaconis (1981) propose: $$N_{classes}=\left\lceil \frac{n^{1/3}(max-min)}{2(Q_3-Q_1)}\right\rceil $$ 
De cette formulation, on constate que le nombre de classe doit tenir compte de l'étendue de la variable [max(x)-min(x), et écart inter-quartile], et du nombre d'observations. La fonction suivante effectue cette opération. Son unique argument est la variable que l'on veut examiner:

```{r 03-Chap3-46}
n_classes = function(x){
  nc = ceiling((length(x)^(1/3)*(max(x)-min(x)))/(2*IQR(x)))
  return(nc)
}
```

La fonction **hist()** permet également l'argument **breaks="FD"** pour accomplir cette tâche.

Plusieurs méthodes alternatives ont été proposées pour déterminer le nombre optimal de classes. On trouvera une discussion intéressante à ce sujet sur Wikipedia, au lien suivant: [Calcul de N_Classes](https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width "Wikipedia").

La fonction suivante permet de produire un tableau de fréquence en regroupant des observations en intervalles de classe. Par la suite, les fréquences absolues, relatives, cumulatives, et cumulatives-relatives sont reproduites. La variable contenant les données est le seul argument qui est nécessaire d'utiliser. Un second argument permet de spécifier le nombre approximatif d'intervalles composant la distributon. Par défaut, ce nombre est 20:

```{r TblFreq}
TblFreq = function(x,bins=20){	# Fonction TblFreq: x = vecteur de données; 
				                        # bins = nombre d'intervalles
hg = hist(x,bins)			          # Obtenir les paramètres de l'histogramme
FreqAbs=hg$counts			          # Fréquences absolues
FreqRel=prop.table(FreqAbs)		  # Fréquences relatives
FreqCum = cumsum(FreqAbs)		    # Fréquences cumulatives
FreqRelCum = cumsum(FreqRel)	  # Fréquences Relatives et Cumulatives
lim = hg$breaks[1:(length(hg$breaks))]-.5  # Limites exactes des intervalles
LimInf=lim[1:(length(lim)-1)]
LimSup=lim[2:(length(lim))]
output = data.frame(LimInf,LimSup,FreqAbs,FreqRel,FreqCum,FreqRelCum)
return(output)
}

```

Mettons à l'épreuve cette fonction sur les données que nous avons préalablement générées:

```{r 03-Chap3-47, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
TF = TblFreq(x,bins=n_classes(x))
kable(TF, booktabs=TRUE, caption="Répartition des Effectifs") %>% kable_styling(full_width = TRUE)
```

Dans un premier temps, on obtient un histogramme représentant la distribution des données. On peut ainsi visualiser le centre, la dispersion, et la forme de cette distribution.

Le tableau qui fait suite à l'histogramme présente les limites inférieures et supérieures exactes des intervalles de classe. Dans le cas présent, le nombre d'intervalles n'est pas nécessairement 20, valeur qui est implicite dans la commande. Ceci est dû au fait que nous voulons des limites entières, ce qui facilite la lecture. La fonction ajuste le nombre d'intervalles de manière à ce que cette contrainte soit respectée. Pour chacun des intervalles, les quatre colonnes de droite reproduisent les fréquences absolues, les fréquences relatives, puis cumulatives et cumulatives-relatives.

La fonction suivante reprend la fonction TblFreq() en y ajoutant différents types de représentation graphique, incluant un polygone de fréquence et une ogive:

```{r TblFreq2, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
TblFreq2 = function(x,bins=20){	# Fonction TblFreq2: x = vecteur de données; bins = nombre d'intervalles
hg = hist(x,bins,main="Histogramme",xlab="X")	# Obtenir les paramètres de l'histogramme
MidPt=hg$mids
FreqAbs=hg$counts		# Fréquences absolues
FreqRel=prop.table(FreqAbs)	# Fréquences relatives
FreqCum = cumsum(FreqAbs)	# Fréquences cumulatives
FreqRelCum = cumsum(FreqRel)	# Fréquences Relatives et Cumulatives
lim = hg$breaks[1:(length(hg$breaks))]-.5  # Limites exactes des intervalles
LimInf=lim[1:(length(lim)-1)]
LimSup=lim[2:(length(lim))]
hist(x,bins,main="Histogramme et Polygone de Fréquences",xlab="X")
par(new=TRUE)
# Polygone de fréquences
lines(MidPt,FreqAbs,col="red",lwd=3)
par(new=FALSE)
plot(MidPt,FreqAbs, type="l",main="Polygone de Fréquences",xlab="X",ylab="Fréquence",col="red",lwd=3)
boxplot(x,main="Diagramme en Boîte",xlab="X",range=1.5,horizontal=TRUE,boxwex=0.25)
# Distribution Cumulative
barplot(FreqCum,main="Distribution de Fréquences Cumulatives",xlab="X",ylab="Fréquences Cumulatives",
	space=FALSE,names.arg=MidPt)
plot(MidPt,FreqRelCum, type="l",main="Distribution des fréquences Relatives Cumulatives",
	xlab="X",ylab="Proportion",col="red",lwd=3)
output = data.frame(LimInf,LimSup,FreqAbs,FreqRel,FreqCum,FreqRelCum)
return(output)
}

```

En appelant cette fonction, on obtient:

```{r 03-Chap3-48}
TF = TblFreq2(x)

```

#### Symétrie et Voussure
  
L'histogramme et le polygone de fréquences nous offrent un aperçu de la forme de la distribution. Il existe cependant des indices numériques quantifiant deux aspects particuliers relatifs à cette forme: le degré de symétrie, et le degré de voussure. On peut les obtenir à l'aide des fonctions **skewness()** et **kurtosis()** contenues dans la librairie **moments**:

```{r 03-Chap3-49}
library(moments)
cat("Indice de symétrie = ",skewness(x))
cat("Indice de voussure = ",kurtosis(x))
```

Un indice de symétrie nul indique que la distribution est symétrique autour de son centre. Un indice positif indique que l'extrémité droite de la distribution est allongée, alors qu'un indice négatif indique un allongement de l'extrémité gauche. La figure suivante illustre cette caractéristique: ![Symétrie](./images/skewness.png)

Pour sa part, la voussure réfère à la *lourdeur* des extrémités. On distinguera une distribution dont les extrémités sont légères (*leptokurtique*), intermédiaire (*mésokurtique*) ou lourdes (*platykurtique*). La figure suivante en fait l'illustration: ![Voussure](./images/voussure.jpg)

Pour une distribution Gaussienne, on obtient de la même manière:

```{r 03-Chap3-50}
x = rnorm(500,100,15)
TF = TblFreq2(x)
kable(TF, booktabs=TRUE, caption="Distribution de Fréquences") %>%
kable_styling(full_width = TRUE)

```

Le code R suivant génére trois distributions: une symétrique, une asymétrique positive, et une asymétrique négative:

```{r Demo-Sym, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(mnonr)
x = unonr(10000, rep(0,3), diag(3), skewness = c(-.7,0,.7), kurtosis = c(0, 0, 0),empirical = TRUE)
hist(x[,1],main="Distribution asymétrique négative",breaks=20)
cat("Indice de Symétrie = ",skewness(x[,1]))
hist(x[,2],main="Distribution symétrique",breaks=20)
cat("Indice de Symétrie = ",skewness(x[,2]))
hist(x[,3],main="Distribution asymétrique positive",breaks=20)
cat("Indice de Symétrie = ",skewness(x[,3]))

```

Le code R suivant génère des données dont la distribution est platykurtique, mésokurtique, ou leptokurtique:

```{r Demo_Voussure, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(mnonr)
x = unonr(10000, rep(100,3), diag(3)*15, skewness = c(0,0,0), kurtosis = c(-0.6, 0, 3))
hist(x[,1],main="Distribution platykurtique")
cat("Indice de Voussure = ",kurtosis(x[,1])-3)
hist(x[,2],main="Distribution mésokurtique")
cat("Indice de Voussure = ",kurtosis(x[,2])-3)
hist(x[,3],main="Distribution leptokurtique")
cat("Indice de Voussure = ",kurtosis(x[,3])-3)

```
#### Courbe de densité

Pour une variable quantitative et continue, il est souvent intéressant d'afficher la distribution des densités. Il s'agit d'une évaluation de la densité de la distribution pour différents intervalles tout au long de l'étendue de la variable. 

Dans une distribution continue, la probabilité d'une valeur X particulière est **toujours** nulle. En effet, le nombre de valeurs possibles que peut prendre cette variable est infini. En conséquence la probabilité d'obtenir une valeur spécifique est égale à $1/\infty=0$ . Une autre manière d'exprimer ce fait est:
$$P(X=a)=\int_{a}^{a}{f(x)} dx=0$$
Par contre, si on considère un intervalle, aussi petit que nécessaire autour d'une valeur spécifique, la probabilité sera supérieure à 0:
$$P(X=a)=\int_{a-\delta }^{a+\delta }{f(x)} dx\gt  0 $$
C'est ce qui définit une **densité**, que l'on peut représenter graphiquement. 

Sous R, la fonction **density()** permet d'obtenir les densités correspondant à chacune des valeurs d'une distribution. Considérons les données suivantes:

```{r 03-Chap3-51}
n <- 1e4
set.seed(1)
betas<-rbeta(n,2,3)
x <- c(betas[1:(n/2)]*2+1,betas[(n/2+1):n]*2+2)
hist(x,breaks="FD")
```
Les densités qu'il est possible de calculer dépendent d'une quantité appelée **bandwidth**. Plus cette quantité est petite, plus les détails de la distribution ressortiront; inversement, plus la bandwidth est grande, plus nous perdons de détail sur la distribution. La 'bandwidth' est un argument de la fonction **density()**. Examinons la distribution de densité des données générées ci-dessus:

```{r 03-Chap3-52}
d = density(x)
cat("Contenu: ")
summary(d)
cat("Par défaut:  bw = ",d$bw)
head(cbind(d$x,d$y),15)
cat("Fonction de densité")
plot(d)
```
En fixant bw = 0.01, on obtient:

```{r 03-Chap3-53}
d = density(x,bw=0.01)
cat("Fonction de densité")
plot(d)
```
Et en la fixant à 0.3:


```{r 03-Chap3-54}
d = density(x,bw=0.3)
cat("Fonction de densité")
plot(d)
```
On constate donc que la bandwidth doit être sélectionnée de manière à cacher le bruit autant que possible, tout en maintenant les principales caractéristiques de la distribution.  Ce serait un travail difficile, si nous ne disposions pas d'outils pour le faire. Et ces outils existent:

  1. La valeur utilisée par défaut (**bw-nrd0**) est calculée en fonction des données elles-mêmes, et fonctionne bien généralement.
  2. Plusieurs fonctions intégrées dans l'environnement R font l'estimation de bw:
    
```{r 03-Chap3-55}
cat("bw.nrd: bw = ",bw.nrd(x))
cat("bw.nrd0: bw = ",bw.nrd0(x))
cat("bw.ucv: bw = ",bw.ucv(x))
cat("bw.bcv: bw = ",bw.bcv(x))
cat("bw.SJ: bw = ",bw.SJ(x))
```
En bref, pour nos besoins immédiats, on peut se fier à l'une ou l'autre de ces modes de sélection. Mais on conserve toujours la liberter de moduler le résultat en fonction des objectifs visés.

### Données quantitatives: Tendance centrale

Dans le cas de données quantitatives, il est possible d'obtenir non seulement le mode et la médiane, mais aussi la moyenne arithmétique de l'ensemble des données.

#### Le mode

Une variable quantitative peut être formée de données discrètes ou continues. Dans le cas de données discrètes, on peut obtenir le mode de la même manière que pour les données qualitatives: il s'agit simplement de la valeur dont la fréquence est la plus élevée. Dans le cas de données continues, par contre, il est fréquent que les valeurs observées soient uniques et qu'il n'y ait que peu de valeurs égales. Par exemple, examinons un ensemble de 20 données choisies suivant une distribution Gaussienne dont la moyenne est 50 et l'écart-type est 8:

```{r 03-Chap3-56}
x = rnorm(20,50,8)
cat("Valeurs uniques:")
unique(x)
cat("Nombre de valeurs uniques = ",length(unique(x)))

```

On constate que chaque valeur est unique ce qui, selon la définition que nous avons donnée du mode, correspond à une distribution **amodale**. Cependant, on pourra considérer le mode sous l'angle de la densité: le mode sera la valeur pour laquelle on retrouve la plus grande densité, cette densité représentant la probabilité qu'un score se retrouve à l'intérieur d'un intervalle étroit autour de ce score. Par exemple, dans une distribution continue, la probabilité d'obtenir un score donné est nulle: $p(x)=1/\infty=0$. Par contre, la probabilité d'obtenir un score compris entre $x\pm0.001 \neq 0$ et on peut la déterminer,

Considérons la distribution de densité d'un ensemble de 1000 données. La fonction **density()** permet de déterminer les densités requises:

```{r 03-Chap3-57}
x = rnorm(1000,50,8)
cat("Nombre de valeurs uniques = ",length(unique(x)))

hist(x,breaks="FD")

dx = density(x, bw = bw.ucv(x))
plot(dx, frame = TRUE, col = "steelblue", 
     main = "Diagramme de Densité", cex.lab = 1.5,
     cex.main = 1.75, lwd = 2)

```
Alternativement, la fonction **mlv()** de la librairie **modeest**  fait le travail avec davantage de versatilité:

```{r ModeEst}
library(modeest)

m = mlv(x,method="meanshift")
cat("Mode = ",m)
# Histogram
hist(x, freq = FALSE,breaks="FD")
# Mode
abline(v = m, col = 4, lty = 2, lwd = 3)

# Density
dx <- density(x)
lines(dx$x, dx$y, col = 2, lwd = 2)

```

De la distribution des densités, on trouve le mode. Il s'agit cette fois de la valeur de X pour laquelle la densité est la plus élevée. La fonction suivante nous permet d'obtenir cette valeur:

```{r 03-Chap3-58}
DensMode <- function(x){
  d<-density(x)
  m = d$x[which(d$y==max(d$y)[1])]
  return(m)
}

```
Cette fonction requiert la librairie **ggplot2**. Examinons son bon fonctionnement pour déterminer le mode des données générées ci-dessus:

```{r 03-Chap3-59}
m = DensMode(x)
cat("Mode = ",m)
# Représentation graphique

require(ggplot2)
ggplot(data=data.frame(x), aes(x=x)) + geom_density(fill="lightblue",color = "midnightblue") +
geom_vline(xintercept=m,linetype="dashed",color="red",size=1)

```

La librairie **multimode** est particulièrement intéressante pour l'examen du ou des modes dans une distribution continue. 

Considérons une distribution bi- ou multimodale:

```{r Multimode}
library(multimode)

set.seed(23474567)
n <- 1000
bin <- rbinom(n, 1, 0.6)
y2 <- rnorm(n, mean = 80, sd = 11) * bin + rnorm(n, mean = 40, sd = 8) * (1 - bin)
hist(y2,breaks=30)

modes <- locmodes(y2, mod0 = 2,lowsup=min(y2),uppsup=max(y2))
modes

abline(v = modes$locations[c(1,3)], col = 2, lwd = 2)
abline(v = modes$locations[2], col = "green", lwd = 2)

plot(modes)

```
De la fonction de densité, on estime l'existence de deux modes dont les valeurs sont respectivement 40.28 et 79.99 et sont associés à des densités similaires (0.018 et 0.021). L'antimode se trouve à x = 57.31, avec une densité très faible (0.005).  L'histogramme et la courbe de densité illustrent la situation, Les deux modes sont affichés, de même que l'**antimode**, la valeur dont la densité est la plus faible, entre deux modes, lorsque la distribution est bi- ou multimodale.

#### La médiane

Pour la médiane et les quartiles, on peut utiliser à nouveau les fonctions **quantile()** et **summary()** comme on l'a fait pour des données ordinales. Par exemple,

```{r Median}
x = runif(500,1,50)
cat("La médiane est ",median(x))
cat("Les quartiles sont :\n ",quantile(x,c(0.25,0.5,0.75)))
summary(x)

```
#### La moyenne
  
On peut obtenir la moyenne à l'aide de la fonction **mean()**:

```{r 03-Chap3-60}
cat("La moyenne est égale à ",mean(x))
```

On se souviendra que la moyenne est le centre de gravité d'un ensemble de scores. De ce fait, la somme des écarts par rapport à sa valeur sera toujours nulle: $$\sum_{i = 1}^{n}(X - \overline{X})=0$$ Pour le vérifier:

```{r 03-Chap3-61}
cat("La somme des écarts à la moyenne = ", sum(x-mean(x)))
```

### Données quantitatives: Dispersion

L'écart inter-quartile permettant de quantifier la dispersion d'une variable ordinale peut s'obtenir de la même manière pour une variable quantitative. Par contre, on préférera un indice plus efficace dans le cas de variables de ce niveau (mesure d'intervalle ou de rapport). Il s'agit ici d'utiliser toute l'information disponible pour obtenir un indice fiable de l'éparpillement des données autour du centre de la distribution.

On peut facilement imaginer qu'un tel indice doit se fonder sur les distances qui séparent chaque donnée du centre de la distribution. Puisque la médiane et la moyenne représentent ce centre (quoique d'une manière conceptuellement distincte!), notre attention se portera soit sur $(X - Md)$, soit sur $(X - \bar X)$. Mais puisque $\sum (X - \bar X)=0$, il faudra trouver une manière de ne considérer que les distances **absolues** séparant les données du centre. Les candidats qui satisfont ce critère sont:\
1. La déviation médiane:

$$\sum_{i=1}^{n}\frac{\left | X_{i}-Md   \right|}{n} $$ Cette mesure est simplement la moyenne arithmétique des écarts par rapport à la médiane.

2.  La déviation moyenne: $$\sum_{i=1}^{n}\frac{\left | X_{i}-\bar{X} \right|}{n} $$ Cette mesure est très similaire à la précédente: il s'agit de la moyenne arithmétique des écarts par rapport à la moyenne.

3.  La variance: $$s^{2} = \frac{\sum_{i=1}^{n}\left(X_{i} - \bar{X}\right)^{2}}{n-1}$$ Cette mesure est analogue à la déviation moyenne, sauf que les écarts absolus par rapport à la moyenne sont remplacés par le carré de ces quantités. On note aussi qu'au niveau du dénominateur, une correction est apportée $(n-1)$. Ce point particulier sera explicité plus tard. On obtient de cette manière une moyenne arithmétique des carrés des écarts par rapport à la moyenne.

4.  L'écart-type: $$s = \sqrt{\frac{\sum\limits_{i=1}^{n} \left(X_{i} - \bar{X}\right)^{2}} {n-1}}$$ Cette mesure de dispersion, omniprésente dans les méthodes d'analyse de données quantitatives n'est que la racine carrée de la variance. Donc la racine carrée de la moyenne des carrés des écarts par rapport à la moyenne.

Ces différents indices sont aisément obtenus à l'aide des fonctions **mad()**, **var()**, et **sd()**:

```{r Quant_Dispersion}
cat("La déviation moyenne = ",mad(x,center=mean(x)))
cat("La déviation médiane = ",mad(x,center=median(x)))
cat("La variance          = ",var(x))
cat("L'écart-type         = ",sd(x))
```

5.  Le Coefficient de Variation: ce coefficient est une mesure de dispersion **relative**. Il est défini par: $$ CV = \frac {\sigma}{\bar X}\times 100 $$

6.  Le coefficient de variation robuste: Ce coefficient est analogue au coefficient de variabion, sauf qu'il n'est pas affecté par les données extrêmes: il utilise la médiane et la déviation médiane absolue: $$CVR =\frac {Déviation Médiane}{Médiane}\times 100 $$ \#\# Exercice 5:

7.  Générez un ensemble de 500 observations Gaussiennes avec une moyenne avoisinnant 50 et un écart-type avoisinnant 10.

8.  Obtenez les indices de tendance centrale: moyenne et médiane

9.  Obtenez les indices de dispersion

10. Obtenez les indices de symétrie et de voussure

11. Obtenez la distribution des fréquences et les représentations graphiques habituelles

12. Sous quel score trouve-t'on 67% de la distribution?

13. Quelle est la proportion des scores de la distribution sous X = 40?

14. Quelle est la probabilité, en choisissant une observation au hasard, d'obtenir un score inférieur à 35, ou supérieur à 65?

15. Déterminez les valeurs limitant les 95% centraux de la distribution.

16. Quelle est la proportion des scores compris entre 38 et 57?

## Conclusion

Dans cette section, nous avons appris à générer des données qualitatives (nominales et ordinales) et quantitatives (d'intervalle et de rapport). Les distributions des scores générés ont été décrites à l'aide d'indices de tendance centrale, de dispersion et de symétrie et de voussure (forme de la distribution). L'ensemble de ces acquis sont à la base de tout ce qui suit.

## Références

1.  KVALSETH, TARALD O., Measuring variation for nominal data, Bulletin of the Psychonomic Society, 1988. 26 (5), 433-436\
2.  REYNOLDS, H. T. The analysis of cross-classifications. New York: Free Press, 1977
3.  TASTLE, William J. & Wierman, Mark J., Consensus and dissention: A measure of ordinal dispersion, International Journal of Approximate Reasoning 45 (2007) 531--545

\newpage

## Solutions aux exercices

### Exercice 1

1.  Chargez la librairie **wakefield**:

```{r 03-Chap3-62}
library(wakefield)
```

2.  Utilisez la fonction **employment()** pour générer le statut d'emploi de 500 personnes. Les proportions pour chaque catégories sont estimées à 50%, 15%, 10%, 15% et 10%, respectivement. La variable qui sera générée portera le nom: EMPLOI. Éditez le code R ci-dessous pour obtenir ce résultat

```{r 03-Chap3-63, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
n = 500
empl = employment(500,x = c("Plein Temps", "Temps Partiel", "Sans Emploi", "Retraité", "Étudiant"), prob = c(0.6, 0.15, 0.1, 0.15, 0.1), name = "Emploi")
```

3.  Obtenez un diagramme en barres pour représenter ces données. Notez que vous devez obtenir le tableau de fréquences au préalable.

```{r 03-Chap3-64}
tbl = table(empl)
barplot(tbl)
```

4.  Construisez un diagramme en secteur pour représenter les proportions des différents statuts d'emploi.

```{r 03-Chap3-65}
slices <- tbl
lbls <- c("Plein Temps", "Temps Partiel", "Sans Emploi", "Retraité", "Étudiant")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls," (", pct)
lbls <- paste(lbls,"%)",sep="")
pie(slices, labels = lbls, main="Diagramme en secteurs\n (Pourcentages)")
```

5.  Déterminez le mode de cette distribution, en utilisant la fonction **Mode()**\* de la librairie **DescTools**. Dans ce cas, le mode est déterminé à partir des données originales (**x**). Puis, refaites le travail en utilisant la fonction **modes()** de la librairie **agrmt**. Dans ce cas, le mode est calculé à partir du tableau de fréquences.

```{r 03-Chap3-66}
library(DescTools)
library(agrmt)
Mode(empl)
tbl = table(empl)
modes(tbl)
```

### Exercice 2

1.  Utilisez la fonction **likert_7()** de la librairie **wakefield** afin de générer 500 données ordinales dont les attributs sont les 7 points suivants:

-   "Parfaitement d'accord"
-   "D'accord"
-   "Un peu d'accord"
-   "Neutre"
-   "Un peu en désaccord"
-   "En désaccord"
-   "Parfaitement en désaccord"

Faites en sorte que les proportions des "accords" soient plus élevées que les proportions de "désaccord". Le nom de la variable générée sera: OPINION

```{r 03-Chap3-67,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(wakefield)
n = 500
x = likert_7(n,x = c("Parfaitement d'accord", "D'accord", "Un peu d'accord", "Neutre", "Un peu en désaccord","En désaccord", "Parfaitement en désaccord"),prob = c(6,8,3,2,3,3,2),name = "Opinion")
```

2.  Obtenez des représentations graphiques de cet ensemble de données: diagramme en barres et diagramme en boîte.

```{r 03-Chap3-68}
tbl = table(x)
barplot(tbl)
boxplot(x)
```

3.  Calculez la médiane de cet ensemble de données (essayez plusieurs méthodes)

```{r 03-Chap3-69, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(plyr)
categor = c("Parfaitement d'accord", "D'accord", "Un peu d'accord", "Neutre", "Un peu en désaccord","En désaccord", "Parfaitement en désaccord")
xrec = as.numeric(mapvalues(x, from = categor, to = c(1:7)))

median(xrec)
summary(xrec)
quantile(xrec,0.5)

library(psych)
t(describe(xrec))

```

### Exercice 3

1.  Reprenez la variable EMPLOI générée antérieurement:

```{r 03-Chap3-70, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
n = 500
x = employment(n,x = c("Plein Temps", "Temps Partiel", "Sans Emploi", "Retraité", "Étudiant"), prob = c(0.5, 0.15, 0.1, 0.15, 0.1), name = "Variable")
```

2.  Examinez les indices de dispersion pour cette variable nominale. Comment qualifieriez-vous cette distribution? (NOTE: la librairie **DescTools** est nécessaire ici!)

```{r 03-Chap3-71}
library(DescTools)

tbl = table(x)
N = sum(tbl)
tbl
mode = Mode(x)            # Détermination du/des mode(s)
nomode = length(mode)
distmode = c("Unimodale","Bimodale","Multimodale")
if(nomode>2){nomode = 3}
cat("Catégorie(s) Modale(s) = ",Mode(x))
cat("La distribution est ",distmode[nomode])
VarRat = 1-length(mode)*max(tbl)/N           # Rapport de Variation
cat("Rapport de Variation = ",VarRat)

k = length(tbl)
N = sum(tbl)
xs = sort(tbl,descending=FALSE)
DMM = 1-(sum(xs[k]-xs[1:k-1]))/(N*(k-1)) 
cat("DMM = ",DMM)

IVQ = (k/(k-1))*(1-sum((tbl/N)**2))
cat("IQV = ",IVQ)

IER = -sum((tbl/sum(tbl))*log(tbl/sum(tbl)))/log(k)
cat("IER = ",IER)

ETM = 1-sqrt(1/((k-1)*N^2)*sum((xs[k]-xs)^2))
cat("ETM = ",ETM)
```

### Exercice 4

1.  Reprenez la variable **ord** générée antérieurement:

```{r 03-Chap3-72}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = NULL, name = "Réponse")
head(ord,10)
ord = as.numeric(ord)
cat("La médiane est égale à ",median(ord))
```

2.  Obtenez un diagramme en barres pour cette variable. Quelle est votre perception initiale concernant la dispersion de cette variable?

```{r 03-Chap3-73}
barplot(table(ord))
```

3.  Obtenez un diagramme en boîte pour ces données. Votre appréciation de la dispersion change-t-elle?

```{r 03-Chap3-74}
boxplot(ord)
```

4.  Calculez l'intervalle inter-quartile et prenez-en note pour les exercices suivants.

```{r 03-Chap3-75}
IQR(ord)
```

5.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu. Prenez-en note pour les exercices suivants.

```{r 03-Chap3-76}
library(agrmt)
consensus(table(ord))
```

6.  Générez une nouvelle variable ordinale en utilisant le même processus générateur, mais en altérant les probabilités associés à chaque attribut possible:

```{r 03-Chap3-77}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = c(10,8,2,0,2,8,10), name = "Réponse")
ord = as.numeric(ord)

```

a.  Obtenez la médiane de cette variable

```{r 03-Chap3-78}
cat("Médiane = ",median(ord))
```

b.  Calculez l'écart inter-quartile pour ces données

```{r 03-Chap3-79}
cat("EIQ = ",IQR(ord))
```

c.  Obtenez un diagramme en barres pour cette variable

```{r 03-Chap3-80}
barplot(table(ord))
```

d.  Quelle est votre perception de la dispersion de cette variable?

e.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu.

```{r 03-Chap3-81}
library(agrmt)
consensus(table(ord))
```

7.  Générez une nouvelle variable ordinale en utilisant le même processus générateur, mais en altérant les probabilités associés à chaque attribut possible:

```{r 03-Chap3-82}
library(wakefield)
ord = r_sample_ordered(n=50, c(1:7), prob = c(1,0,1,0,4,8,10), name = "Réponse")
ord = as.numeric(ord)

```

a.  Obtenez la médiane de cette variable

```{r 03-Chap3-83}
cat("Médiane de ord = ",median(ord))
```

b.  Calculez l'écart inter-quartile pour ces données

```{r 03-Chap3-84}
cat("EIQ de ord = ",IQR(ord))
```

c.  Obtenez un diagramme en barres pour cette variable

```{r 03-Chap3-85}
barplot(table(ord))
```

d.  Obtenez un diagramme en boîte pour cette variable

```{r 03-Chap3-86}
boxplot(ord)
```

e.  Quelle est votre perception de la dispersion de cette variable?

f.  Obtenez la mesure de consensus pour cette variable, et qualifiez la dispersion à partir du résultat obtenu.

```{r 03-Chap3-87}
library(agrmt)
consensus(table(ord))
```

8.  Faites l'expérience de différentes distributions en modifiant, dans le code R utilisé ci-dessus, les probabilités des différents attributs. Le vecteur définissant **prob=** correspond aux poids accordés aux attributs 1 à 7. Les probabilités sont simplement les poids relatifs: **prob/sum(prob)**. Expérimentez différentes configurations pour apprécier le fonctionnement de l'indice de consensus. Comparez les intervalles inter-quartiles obtenus dans chaque cas, avec celui obtenu en (4). Comparez de la même manière les indices de consensus.

### Exercice 5:

1.  **Générez un ensemble de 500 observations Gaussiennes avec une moyenne avoisinnant 50 et un écart-type avoisinnant 10.**

```{r 03-Chap3-88}
x = rnorm(500,50,10)
head(x,10)
```

2.  **Obtenez les indices de tendance centrale: moyenne et médiane**

```{r 03-Chap3-89}
cat("Moyenne = ",mean(x))
cat("Médiane = ",median(x))
```

3.  **Obtenez les indices de dispersion**

```{r 03-Chap3-90}
cat("Écart Inter-Quartile = ",IQR(x))
cat("Déviation Moyenne    = ",mad(x,center=median(x)))
cat("Déviation moyenne    = ",mad(x,center=mean(x)))
cat("Variance             = ",var(x))
cat("Écart-Type           = ",sd(x))
```

4.  **Obtenez les indices de symétrie et de voussure**

```{r 03-Chap3-91}
library(moments)
cat("Indice de symétrie  = ",skewness(x))
cat("Indice de voussure  = ",kurtosis(x))
```

5.  **Obtenez la distribution des fréquences et les représentations graphiques habituelles**

```{r 03-Chap3-92}
TF = TblFreq2(x)
kable(TF, booktabs=TRUE, caption="Distribution de Fréquences") %>%
kable_styling(full_width = TRUE)
```

6.  **Sous quel score trouve-t'on 67% de la distribution?**

```{r 03-Chap3-93}
cat("P67 = ",x[0.67*length(x)])
```

7.  **Quelle est la proportion des scores de la distribution sous X = 40?**

```{r 03-Chap3-94}
cat("RC40 = ",sum(x<=40)/length(x))
```

8.  **Quelle est la probabilité, en choisissant une observation au hasard, d'obtenir un score inférieur à 35, ou supérieur à 65?**

```{r 03-Chap3-95}
cat("P[(x<35) ou (x>57)] = ",sum((x<35)|(x>70))/length(x))
```

9.  **Déterminez les valeurs limitant les 95% centraux de la distribution.**

```{r 03-Chap3-96}
cat("P[",quantile(x,0.025),"< x < ",quantile(x,0.975),"] = 0.95")
```

10. **Quelle est la proportion des scores compris entre 38 et 57?**

```{r 03-Chap3-97}
cat("P[38 < x < 57] = ",sum((x>=38)&(x<=57))/length(x))
```

<!--chapter:end:03-Chap3.Rmd-->

---
title: "Échantillonnage - Méthodes"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
---


```{r 04-Chap4-1, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```

```{r 04-Chap4-2, include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r 04-Chap4-3,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(moments,formatR,kableExtra,sampler,dplyr,agrmt)
```

# Échantillonnage et Distribution d'échantillonnage

## Introduction

L'échantillonnage, processus par lequel des observations sont faites sur une portion (souvent infime!) d'une population, représente le point de départ de l'acquisition de nouvelles connaissances dans un domaine donné. Il occupe donc une place centrale dans le contexte de la recherhe scientifique. Dans cette section, nous abordons quelques-unes des méthodes d'échantillonnage, et les mettons en oeuvre dans le contexte de simulations.

Les populations étudiées sont soit de tailles infinies, ou finies mais trop grandes pour permettre un accès à toutes les entités qui les composent. Un ou plusieurs échantillons tirés de ces populations permettent de contourner ce problème, au prix d'une certaine imprécision.

Pour qu'un échantillon puisse nous informer adéquatement sur les caractéristiques de la population à l'étude, certaines conditions doivent être respectées:
  1. La sélection des entités lors de la création de l'échantillon doit être aléatoire
  2. Chaque entité dans la population doit avoir une chance égale d'être sélectionnée
  3. Si ces deux conditions sont respectées, on pourra croire que l'échantillon obtenu est **représentatif** de la population d'où il est tiré.
  
Plusieurs méthodes d'échantillonnage existent:

1. Échantillonnage probabiliste:  l'objectif est de maximiser nos chances d'obtenir un échantillon représentatif de la population en s'assurant que chaque entité de cette population a une chance égale de faire partie de l'échantillon
  a. Échantillonnage aléatoire simple
  b. Échantillonnage systématique
  c. Échantillonnage de groupements
  d. Échantillonnage stratifié
2. Échantillonnage non-probabiliste: l'objectif est d'obtenir un échantillon qui n'est pas nécessairement représentatif de la population puisque les entités de la population n'ont pas toutes les mêmes chances d'en faire partie. Un tel échantillon peut toutefoir être utile dans un contexte de pré-expérimentation pour vérifier un instrument, par exemple.
  a. Échantillonnage de convenance
  b. Échantillonnage limité par quota
  c. Échantillonnage selon le jugement d'une autorité
  d. Échantillonnage 'boule de neige'
  e. Échantillonnage à partir de banques de sujets volontaires
 
Dans les paragraphes suivants, seules les méthodes probabilistes seront décrites et simulées.
  
## La population

Supposons une population finie de 1000 entités évalués sur une variable $x$ donnée:

```{r 04-Chap4-4}
ID = 1:1000  # Un numéro est assigné à chaque membre de la population
x = round(rnorm(1000,runif(1,40,60),runif(1,8,12)),2)
dfx = data.frame(ID,x)  # Créer une structure contenant les données
head(dfx,10)
```
On s'attend à ce que cette population ait une moyenne comprise entre 40 et 60, et un écart-type compris entre 8 et 12. Mais on ne connait pas les valeurs exactes de ces paramètres. L'objectif d'une recherche scientifique pourrait être de les estimer. Pour les besoins des démonstrations à venir, obtenons ces valeurs:

```{r 04-Chap4-5}
paste("La moyenne de la population   = ",mean(dfx$x))
paste("L'écart-type de la population =",sd(dfx$x))
```


### Échantillonnage aléatoire simple:

Comme son nom l'indique, cette méthode procède simplement en choisissant au hasard le nombre d'entités nécessaires pour former l'échantillon. On pourrait par exemple attribuer un numéro à chacune des 1000 entités de la population, et choisir dans un tableau de nombres aléatoires les numéros des entités sélectionnées. Sous R, on utilise, en supposant un échantillon de 50 sujets:

```{r 04-Chap4-6}
IDx = sample(dfx$ID,50,replace=FALSE) # Obtenir les numéros de 50 sujets
x1 = dfx$x[IDx]
paste("Moyenne de X1    =",round(mean(x1),digits=2)) 
paste("Écart-type de X1 =",round(sd(x1),digits=2))
```

### Échantillonnage systématique:

Cette méthode implique une sélection basée sur un intervalle d'échantillonnage déterminé. Par exemple, on peut choisir les entités à intervalles de 20 unités dans la séquence de leur numéro d'identification:

```{r 04-Chap4-7}
IDx2 = seq(from=1,to=1000,by=20)
x2 = dfx$x[IDx2]
paste("Moyenne de X2    =",round(mean(x2),digits=2))
paste("Écart-type de X2 =",round(sd(x2),digits=2))

```

### Échantillonnage de groupements (ou de grappes):

Idéalement, la population serait divisée en groupements de tailles égales. On sélectionnerait ensuite, de manière aléatoire, un certain nombre de groupes, et tous les membres des groupes sélectionnés composeraient l'échantillon.  En pratique, une telle réalisation est peu probable, mais si on suppose des groupes de tailles approximativement égales, on peut utiliser le même concept.  

Par exemple, la population des élèves du niveau Fondamental en Haiti est répartie dans de nombreuses écoles à l'échelle du pays. On pourrait sélectionner aléatoirement un certain nombre d'écoles accueillant entre 250 et 500 élèves, et former l'échantillon à partir de l'ensemble des élèves fréquentant les écoles sélectionnées.

Ajoutons des groupements à notre population de 1000 entités, chaque groupement comptant 10 éléments, pour un total de 100 groupes: 

```{r 04-Chap4-8}
dfx$Grappe <- c(rep(c(1:100), each=10))
head(dfx)
tail(dfx)
```
Pour les besoins d'une recherche, on a besoin d'un échantillon de 50 sujets, et l'échantillonnage de groupements est envisagé. On a donc besoin de sélectionner au hasard 5 groupes, et de former l'échantillon avec l'ensemble des entités composant ces groupes:

```{r 04-Chap4-9}
grp = sample(1:100,5,replace=FALSE) # Sélection des groupes
x3 = vector()
for (i in grp){
  x3 = append(x3,dfx$x[i == dfx$Grappe])
}
cat("Groupes sélectionnés: ",grp)
paste("Moyenne de X3    =",round(mean(x3),digits=2))
paste("Écart-type de X3 =",round(sd(x3),digits=2))   

```

### Échantillonnage stratifié:

L'échantillonnage stratifié peut se faire de deux manières:
  1. Échantillonnage proportionnel:
    - La population se compose de sous-groupes naturels correspondant à différentes proportions au sein de cette population. La méthode implique que l'on prenne en compte ces proportions dans la composition de l'échantillon: on reproduit dans l'échantillon la représentation proportionnelle de la population. 
  2. Échantillonnage non-proportionnel: La population est divisée en plusieurs strates, et l'échantillon est formé par sélection aléatoire à l'intérieur de chacune des strates, sans tenir compte de la proportion que représente la strate au sein de la population.
  
Par exemple, ajoutons une variable indiquant 10 sous-groupes dans notre population: sg1 à sg10. Chaque sous-groupe comprend 100 entités (mais cette égalité n'est pas nécessaire!):

```{r 04-Chap4-10}
dfx$sgrp <- c(rep(c(1:10), each=100))
head(dfx)
tail(dfx)
```

Formons un échantillon de 50 sujets en sélectionnant aléatoirement 5 sujets dans chacun des 10 sous-groupes que contient la population:

```{r 04-Chap4-11}
x4 = vector()
for (i in 1:10){
  grpx = dfx$x[dfx$sgrp == i]
  x4 = append(x4, sample(grpx,5))
}
paste("Moyenne de X4    =",round(mean(x4),digits=2))
paste("Écart-type de X4 =",round(sd(x4),digits=2))

```
La fonction **ssamp**, qui se trouve dans la librairie **sampler** permet de produire des échantillons stratifiés de manière efficace. Par exemple:


```{r 04-Chap4-12}
library(sampler)
x4b = ssamp(dfx,200,strata=sgrp)
paste("Moyenne de X    =",round(mean(x4b$x),digits=2))
paste("Écart-type de X =",round(sd(x4b$x),digits=2))
```
Il est donc possible, à l'aide de cette fonction, de répéter une expérience un certain nombre de fois. Par exemple, tirons 15 échantillons stratifiés de 25 observations chacun, à partir de la population de 1000 sujets définie plus haut, et pour chacun de ces échantillons, calculons la moyenne:

```{r 04-Chap4-13}
moy = vector()
for (i in 1:15){
  samp = ssamp(dfx,200,strata=sgrp)
  moy[i] = mean(samp$x)
  cat("Moyenne de l'échantillon ",i," = ",moy[i],"\n")
}
```
On prendra note de la variation des moyennes, d'un échantillon à l'autre. Cette variation se trouve au coeur de toutes les procédures de statistiques inférentielles dont il sera question dans les unités suivantes.
 

#### NOTE: Les simulations d'échantillonnage présentées ci-dessus se font dans des conditions idéales. dans tous les cas, l'échantillonnage était parfaitement aléatoire. Ces conditions sont difficiles à rencontrer dans le monde réel. Les facteurs susceptibles de miner la représentativité d'un échantillon sont nombreux et ils sont la source de beaucoup de résultats contradictoires obtenus en recherche. Il faut simplement mettre en oeuvre tout ce qui peut l'être, afin de s'approcher de cet idéal.

### Exercice 1

Pour les exercices suivants, considérez la population suivante, contenant 10,000 élèves de niveau Fondamental provenant de 5 régions administratives. Dans chacune de ces régions, on compte  10 écoles comptant 200 élèves chacune. Et dans chacune des écoles, 10 classes se composent de 20 élèves chacune. Pour chaque élève, on obtient le score de performance à l'examen de mathématiques du MENFP. Les données se trouvent dans l'objet  **res**.

```{r 04-Chap4-14}
math = vector()
region = 1:5
ecole = 1:10
classe = 1:10
for(region in 1:5){
  for(ecole in 1:10){
    for(classe in 1:10){
    math = append(math,rnorm(20,65,10))
    }
  }
}
Region = rep(1:5,each=2000)
Ecole = rep(1:10,each=200)
Classe = rep(rep(1:10,each=20),each=50)
res = data.frame(Region,Ecole,Classe,math)
```

1. Effectuez un échantillonnage aléatoire simple de manière à obtenir un échantillon de 50 élèves, et obtenez une description des observations (tendance centrale, dispersion, histogramme, etc.)

```{r 04-Chap4-15}
IDx = sample(1:10000,50,replace=FALSE) # Obtenir les numéros de 50 sujets
x1 = res$math[IDx]
paste("Moyenne    =",round(mean(x1),digits=2)) 
paste("Écart-type =",round(sd(x1),digits=2))
hist(x1)
```

2. Effectuez un échantillonnage systématique pour obtenir un échantillon aléatoire composé de 100 élèves, et obtenez une description des données recueillies relatives à la performance en mathématiques.

```{r 04-Chap4-16}
IDx2 = seq(from=1,to=10000,by=100)
x2 = res$math[IDx2]
paste("Moyenne    =",round(mean(x2),digits=2))
paste("Écart-type =",round(sd(x2),digits=2))
hist(x2)
```
3. Effectuez un échantillonnage par grappes, les grappes étant représentées par les différentes classes, de manière à obtenir un échantillon aléatoire composé de 200 élèves. Puisque tous les élèves d'une classe seront sélectionnés, on devra échantillonner 10 classes à l'échelle du pays. Obtenez une description des données recueillies relatives à la performance en mathématiques.

```{r 04-Chap4-17}
  x3 = vector()
  reg = sample(1:5,10,replace=TRUE) # sélection d'une région
  ecl = sample(1:10,10,replace=TRUE)# sélection d'une école
  cls = sample(1:10,10,replace=FALSE)# sélection d'une classe
for(i in 1:10){
  cat("Région",reg[i],"École ",ecl[i],"Classe ",cls[i],"\n")
  select = which((res$Region==reg[i])&(res$Ecole==ecl[i])&(res$Classe==cls[i]))
  x3 = append(x3,res$math[select])
  }

paste("Moyenne    =",round(mean(x3),digits=2))
paste("Écart-type =",round(sd(x3),digits=2))   
hist(x3,breaks="FD")
```
4. Effectuez un échantillonnage stratifié, les strates étant représentées par les différentes régions, de manière à obtenir un échantillon aléatoire composé de 200 élèves. Obtenez une description des données recueillies relatives à la performance en mathématiques.

```{r 04-Chap4-18}
library(sampler)
x4 = ssamp(res,200,strata=Region)
paste("Moyenne     =",round(mean(x4$math),digits=2))
paste("Écart-type  =",round(sd(x4$math),digits=2))
hist(x4$math)
```


### Fonctions d'échantillonnage aléatoire:

Dans les sections suivantes, nous examinons les différents outils disponibles sous R pour générer des données aléatoires, et en profiterons pour examiner certains aspects de la distribution des données obtenues de cette manière. Quelques fonctions seront aussi définie pour faciliter le travail...

#### **Génération de nombres entiers**:
  
La fonction suivante permet d'obtenir un échantillon aléatoire provenant d'une population de valeurs entières. En particulier, il s'agit d'une fonction générant n (n=50 par défaut) valeurs observées lorsque k expériences aléatoires (k = 10 par défaut) résultant en l'une ou l'autre de deux valeurs (eg: 0 et 1, ou VRAI et FAUX) lorsque la probabilité de l'une de ces valeurs est spécifiée (p = 0.5 par défaut). Par exemple, on lance 10 fois une pièce de monnaie (expérience aléatoire) et on comptabilise le nombre de 'FACEs', en supposant que la probabilité d'obtenir ce résultat est p = 0.5.

Copiez/Collez la fonction intégralement dans la console pour l'introduire dans votre système.

```{r 04-Chap4-19}
RandBin = function(N=50,n=10,p=0.5){
  ppop = runif(1,runif(1,0,p),runif(1,p,1))
  x = rbinom(N,n,ppop)
  return(x)
}

```
Vous n'avez à exécuter ce code qu'une seule fois pour que la fonction s'ajoute aux objets composant votre espace de travail. Par la suite, vous pouvez l'utiliser en inscrivant dans la console:

```{r 04-Chap4-20}
RandBin()

```
ou, pour sauvegarder les données dans une variable et en spécifiant 15 expériences aléatoires où la probabilité d'un 'succès' est p = 0.3, et en donnant un aperçu des 10 premières données:

```{r 04-Chap4-21}
x = RandBin(N=100,n=15,p=0.5)
head(x,10)

```

Lorsque les données sont ainsi générées, il est souvent utile d'examiner leur distribution, chose que nous ferons systématiquement dans les discussions futures, Par exemple traçons un histogramme représentant la distribution de la variable x que nous avons générée ci-dessus::
```{r 04-Chap4-22}
hist(x)
```

#### Exercice 1  
En utilisant la fonction RandBin(), générez plusieurs ensembles de données, en variant le nombre d'expériences aléatoires, la probabilité associée à un 'succès' et la taille de l'échantillon. Dans chaque cas, obtenez un histogramme et prenez note de sa forme générale: la distribution est-elle symétrique? Si elle est asymétrique, l'extrémité 'longue' se trouve-t'elle à gauche (asymétrie négative) ou a droite (asymétrie positive)?

**Petit truc**:  Vous pouvez revenir à la commande précédente dans la console, en appuyant sur la *flèche vers le haut*, sur votre clavier. Dès lors, vous pouvez changer les valeurs des arguments de la fonction...

#### Génération de données équiprobables, échelle continue:
  
  La fonction RandBin() permet de générer des nombres entièrs, ce qui est utile dans plusieurs situations. La fonction suivante fonctionne de manière similaire, mais cette fois, les données générées sont des nombres réels ('Floating point'). Copiez/collez le code suivant dans la console, comme vous l'avez fait pour la fonction RandBin().
  
```{r 04-Chap4-23}
RandUnif = function(n=50,a=c(10,20),b=c(25,50)){
  if(b[1]<a[2]){
    stop("b[1]<a[2]! Veuillez corriger!")
  }
  liminf = runif(1,a[1],a[2]) # limite inférieure (Pop)
  limsup = runif(1,b[1],b[2]) # limite supérieure (Pop)
  x = runif(n,liminf,limsup)
return(x)
}
```
Cette fonction génère n (n=50 par défaut) données tirées d'une population où la valeur minimum est inconnue mais comprise entre deux limites spécifiées par l'argument 'a', La valeur maximum est aussi inconnue, mais comprise entre deux limites spécifiées par l'argument 'b'. Il est nécessaire, évidemment, que la limite inférieure de **b** soit plus grande que la limite supérieure de **a**.

Par exemple, générons 100 données à partir d'une population dont le minimum est compris entre 20 et 30, et le maximum est compris entre 40 et 60. Vérifions les 15 premières données, et ensuite, examinons la forme de la distribution en traçant un histogramme:

```{r 04-Chap4-24}
x = RandUnif(n=100,a=c(20,30),b=c(40,60))
head(x,15)
hist(x,xlim = c(min(x),max(x)))
```
  
#### Exercice 2:

En utilisant la fonction RandUnif(), générez plusieurs ensembles de données, en variant la taille des échantillons, et les limites des valeurs minimales et maximales. Dans chaque cas, obtenez un histogramme et prenez note de sa forme générale: la distribution est-elle symétrique? Si elle est asymétrique, l'extrémité 'longue' se trouve-t'elle à gauche (asymétrie négative) ou a droite (asymétrie positive)? Comparez les distributions obtenues pour des tailles d'échantillon croissantes. Pouvez-vous en tirer une conclusion générale?

### Génération de données: population '**normalement distribuée**'

Comme la fonction RandUnif(), la fonction suivante permettra de générer des données quantitatives, continues. La distribution de ces données aura un 'centre' identifié par une valeur moyenne (moy = 100 par défaut) et une certaine dispersion identifié par l'argument 'et' (et = 15 par défaut). Par défaut, la taille de l'échantillon généré est n = 50. Comme pour les fonctions précédentes, copiez/collez le code suivant dans la console:

```{r 04-Chap4-25}
RandNorm = function(n=50,moy=100,et=15){
  sem = et/sqrt(n)
  moypop = runif(1,moy-1.96*sem,moy+1.96*sem)
  etpop = runif(1,et-et/3,et+et/3)
  x = rnorm(n,moypop,etpop)
  return(x)
}

```

Par exemple, générons 100 données à partir d'une population dont le centre occupe une position inconnue, mais devrait se situer autour de 50, et une dispersion aussi inconnue, mais devrait se trouver autour de et = 10, dans un intervalle compris entre et plus ou moins et/3.  Vérifions les 15 **dernières** données, et ensuite, examinons la forme de la distribution en traçant un histogramme comportant 25 barres:

```{r 04-Chap4-26}
x = RandNorm(n=100,moy=50,et=10)
tail(x,15)
hist(x,breaks=25)

```

#### Exercice 3:

En utilisant la fonction RandNorm(), générez plusieurs ensembles de données, en variant la taille des échantillons, et arguments **moy** et **et**. Dans chaque cas, obtenez un histogramme et prenez note de sa forme générale: la distribution est-elle symétrique? Si elle est asymétrique, l'extrémité 'longue' se trouve-t'elle à gauche (asymétrie négative) ou a droite (asymétrie positive)? Comparez les distributions obtenues pour des tailles d'échantillon croissantes. Pouvez-vous en tirer une conclusion générale?  Êtes-vous en mesure de déterminer la position du centre? Sinon, pourquoi ne pouvez-vous pas le faire?

Pour les dernières données générées, produisez différents histogrammes en modifiant à chaque fois le nombre de barres le composant (argument '**breaks**' de la fonction hist()). Est-ce que **plus** de barres produit nécessairement un meilleur graphique?


### Échantillonnage à partir d'une population **finie**

Les fonctions présentées ci-dessus nous ont permis de tirer des échantillons à partir de populations que l'on pouvait considérer comme **infinies**. Supposons maintenant que nous voulions étudier une population finie, qui pourrait en principe être examinée dans son intégralité, mais que nous préférons l'approcher en y tirant des échantillons.

Commençons par générer une population composée de 2000 données, dont le centre se trouverait aux environs de moy = 50, et une dispersion avoisinnant  et = 10:

```{r 04-Chap4-27}
pop = RandNorm(2000,50,10)

```
Pour l'instant, nous ne connaissons rien de cette population, mais on pourrait en apprendre un peu en tirant un échantillon de 25 observations. La fonction **sample()**, faisant partie de l'environnement R par défaut, pourra nous être utile:

```{r 04-Chap4-28}
x = sample(pop,25)
x
hist(x)

```

La fonction **sample()** génère un échantillon aléatoire simple à partir d'une population définie. De ce fait, il y a de fortes chances que l'échantillon obtenu réflète relativement bien les caractéristiques de la population.  En examinant l'histogramme illustrant la distribution de **x** pour notre échantillon, que pouvons-nous dire de la population?  La distribution est-elle symétrique? Où placeriez-vous le centre de la distribution?

Répétons l'expérience et tirons un second échantillon:

```{r 04-Chap4-29}
x = sample(pop,25)
x
hist(x)

```
Rappelons que la population (variable 'pop') n'a pas changé: elle constitue un ensemble fixe duquel on tire des échantillons dont les caractéristiques varient d'un échantillon à l'autre.

#### Exercice 4

Tirez des échantillons de la population définie plus haut ('pop') en variant leur taille. Essayez n={10, 25, 50, 100, 200}. Dans chaque cas, construisez un histogramme. Quel semble être l'impact de la taille des échantillons?

### Proportions des données dans une distribution

Il est souvent utile de connaître la proportion des données qui se trouvent dans certaines zones de la distribution.  Par exemple, considérons un échantillon de n=100 observations tiré de la population 'pop':

```{r 04-Chap4-30}
x = sample(pop,100)
head(x)
hist(x)
```
On peut vérifier que l'échantillon contient bien 100 observations:

```{r 04-Chap4-31}
n = length(x)
n

```

En effet, la fonction **length()** retourne le nombre d'éléments contenu dans son argument.

Quelle serait le nombre de scores supérieurs à 45, dans cet échantillon?  Pour obtenir la réponse, nous utiliserons un opérateur relationnel: x > 45. Cet opérateur sera appliqué à chacune des 100 données, et retournera 'TRUE' (avec une valeur de '1') si effectivement la donnée est supérieure à 45, et 'FALSE' (avec une valeur de '0') si ce n'est pas le cas. On obtiendra donc un vecteur de TRUE/FALSE (1/0), et la somme de ce vecteur correspondra ce que nous recherchons:

```{r 04-Chap4-32}
sum(x > 45)

```

Puisque la taille de l'échantillon est n=100, il est facile d'obtenir la proportion des données qui sont supérieures à 45:

```{r 04-Chap4-33}
sum(x > 45)/length(x)

```

De manière identique, on peut déterminer la proportion des données qui se trouvent **sous** 45:

```{r 04-Chap4-34}
sum(x < 45)/length(x)
```

Au fait, ce que nous venons d'accomplir est le calcul d'un **rang centile**: proportion d'une distribution qui se trouve sous un score donné. On dira ici que le rang centile de x=45 est 0.32 ou 32%.

#### Exercice 5:

1. Tirez un échantillon aléatoire de 200 observations de la population 'pop'.
2. Quelle est la proportion des données se trouvant au-dela de 60?
3. Quelle est la proportion des données se trouvant en-deça de 40?
4. Quelle est la proportion des données comprises entre 40 et 60?
5. Trouvez le rang centile de x = 60.

### Au-delà ou en deçà de quelle valeur retrouve-t'on une certaine proportion des données?

Posons maintenant la question inverse: on veut déterminer quelle valeur coupe la distribution en deux parties: une partie au-delà et l'autre partie en-deçà. Par exemple, au-delà de quelle valeur, retrouve-t'on 10% de la distribution?

Si on a 10% de la distribution au-dessus de la valeur recherchée, on aura 90% de la distribution en-dessous de cette valeur. Pour un échantillon de 100 données mises en ordre croissant, on pourrait récupérer le 90ième score, qui correspondrait à celui qu'on cherche. Pour un échantillon de 200 observations, on devrait récupérer le 180ième score dans la séquence ordonnée. En général, il suffirait de multiplier la proportion considérée (90%) par le nombre de données dans l'échantillon (200) pour obtenir la position du score recherché dans la séquence ordonnée des scores (180).

Au fait, ce que nous recherchons ici s'appelle **percentile**: valeur sous laquelle on retrouve une certaine proportion d'une distribution.

On peut facilement trouver tout percentile d'une distribution. Trouvons le 45ième percentile de la distribution de notre échantillon:

```{r 04-Chap4-35}
P45 = x[0.45*length(x)]
P45

```
Ici, l'utilisation de "[ ]", on l'a vu, permet de référer à l'indice d'une donnée à l'intérieur d'un vecteur. On trouve donc que 68 est le 45ième percentile, ou le score sous lequel on trouve 45% de la distribution.

### La fonction **quantile()**

Le calcul des percentiles d'une distribution est facilitée par la fonction **quantile**, dont l'argument est la proportion d'une distribution se trouvant sous le score recherché. Pour reprendre l'exemple précédent:

```{r 04-Chap4-36}
quantile(x,.45)

```

Il est possible d'inclure autant de proportions que nécessaire:

```{r 04-Chap4-37}
pr = c(0.1,0.25,0.5,0.75)
quantile(x,pr)
```

Dans l'histogramme illustrant la distribution des scores, on peut visualiser ces points:

```{r 04-Chap4-38}
hist(x)
abline(v=quantile(x,pr),col="red",lwd=3)

```

#### Exercice 5:

1. Générez une population de 5000 observations, en utilisant la fonction **RandNorm()** et en spécifiant une moy=75 et et=12.
2. De cette population, tirez un échantillon de 250 scores.
3. Calculez les 5ième, 10ième, 25ième, 50ième, 75ième, 90ième et 95ième percentiles de la distribution de votre échantillon
4. Calculez les rangs centiles des scores: 50, 60, 70, 80 et 90.


#### Prenez le temps de relire l'ensemble de cette section. Exercez-vous en générant des populatons de différents types, et de ces populations, tirez des échantillons. Pour les échantillons tirés d'une même population, comparez les moyennes, médianes, écarts-types, ou tout indice couvert dans les sections précédentes. Si vous observez des différences, comment les expliqueriez-vous?


## Complément: Génération d'une Population: Haiti 2019

Afin d'illustrer le processus de recherche et d'analyse de données, la simulation offre des avantages très intéressants. A partir d'une banque de données simulant un contexte donné, il est possible d'échantillonner un ensemble de scores et de les traiter comme s'il s'agissait de données réelles.

Dans cette section, ce processus est illustré de la génération d'une population fictive, mais correspondant démographiquement à la population Haitienne de 2019. 

Le code R suivant génère des données pour 11,263,079 d'individus, soit la population Haitienne en 2019. La répartition des individus dans les 10 départements du pays est respectée. Les variables qui composent le fichier de sortie sont:

1. **GENRE**: "M" = Masculin
              "F" = Féminin
              
2. **AGE**:   Âge, en unité d'années

3. **EDUC**:  Niveau de scolarité atteint à l'âge de 25 ans
              - Aucun
              - PréScol
              - Fond.1
              - Fond.2
              - Fond.3
              - Second
              - Univ.1
              - Univ.2
              - Univ.3
              
4. **DEPARTEMENT**: Département de résidence

![Départements Administratifs d'Haiti](./images/Haiti.png)
              
5. **CondxVie**: Indice général de condition de vie, sur une échelle de 0 à 50. Plus l'indice est élevé, plus les conditions de vie de la personne sont bonnes.

6. **CRIMIDX**: Indice de propension à la criminalité et à la corruption, sur une échelle de 0 à 150. Plus l'indice est élevé, plus l'individu a des tendances criminelles.

7. **HabilCog**: Indice d'habiletés cognitives, sur une échelle de 20 à 200. Plus cet indice est élevé, plus les habiletés intellectuelles et cognitives de l'individu sont grandes.

8. **PrefPres**: Préférence pour un candidat donné à la présidentielle de 2022. Cinq candidats sont en liste: CandA - CandE.

9. **OpinConst**: Opinion concernant à un nouveau projet de Constitution:
    1. "Tout à fait d'accord"
    2. "D'accord"
    3. "Indécis"
    4. "Pas d'accord"
    5. "Pas du tout d'accord"

L'objet sortant est un 'DATA.FRAME' appelé **HaitiPop**.

### Le script:
```{r GenPop, message=FALSE}

set.seed(1234567)
# HaitiPop: Simultation de la population Haitienne (2019)
# Genre Féminin, effectif par groupe d'âge (données de 2019)
Age_female=c(620842, 608143, 590011, 565536, 531966, 493101, 458841,
             410081, 309636, 245577, 218103, 187285, 149090, 118763,
             80994, 60334, 34915, 15440, 5211, 1397, 217) 

# Genre Masculin, effectif par groupe d'âge (données de 2019)
Age_male=c(645049, 629568, 607263, 571570, 525211, 479976, 442822, 
           390944, 286062, 221042, 199898, 169226, 135904, 102412, 
           64604, 47325, 24681, 10151, 3085, 720, 83)  
PopTotale = sum(Age_male)+sum(Age_female)

# Génération des âges et du genre
age = c(seq(4,104,by=5))
AGE = vector()
GENRE = vector()
for(i in 1:length(age)){
  AGE = append(AGE,floor(runif(Age_male[i],(age[i]-4),age[i]+5)))
  AGE = append(AGE,floor(runif(Age_female[i],(age[i]-4),age[i]+5)))
  GENRE = append(GENRE,rep("M",Age_male[i]))
  GENRE = append(GENRE,rep("F",Age_female[i]))
}
Tdt = data.frame(AGE,GENRE)
Tdt = Tdt[sample(nrow(Tdt)),1:2]
# Génération des Départements et répartition de la population
Dept = matrix(c(1:10,"Artibonite","Centre","Grand'Anse","Nippes",
                "Nord","Nord-Est","Nord-Ouest","Ouest","Sud-Est",
                "Sud"),nrow=2,byrow=TRUE)
# cat("Identification des Départements: ","\n",t(Dept))
# Proportion de la population par département
PropDept = c(0.158316776, 0.037133348, 0.024201755, 0.018140699, 
             0.057563731, 0.022548639, 0.042675426, 0.24647869, 
             0.051350001, 0.066312107) 
DEPARTEMENT = sample(Dept[1,],size=PopTotale,replace=TRUE, prob=PropDept)
# Indice de condition de vie, distribution asymétrique
CondxVie = rchisq(PopTotale,6)
Tdt = data.frame(Tdt,DEPARTEMENT,CondxVie)
# Génération des niveaux de scolarité atteints, population adulte
EDUC=rep("NA",PopTotale)
NivScol = c("Aucun","PréScol","Fond.1","Fond.2","Fond.2","Second",
            "Univ.1","Univ.2","Univ.3")
PropNivScol = c(0.2, 0.07, 0.2, 0.18, 0.12, 0.09, 0.06, 0.05, 0.03) 
EDUC[Tdt$AGE>25] = sample(NivScol,sum(Tdt$AGE>25),prob=PropNivScol,replace=TRUE)

# Génération de l'indice de criminalité

CRIMIDX = rep("NA",PopTotale)
idx = vector()
for(i in 1:10){
  idx[i] = sample(c(1,-1),1)*runif(1,0,30)
}

CRIMIDX = 0.3*CondxVie + 3.5*(Tdt$GENRE=="M")-1.2*Tdt$AGE
CRIMIDX = CRIMIDX +1.5*(EDUC<"Second")+10 +rnorm(1,0,4)
CRIMIDX = CRIMIDX + idx[as.numeric(DEPARTEMENT)]
CRIMIDX = CRIMIDX + (min(CRIMIDX)<0)*abs(min(CRIMIDX))
CRIMIDX[which(Tdt$AGE<15)] = NA

# Génération de l'indice d'habiletés cognitives
HabilCog = rnorm(PopTotale,100,15)
idx = vector()
for(i in Dept){
  idx[i] = sample(c(1,-1),1)*runif(1,0,10)
}
HabilCog = HabilCog+idx[DEPARTEMENT]

# Génération de la préférence pour un candidat présidentiel
# et de l'Opinion concernant la proposition d'une nouvelle Constitution

library(wakefield)
PrefPres = rep(NA,PopTotale)
OpinConst = rep(NA,PopTotale)

# Probabilités associées aux variables PresProb et OpinProb
PresProb = matrix(c(0.58, 0.41, 0.003, 0.004, 0.003, # Nord-Ouest, Nord et Nord-Est
                    0.41, 0.54, 0.003, 0.004, 0.043, # Artibonite et Centre
                    0.27, 0.65, 0.006, 0.01, 0.064,  # Ouest
                    0.15, 0.23, 0.38, 0.19, 0.05),   # Grand<Anse, Nippes, Sud et Sud-Est
          nrow = 4, ncol = 5, byrow = TRUE)
OpinProb = matrix(c(0.5,0.3,0.1,0.05,0.05,           # Nord-Ouest, Nord et Nord-Est
                    0.2,0.4,0.1,0.1,0.2,             # Artibonite et Centre
                    0.01,0.09,0.2,0.4,0.3,           # Ouest
                    0.4,0.1,0.0,0.1,0.4),            # Grand<Anse, Nippes, Sud et Sud-Est
          nrow = 4, ncol = 5, byrow = TRUE)
Regions = list(c("5","6","7"), c("1","2"), c("8"),c("3","4","9","10"))         
OptPres = c("CandA", "CandB", "CandC", "CandD", "CandE")
OptOpin = c("Tout à fait d'accord","D'accord","Indécis","Pas d'accord","Pas du tout d'accord")

for (i in 1:length(Regions)){
  select = (Tdt$AGE>=18)&(DEPARTEMENT %in% Regions[[i]])
  PrefPres[select] = political(n=sum(select), x = OptPres,prob = PresProb[i,], name = "PrefPres")
  OpinC = r_sample_ordered(n=sum(select),x = OptOpin,prob = OpinProb[i,])
  OpinConst[select] = OpinC
  }

# DATA.FRAME de sortie
HaitiPop = data.frame(Tdt,EDUC,CRIMIDX,HabilCog,PrefPres,OpinConst)
```

### Sauvegarde des données de la population

Une fois générée, il est souhaitable de sauvegarder cet ensemble de données pour une utilisation ultérieure, sans que l'on ait à re-générer le tout. Plusieurs options existent.
  
1. Sauvegarde binaire: le fichier peut être lu par R, mais pas par d'autres logiciels:
  a. Écriture:  Le fichier est sauvegardé dans le répertoire de travail:
      
    (SESSION -> Set working directory -> Choose directory...):
      
```{r 04-Chap4-39, eval = FALSE}

saveRDS(HaitiPop, file = "*nom de fichier*")
```
      
  b. Lecture: Les données sont chargées dans un objet de votre choix:
      
```{r 04-Chap4-40, eval = FALSE}
HaitiPop = readRDS("*nom de fichier*")
```
      
2. Sauvegarde CSV: le fichier peut être lu par d'autres logiciels, incluant les éditeurs de texte. Le fichier est sauvegardé dans le répertoire de travail:
  
  (SESSION -> Set working directory -> Choose directory...)
    
  a. Écriture: le fihier est sauvegardé dans un fichier de votre choix, la séparation entre les éléments est le TABulateur, et on ne veut pas conserver les noms des rangées:
      
```{r 04-Chap4-41, eval = FALSE}
write.table(HaitiPop, file = "HaitiPop.csv", sep = "\t", row.names = FALSE)
```
  b. Lecture: 
```{r 04-Chap4-42, eval = FALSE}
HaitiPop = read.table(file = "HaitiPop.csv", sep = "\t")
```

### Accès aux données

Pour les besoins des exercices à venir, il convient de pouvoir échantillonner la population générée. Les paragraphes suivants présentent les stratégies possibles.

#### Échantillonner des lignes entières de la population globale:

Supposons que l'on veuille tirer un échantillon de 50 sujets choisis aléatoirement parmi l'ensemble de la population Haitienne:

```{r 04-Chap4-43}
x = HaitiPop[sample(nrow(HaitiPop),50),1:ncol(HaitiPop)]

head(x)
```
  
Essentiellement, on tire au hasard 50 numéros de rangées parmi les 11+ millions de rangées que contient la banque de données (**sample(nrow(HaitiPop),50)**); pour chacune de ces rangées, on extrait ensuite l'ensemble des variables disponibles (**HaitiPop[_indices_,1:ncol(HaitiPop)]**).

Notez que les départements de résidence sont numérotés de 1 à 10. Ces nombres correspondent à:

```{r 04-Chap4-44}
cat("Liste des départements:")
t(Dept)
```
On peut ensuite obtenir les indices quantitatifs jugés utiles. Par exemple:

```{r 04-Chap4-45}

cat("La moyenne de CRIMIDX   = ", mean(x$CRIMIDX,na.rm=TRUE))
cat("L'écart-type de CRIMIDX = ", sd(x$CRIMIDX,na.rm=TRUE))
```
Notez la nécessité d'inclure **na.rm=TRUE** puisque CRIMIDX n'est disponible que pour les sujets âgés de 15 ans et plus. Pour les sujets plus jeunes, cette donnée est donc manquante (**NA**), et l'option **na.rm=TRUE** l'élimine des calculs.

### Accès aux données d'un département particulier

Pour accéder aux données obtenues dans un département particulier, on pourra effectuer l'opération suivante:

```{r 04-Chap4-46}
cat("Liste des départements:")
t(Dept)
# Département de l'Ouest (8)
subpop = HaitiPop[which(HaitiPop$DEPARTEMENT=="8"),1:ncol(HaitiPop)]
x = subpop[sample(nrow(subpop),50),1:ncol(subpop)]
head(x,10)
```

### Accès aux HOMMES d'Artibonite:

```{r 04-Chap4-47}
cat("Liste des départements:")
t(Dept)
# Département de l'Ouest (8)
subpop = HaitiPop[which((HaitiPop$DEPARTEMENT=="1") & (HaitiPop$GENRE=="M")),1:ncol(HaitiPop)]
x = subpop[sample(nrow(subpop),50),1:ncol(subpop)]
head(x,10)
```

### Accès à la variable HabilCog, pour les femmes âgées de 50 ans et plus


```{r 04-Chap4-48}
subpop = HaitiPop[which((HaitiPop$GENRE=="F") & (HaitiPop$AGE>=50)),1:ncol(HaitiPop)]
x = subpop[sample(nrow(subpop),50),7]
head(x,10)
```
### La fonction **subset()**

La fonction **subset()** permet de simplifier quelque peu l'extraction de données d'un objet tel que **HaitiPop**. Trois arguments sont requis:  
  - Le nom de l'objet d'où on veut tirer de l'information (*HaitiPop*)  
  - La condition logique de la sélection des rangées (*AGE>=50*)  
  - La ou les variables à extraire (*c(HabilCog,IndxVie)*)  

Par exemple, pour obtenir la population des HOMMES vivant en ARTIBONITE, et extraire la variable CondxVie, on pourra lancer la commande:
```{r 04-Chap4-49}
subpop = subset(HaitiPop,((GENRE=="M")&(DEPARTEMENT=="8")),CondxVie)
```
Pour obtenir l'ensemble des femmes âgées de 50 ans et plus au pays, et obtenir la variable HabilCog, on utilisera:

```{r 04-Chap4-50}
subpop = subset(HaitiPop,((GENRE=="F") & (AGE>=50)),select=HabilCog)
```
Si toutes les variables contenues dans l'objet sont requises, il suffit d'omettre le troisième paramètre.  

L'utilisation de cette fonction, on le constatera, est beaucoup moins laborieuse que la procédure décrite plus haut.

### La fonction **filter** de la librairie **dplyr**

La fonction **filter()**, qui se trouve dans la librairie **dplyr** fonctionne de manière analogue à la fonction **subset()**, sauf qu'elle ne permet pas de sélectionner la ou les variables à inclure. Par exemple,

```{r 04-Chap4-51}
library(dplyr)
subpop = filter(HaitiPop,((GENRE=="F") & (AGE>=50)))
head(subpop)
```
Cette fonction peut être combinée avec la fonction **select()**, provenant de la même librairie. Le premier argument de cette fonction est le data.frame d'où on veut extraire des données, et le second est la liste des variables à extraire. Par exemple:

```{r 04-Chap4-52}
library(dplyr)
subpop = select(filter(HaitiPop,((GENRE=="F") & (AGE>=50))),c(AGE,HabilCog, CondxVie,CRIMIDX))
head(subpop)

```

### Exemples additionnels

Tirons un échantillon de 50 sujets âgés de 40 ans ou plus, et qui ont une préférence pour le candidat A, à la présidence, de manière à étudier la variable CondxVie:

```{r 04-Chap4-53}
# Départements du Nord (6, 7 et 8)
subpop = subset(HaitiPop,((AGE>=40)&(PrefPres==1)),CondxVie)
x = subpop[sample(1:nrow(subpop),50,replace=FALSE),]
head(x)
```
Tirons un échantillon de 20 filles âgées de 15 ans ou plus, vivant dans le département de l'Ouest, de dont l'indice de criminalité (CRIMIDX) est supérieur à 100, de manière à pouvoir examiner leur condition de vie (CondxVie):

```{r 04-Chap4-54}
# Départements de l'Ouest (8)
subpop = subset(HaitiPop,((AGE>=15)&(GENRE=="F")&(DEPARTEMENT==8)&(CRIMIDX>100)),CondxVie)
x = subpop[sample(1:nrow(subpop),20,replace=FALSE),]
head(x)
```

Pouvez-vous imaginer quelques autres sous-populations qu'il serait intéressant d'investiguer?

Certes, R nous offre tous les outils qui nous permettraient d'étudier directement les caractéristiques de cette population, mais oublions ce fait. Étudions cette population comme on le ferait en réalité, aidé des outils statistiques appropriés selon la nature de la question de recherche.

### Exercice

Considérez la population créée dans cette section, **HaitiPop**, pour répondre aux questions suivantes:

1. Tirez un échantillon de 50 observations pour chacun des 10 départements, et obtenez une description de chacune des distributions (tendance centrale, dispersion, forme) de la variable **CondxVie**

```{r 04-Chap4-55}
library(moments)
x = vector()
moy = vector()
med = vector()
stddev = vector()
skew = vector()
kurt = vector()

for (i in as.character(1:10)){
 subpop = which(HaitiPop$DEPARTEMENT==i) 
 x = HaitiPop$CondxVie[sample(subpop,50)]
 moy[i] = mean(x)
 med[i] = median(x)
 stddev[i] = sd(x)
 skew[i] = skewness(x)
 kurt[i] = kurtosis(x)
}
 res = data.frame(t(Dept),moy,med,stddev,skew,kurt)
 colnames(res) = c("Dept#","Depart","Moyenne","Médiane","Écart-Type","Symétrie","Voussure")
 kable(res, booktabs=TRUE, caption="Statistiques Descriptives / Département") %>%
  kable_styling(full_width = FALSE)

```
  
2. Tirez un échantillon de 50 observations pour chacun des 10 départements, et obtenez une description de chacune des distributions (tendance centrale, dispersion, forme) de la variable **CRIMIDX** (Prenez exemple sur l'exercice précédent!)


```{r 04-Chap4-56}
library(moments)
x = vector()
moy = vector()
med = vector()
stddev = vector()
skew = vector()
kurt = vector()

for (i in as.character(1:10)){
 subpop = subset(HaitiPop,(DEPARTEMENT==i)&(AGE>=15)) 
 x = sample(subpop$CRIMIDX,50)
 moy[i] = mean(x)
 med[i] = median(x)
 stddev[i] = sd(x)
 skew[i] = skewness(x)
 kurt[i] = kurtosis(x)
}
 res = data.frame(t(Dept),moy,med,stddev,skew,kurt)
 colnames(res) = c("Dept#","Depart","Moyenne","Médiane","Écart-Type","Symétrie","Voussure")
 kable(res, booktabs=TRUE, caption="Statistiques Descriptives / Département") %>%
  kable_styling(full_width = FALSE)

```

  
3. Pour le département du Centre, quelle serait la différence entre l'indice d'habileté cognitive (**HabilCog**) des hommes et celui des femmes, si on examinait la question à l'aide d'échantillons de taille n = 25? (Note: le département du Centre porte le numéro "2"!)

```{r 04-Chap4-57}
subpop1 = which(HaitiPop$DEPARTEMENT=="2" & HaitiPop$GENRE=="M")
subpop2 = which(HaitiPop$DEPARTEMENT=="2" & HaitiPop$GENRE=="F")
smpl1 = HaitiPop$HabilCog[sample(subpop1,25)]
smpl2 = HaitiPop$HabilCog[sample(subpop2,25)]
cat("La différence entre les moyennes des Hommes et des Femmes est ",mean(smpl1)-mean(smpl2))

```
  
4. Est-ce que l'indice de condition de vie diffère entre le département du Nord (#5), et le département du Sud (#10)? Utilisez des échantillons de 25 sujets. Puis augmentez graduellement la taille des échantillons: 50, puis 100, puis 500. Qu'observez-vous?

```{r 04-Chap4-58}
subpop1 = which(HaitiPop$DEPARTEMENT=="5")
subpop2 = which(HaitiPop$DEPARTEMENT=="10")
smpl1 = HaitiPop$CondxVie[sample(subpop1,25)]
smpl2 = HaitiPop$CondxVie[sample(subpop2,25)]
cat("La différence entre les moyennes est ",mean(smpl1,na.rm=TRUE)-mean(smpl2,na.rm=TRUE))

```
5. A l'échelle du pays, quel candidat à la présidence a la faveur populaire? Utilisez un échantillon aléatoire composé de 1200 sujets pour le déterminer.

```{r 04-Chap4-59}
library(agrmt)
subpop = subset(HaitiPop,(AGE>=18))$PrefPres
x = sample(subpop,1200)
tbl = table(x)
tbl
tblm=as.matrix(tbl) # Conversion du tableau en matrice
tblm = cbind(1:nrow(tblm),tblm)
colnames(tblm) = c("Candidat","Fréquence")
kable(tblm, booktabs=TRUE, caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)
cat("Candidat préféré = ",modes(tbl)$mode)

```

6. Reprenez l'enquête visant à déterminer le candidat favori pour la présidence, mais en utilisant un échantillon de 1200 sujets, stratifié en fonction du département.

```{r 04-Chap4-60}
library(sampler)
x = ssamp(HaitiPop,1200,strata=DEPARTEMENT)$PrefPres
tbl = table(x)
tbl
tblm=as.matrix(tbl) # Conversion du tableau en matrice
tblm = cbind(1:nrow(tblm),tblm)
colnames(tblm) = c("Candidat","Fréquence")
kable(tblm, booktabs=TRUE, caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)
cat("Candidat préféré = ",modes(tbl)$mode)
```


7. Au niveau national, quel est l'indice de consensus concernant l'opinion face au projet d'une nouvelle Constitution? Utilisez un échantillon de 500 sujets pour le déterminer.

```{r 04-Chap4-61}
library(agrmt)
subpop = subset(HaitiPop,(AGE>=18))$OpinConst
x = sample(subpop,1500,replace=FALSE)
tbl = table(x)
tbl
tblm=as.matrix(tbl) # Conversion du tableau en matrice
tblm = cbind(1:nrow(tblm),tblm)
colnames(tblm) = c("Opinion","Fréquence")
kable(tblm, booktabs=TRUE, caption="Répartition des Effectifs") %>%
  kable_styling(full_width = FALSE)
cat("Opinion Constitutionnelle = ",modes(tbl)$mode)
cat("Indice de Consensus = ",consensus(tbl))
```

8. Est-ce que l'opinion face au projet d'une nouvelle Constitution suscite le même niveau de consensus d'un département à l'autre? Obtenez l'indice de consensus pour chacun des départements, en utilisant des échantillons départementaux composés de 100 sujets.


```{r 04-Chap4-62}
library(agrmt)
consens = vector()
for (i in as.character(1:10)){
 subpop = subset(HaitiPop,((AGE>=18)&(DEPARTEMENT==as.character(i)))) 
 x = sample(subpop$OpinConst,100)
 consens[i] = consensus(x)
}
 res = data.frame(t(Dept),consens)
 colnames(res) = c("Dept#","Consensus")
 kable(res, booktabs=TRUE, caption="Consensus / Département") %>%
  kable_styling(full_width = FALSE)

```

9. Pouvez-vous élaborer quelques questions de recherche supplémentaires sur cette population?

#### NOTE: Certes, R nous offre tous les outils qui nous permettraient d'étudier directement les caractéristiques de cette population, mais oublions ce fait. Étudions cette population comme on le ferait en réalité, aidé des outils statistiques appropriés selon la nature de la question de recherche.

## Distributions d'échantillonnage

Dans les démonstrations et exercices présentés dans la section précédente, il devrait être acquis que les statistiques calculées pour un échantillon seront très probablement différentes de celles calculées pour un autre échantillon. Étant donné que la sélection des sujets composant un échantillon se fait de manière aléatoire, différents échantillons seront composés de sujets différents. Et puisque les statistiques échantillonnales utilisent les informations fournies par ces sujets, leur variation est un fait avéré.

Puisque les statistiques échantillonnales sont le résultat d'une expérience aléatoire, on peut les considérer de la même manière qu'on l'a fait pour des observations obtenues d'un sujet particulier. Donc, on peut tracer la distribution d'une statistique données (moyenne, médiane, écart-type, consensus, etc) calculée pour un très grand nombre d'échantillons. Cette distribution porte le nom de: **distribution d'échantillonnage** de la statistique qui la compose.

### Distribution d'échantillonnage d'une moyenne

Supposons une population infinie, dont la distribution est uniforme avec des valeurs comprises entre 0 et 10. On tire de cette population, un échantillon de 25 observations. Finalement, on calcule la moyenne des données recueillies auprès de cet échantillon:

```{r 04-Chap4-63}
x = runif(25,0,10)
moy = mean(x)
cat("Moyenne = ",moy)
```
Tirons de cette même population, un second échantillon:

```{r 04-Chap4-64}
x = runif(25,0,10)
moy = mean(x)
cat("Moyenne = ",moy)
```
La variation de la statistique échantillonnale (la moyenne) est évidente. Répétons cette expérience 100,000 fois, en tirant à chaque fois un nouvel échantillon de 25 sujets. Pour nous faciliter la tàche, la fonction **echmoy()** est définie, avec les arguments *n* (= 25 par défaut), *minimum* (= 0 par défaut) et *maximum* ((= 10 par défaut). On utilise ensuite la fonction **replicate()** pour compléter la simulation. Finalement, on trace un histogramme illustrant la **distribution d'échantillonnage des moyennes**: 

```{r 04-Chap4-65}
echmoy = function(n=25,mn=0,mx=10){
  x = runif(n,mn,mx)
  moy = mean(x)
  return(moy)
}
Nechant = 100000 # Nombre d'échantillons à tirer
dismoy = replicate(Nechant,echmoy())
hist(dismoy,main="Distribution d'échantillonnage\n des moyennes",xlab="Moyennes")
cat("Moyenne de la distribution    = ",mean(dismoy))
cat("Écart-Type de la distribution = ",sd(dismoy))
```
Dans les sections précédentes, nous avons souvent rencontré ce type de distribution. Et dans plusieurs cas, nous l'avons utilisée de manière à obtenir certaines probabilités. Dans le contexte actuel, tout ce qui a été dit demeure vrai! On peut donc répondre à certaines questions:

  1. En tirant un échantillon de 25 sujets de cette population, quelle est la probabilité que la moyenne de cet échantillon soit supérieure à 6? Toutes les moyennes composant la distribution d'échantillonnage des moyennes se trouvent dans l'objet **dismoy**.
  
```{r 04-Chap4-66}
cat("P(Moy > 6) = ",sum(dismoy>6)/Nechant)
```
  2. En tirant un échantillon de 25 sujets de cette population, quelle est la probabilité que la moyenne de cet échantillon soit inférieure à 3? 
  
```{r 04-Chap4-67}
cat("P(Moy < 3) = ",sum(dismoy<3)/Nechant)
``` 
  3. En tirant des échantillons de 25 sujets de cette population, à l'intérieur de quelles limites centrales retrouve-t-on 95% des moyennes? 
  
```{r 04-Chap4-68}
intvl = quantile(dismoy,c(0.025,0.975))
cat("(",intvl[1],"< Moy < ",intvl[2],") = 0.95")
``` 

Suite à cette dernière question, on peut aisément imaginer des zones particulières, sous cette distribution:  

  1. une région où les moyennes ont des valeurs probables: elles sont localisées autour de la moyenne de la population. Cette zone pourrait correspondre à un intervalle tel que celui obtenu ci-dessus.  En tirant un échantillon de n=25 observations, la probabilité que sa moyenne soit comprise dans cet intervalle est égale à 0.95. Évidemment, on pourrait définir des intervalles correspondant à différents niveaux de probabilité. Pour le besoin des discussions future, identifions cette probabilité par $(1-\alpha)$
  
  2. Deux régions où les moyennes ont des valeurs improbables: elles sont localisées dans chacune des extrémités de la distribution d'échantillonnage. En tirant un échantillon de n=25 observations, la probabilité que sa moyenne soit comprise dans l'une ou l'autre de ces extrémités est égale à 1-0.95 = 0.05. Évidemment, on pourrait définir des intervalles correspondant à différents niveaux de probabilité. Pour les besoins des discussions futures, identifions cette probabilité par $\alpha$.

#### La moyenne de la DEM

La moyenne de la distribution d'échantillonnage des moyennes, on peut le concevoir aisément, devrait être très rapprochée de la moyenne de la population. La simulation suivante en fait la démonstration. Supposons une population infinie, distribuée de manière uniforme entre 0 et 10. La moyenne de cette population est **(10-0)/2 = 5.0**. En tirant trois échantillons de taille 25, on obtient les moyennes suivantes:

```{r 04-Chap4-69}
set.seed(1234567)
moys = replicate(3,mean(runif(25,0,10)))
cat("Moyenne Ech 1 = ", moys[1])
cat("Moyenne Ech 2 = ", moys[2])
cat("Moyenne Ech 3 = ", moys[3])
```
On note qu'aucune des trois moyennes n'est égale à la véritable moyenne de la population. Par contre, elles n'en sont pas très éloignées. Si on calcule la moyenne de ces trois moyennes, on obtient:

```{r 04-Chap4-70}
cat("Moyenne de l'ensemble = ", mean(moys))
```
On constate que la moyenne des trois moyennes se rapproche de la moyenne de la population. Effectivement, puisque les échantillons sont tous de la même taille, la moyenne des moyennes est la moyenne d'un échantillon unique de taille $3\times n$. Or, on sait que plus la taille d'un échantillon est grande, plus les statistiques échantillonnales (la moyenne par exemple) se rapproche du paramètre correspondant de la population. À la limite, en tirant un nombre infini d'échantillons, la moyenne des moyennes est égale à la moyenne de la population. Répétons l'expérience 100,000 fois pour se convaincre que cet énoncé est vrai:

```{r 04-Chap4-71}
moys = replicate(100000,mean(runif(25,0,10)))
cat("Moyenne de la DEM = ", mean(moys))

```
Cette fois-ci, on est très près du but! Avec un nombre infini de réplications, la moyenne de la distribution d'échantillonnage des moyennes serait effectivement égale à la moyenne de la population.

#### L'écart-type de la DEM

L'écart-type de la distribution d'échantillonnage des moyennes porte un nom spécial: il s'agit de l'**erreur-standard des moyennes**. En effet, l'écart-type de toute distribution d'échantillonnage est un **erreur-standard** ou **erreur-type** d'une statistique donnée. 

Devrait-on s'attendre à ce que, comme c'est le cas pour la moyenne, l'écart-type de la distribution d'échantillonnage des moyennes soit égal à l'écart-type de la population? 

L'écart-type d'une distribution uniforme est données par:
$$\sigma =\sqrt{\frac{(b-a)^2}{12} }  $$
Pour la population étudiée, on obtient:
$$\sigma =\sqrt{\frac{(10-0)^2}{12}}=2.8868   $$
Par simulation, on trouve:

```{r 04-Chap4-72}
cat("Écart-type de la population = ",sd(runif(1000000,0,10)))
```
ce qui est une approximation très satisfaisante.

Pour la suite des choses, identifions l'erreur-standard des moyennes par le symbole $\sigma_{\overline{x}}$.

Réfléchissons un peu: l'écart-type de la population ($\sigma$) mesure la dispersion des données composant cette population. D'autre part, l'**erreur-standard des moyennes** ($\sigma_{\overline{x}}$) mesure la dispersion des moyennes d'échantillons tirés de cette population! Ces deux éléments sont tout-à-fait différents, et par conséquent, ces deux mesures de dispersion ne peuvent être identiques. Pour s'en convaincre, revenons à notre population et cette fois, calculons l'écart-type de l'ensemble des moyennes calculées pour un grand nombre d'échantillons de 25 sujets chacun:
```{r 04-Chap4-73}
set.seed(1234567)
moys = replicate(100000,mean(runif(25,0,10)))
cat("Moyenne de la DEM            = ", mean(moys))
cat("Erreur-standard des moyennes = ", sd(moys))
```
L'erreur-standard des moyennes se distingue donc de l'écart-type de la population. Mais puisque les échantillons sont tirés d'une population possédant une certaine dispersion, on pourrait parier qu'il existe un lien entre l'écart-type et l'erreur standard. Examinons la possibilité que ce lien en soit un de proportionalité:

```{r 04-Chap4-74}
cat("É.T. / E.S = 2.886947/0.576815 =",2.886947/0.576815)
```
Rappelons que la taille de l'échantillon est *n*=25. Or, la racine carrée de 25 est justement 5.0! Se pourrait-il que l'erreur-standard soit une fonction de la taille de l'échantillon? Intuitivement, on pourrait aisément le penser: plus la taille de l'échantillon est grande, plus on s'approche de la population entière, et par conséquent plus l'erreur standard devrait être petit. Vérifions la possiblité que l'erreur-standard des moyennes puisse se définir par:

$$ \sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}}$$
Répétons l'expérience en variant la taille des échantillons: 25, 100, 144, 225 et 400:

```{r 04-Chap4-75}
set.seed(1234567)
mmoys = vector()
smoys = vector()
semoys = vector()
cnt = 0
ns = c(25,100,144,225,400) # Tailles d'échantillons
for(i in ns){
  cnt = cnt+1
  moys = replicate(100000,mean(runif(i,0,10)))
  mmoys[cnt] = mean(moys)
  smoys[cnt] = sd(moys)
  semoys[cnt] = 2.8868/sqrt(i)
}
res = data.frame(ns,mmoys,smoys,semoys)
colnames(res) = c("n","Moyenne","E.S. Empir","E.S. Théor")
kable(res, booktabs=TRUE, caption="Erreur Standard et n") %>%
  kable_styling(full_width = FALSE)

```
Il semble évident que la relation proportionnelle tient la route. Cela nous sera très utile dans les discussions à venir.


### Distribution d'échantillonnage d'une variance

De la même manière qu'on a pu établir la distribution d'échantillonnage des moyennes dans la section précédente, il est possible d'établir la distribution des variances. Le code R suivant effectue ce travail:

```{r 04-Chap4-76}
echvar = function(n=5,mn=0,mx=10){
  x = runif(n,mn,mx)
  variance = var(x)
  return(variance)
}
library(moments)
Nechant = 100000 # Nombre d'échantillons à tirer
disvar = replicate(Nechant,echvar())
hist(disvar,main="Distribution d'échantillonnage\n des variances",xlab="Variances")
cat("Moyenne de la distribution    = ",mean(disvar))
cat("Écart-Type de la distribution = ",sd(disvar))
cat("Indice de Symétrie            = ",skewness(disvar))
cat("Indice de Voussure            = ",kurtosis(disvar))
```
Dans cette simulation, des échantillons de petites tailles (*n* = 5) de manière à faire ressortir une caractéristique de la distribution d'échantillonnage des variances: elle est asymétrique positive! Le degré d'asymétrie décroit en fonction de la taille de l'échantillon. Répétons l'expérience pour *n* = 25:


```{r 04-Chap4-77}
echvar = function(n=55,mn=0,mx=10){
  x = runif(n,mn,mx)
  variance = var(x)
  return(variance)
}
library(moments)
Nechant = 100000 # Nombre d'échantillons à tirer
disvar = replicate(Nechant,echvar())
hist(disvar,main="Distribution d'échantillonnage\n des variances",xlab="Variances")
cat("Moyenne de la distribution    = ",mean(disvar))
cat("Écart-Type de la distribution = ",sd(disvar))
cat("Indice de Symétrie            = ",skewness(disvar))
cat("Indice de Voussure            = ",kurtosis(disvar))
```

Une autre caractéristique de cette distribution qu'on doit noter est qu'étant donné qu'une variance ne peut être négative, l'ensemble de la distribution est localisé du côté positif, ce qui n'est pas nécessairement le cas pour la distribution d'échantillonnage des moyennes.

Malgré ces différences, il est toujours possible de calculer la probabilité d'obtenir, en tirant un échantillon d'une certaine taille, une variance située dans une zone ou l'autre de la distribution. Par exemple, la probabilité d'obtenir une variance supérieure à 15, en tirant un échantillon de 5 sujets d'une population dont la distribution est uniforme entre 0 et 10, on obtient:

```{r 04-Chap4-78}
echvar = function(n=5,mn=0,mx=10){
  x = runif(n,mn,mx)
  variance = var(x)
  return(variance)
}
Nechant = 100000 # Nombre d'échantillons à tirer
disvar = replicate(Nechant,echvar())
hist(disvar,main="Distribution d'échantillonnage des variances",xlab="Variances")
abline(v=mean(disvar),col="blue",lwd=3,lty=2)
abline(v=15,col="red",lwd=3)
cat("P(Variance > 15) = ",sum(disvar>15)/Nechant)
```

On constate donc que la forme d'une distribution d'échantillonnage varie en fonction de la nature de la statistique qui la compose. Par contre, peu importe cette forme, le calcul des probabilités dont on a besoin dans un contexte particulier se fait toujours de la même manière.

### Exercice 6

  1. Établissez la distribution d'échantillonnage de la médiane, pour des échantillons de taille 50, tirés d'une population dont la distribution est uniforme avec un minimum de 1 et un maximum de 12.  
  
    a. Quelle est la moyenne de cette distribution?  
    b. Quel est l'écart-type de cette distribution?  
    
  2. Quelle est la probabilité que la médiane d'un échantillon de 50 sujets tiré de cette population soit supérieure à 10?
  
  3. Quelle est la probabilité que la médiane d'un échantillon de 50 sujets tiré de cette population soit inférieure à 5?
  
  4. En tirant des échantillons de 50 sujets de cette population, à l'intérieur de quelles limites centrales retrouve-t-on 95% des médianes? 

  5. Établissez la distribution d'échantillonnage d'un écart-type, pour des échantillons de taille 50, tirés d'une population dont la distribution est uniforme avec un minimum de 1 et un maximum de 12. 
  
    a. Quelle est la moyenne de cette distribution?
    b. Quel est l'écart-type de cette distribution?
      
  6. Quelle est la probabilité que l'écart-type d'un échantillon de 50 sujets tiré de cette population soit supérieure à 3.3?
    
  7. Quelle est la probabilité que l'écart-type d'un échantillon de 50 sujets tiré de cette population soit inférieure à 2.5?
  
  8. En tirant des échantillons de 50 sujets de cette population, à l'intérieur de quelles limites centrales retrouve-t-on 95% des écarts-types? 

**Le code R suivant génère une population de 100,000 observations ordinale (échelle de Likert en 7 points):**
 
```{r 04-Chap4-79}
library(wakefield)
xord = r_sample_ordered(n=100000, x = 1:7, prob = c(5,8,2,1,2,1,1), name = "xord")
```
Rappelons que le calcul du consensus peut se faire à l'aide de la fonction **consensus()** qui se trouve dans la librairie **agrmt**. L'argument de cette fonction est le tableau des fréquences obtenu par **table()**.   
  
  9. En tirant des échantillons de 100 sujets de cette population, établissez la distribution d'échantillonnage du consensus.   
    a. Quelle est la moyenne de cette distribution?  
    b. Quel est l'écart-type de cette distribution?  
    
  10. Quelle est la probabilité que le consensus observé pour un échantillon de 100 sujets tiré de cette population soit supérieur à 0.65?
  
  11. Quelle est la probabilité que le consensus observé pour un échantillon de 100 sujets tiré de cette population soit inférieur à 0.50?
  
  12. En tirant des échantillons de 100 sujets de cette population, à l'intérieur de quelles limites centrales retrouve-t-on 95% des indices de consensus? 

### Pistes de solutions pour l'Exercice 6

#### Distribution d'échantillonnage des médianes

```{r 04-Chap4-80}
echmed = function(n=25,mn=0,mx=10){
  x = runif(n,mn,mx)
  med = median(x)
  return(med)
}
Nechant = 100000 # Nombre d'échantillons à tirer
dismed = replicate(Nechant,echmed())
hist(dismed,main="Distribution d'échantillonnage\n des médianes",xlab="Médianes")
cat("Moyenne de la distribution    = ",mean(dismed))
cat("Écart-Type de la distribution = ",sd(dismed))
```

#### Distribution d'échantillonnage des écarts-types
```{r 04-Chap4-81}
echsd = function(n=25,mn=0,mx=10){
  x = runif(n,mn,mx)
  xsd = sd(x)
  return(xsd)
}
Nechant = 100000 # Nombre d'échantillons à tirer
dissd = replicate(Nechant,echsd())
hist(dissd,main="Distribution d'échantillonnage\n des écarts-types",xlab="Écarts-Types")
cat("Moyenne de la distribution    = ",mean(dissd))
cat("Écart-Type de la distribution = ",sd(dissd))
```
#### Distribution d'échantillonnage des consensus 

```{r 04-Chap4-82}
library(agrmt)
echcns = function(n=100){
  x = sample(xord,n,replace=FALSE)
  cns = consensus(table(x))
  return(cns)
}
Nechant = 100000 # Nombre d'échantillons à tirer
cns = replicate(Nechant,echcns())
hist(cns,main="Distribution d'échantillonnage\n du Consensus",xlab="Consensus")
```

<!--chapter:end:04-Chap4.Rmd-->

---
title: "Inférence Statistique - Estimation"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
---


```{r 05-Chap5-1, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```

```{r 05-Chap5-2, include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r 05-Chap5-3,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(moments,formatR,kableExtra,sampler,dplyr,data.table)
```
```{r 05-Chap5-4}
# Importer HaitiPop dans l'environnement
 load("I:/MAT104/HaitiPop_Env.RData")

```

# Inférence statistique: Estimation

Obtenir un échantillon aléatoire d'une population est une étape essentielle lorsqu'on tente de répondre à une question de recherche concernant cette population. Mais un échantillon à lui seul ne peut pas nous permettre d'atteindre ce but: il n'est qu'un reflet, souvent biaisé en raison de procédures d'échantillonnage défaillantes, de la population d'où il est tiré. Ce serait une erreur grave que de considérer cet échantillon comme étant une réplique exacte de la population, et en tirer des conclusions sans autre mesure. Néanmoins, il est souvent possible d'inférer différents paramètres de la population en l'examinant de manière appropriée. 

## Estimation d'une moyenne

Dans la section précédente, on a pu obtenir la distribution des moyennes calculées pour un grand nombre d'échantillons d'une certaine taille, tous tirés de la même population. Il s'agissait de la **distribution d'échantillonnage des moyennes**.  

Dans un contexte de recherche, il n'est pas possible d'obtenir autant d'échantillons: on doit composer, généralement, avec un très petit nombre d'échantillons. Dans un tel contexte, comment peut-on obtenir la distribution d'échantillonnage dont on a besoin pour calculer les probabilités permettant de répondre à une question de recherche?

Pour répondre à cette question, considérons la distribution des indices de condition de vie (**CondxVie**) des Haitiens vivant dans le département de l'Ouest (**DEPARTEMENT == 8**). Le code R suivant extrait de la population que nous avons générée dans l'unité précédente, la sous-population que nous voulons étudier.

```{r subpop_sel}
library(data.table)
DT <- as.data.table(HaitiPop)     # data.table
xcv = DT[DEPARTEMENT==8]$CondxVie  # Population ciblée
cat("Taille de la sous-population = ",length(xcv))
cat("Moyenne de CondxVie          = ", mean(xcv))
cat("Écart-type de CondxVie       =", sd(xcv))

```
Pour les besoins de la discussion, les paramètres de cette sous-population ont été calculés. Rappelons que dans le monde réel, ces éléments ne sont généralement pas connus et doivent être estimés à partir d'observations effectuées sur un échantillon.

Supposons qu'un chercheur s'intéresse à déterminer le niveau de condition de vie moyen chez cette population. Pour ce faire, les contraintes budgétaires limitent à 25 la taille de l'échantillon qu'il peut former. S'il était en mesure de tirer un très grand nombre d'échantillons de taille *n* = 25 de la population, il obtiendrait la distribution d'échantillonnage suivante:

```{r mean_dist}
Nechant = 10000 # Nombre d'échantillons à tirer
n = 25          # Taille de l'échantillon
xb = replicate(Nechant,mean(sample(xcv,n,replace=FALSE)))
hist(xb,main="Distribution d'échantillonnage\n des moyennes", xlab="Moyennes",breaks="FD")
cat("Moyenne         = ",mean(xb))
cat("Erreur-Standard = ",sd(xb))
cat("Étendue         = ",range(xb))
```

On constate que l'étendue des moyennes possibles couvre de `r round(min(xb),3)` jusqu'à `r round(max(xb),3)`: en tirant un échantillon aléatoire de 25 observations de notre sous-population, on peut obtenir une moyenne qui prend n'importe quelle valeur se trouvant entre ces extrêmes, sans exclure des valeurs plus extrêmes encore! D'autre part, les moyennes comprises entre 5 et 7 par exemple, sont plus probables que les moyennes s'éloignant dans les extrémités de la distribution.

Considérons les moyennes qui occupent les 95% centraux de la distribution, de sorte que 5% de cette aire se trouve dans les extrémités (2.5% de chaque côté):

```{r 05-Chap5-5}

nc=0.95 # Niveau de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))     # Calcul des percentiles
gr1=hist(xb,breaks="FD",plot=FALSE)             # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des moyennes", xlab="Moyennes", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Erreur Std = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```
Qualifions de **probables** les moyennes situées au centre, et d'**improbables** les moyennes se trouvant dans les extrémités. Les percentiles 2.5 et 97.5 sont les frontières entre les moyennes probables et improbables, si on considère 95% comme étant le critère à utiliser.

On constate donc qu'en tirant un échantillon aléatoire de la population, un écart plus ou moins grand existera entre la moyenne de la population et la moyenne de l'échantillon. Cet écart représente l'**erreur d'échantillonnage**: $(\bar{X}-\mu)$

Si une erreur d'échantillonnage ne dépasse pas la distance séparant la véritable moyenne de la population et la frontière entre les moyennes probables et improbables, on pourra considérer que la moyenne de l'échantillon est un reflet relativement fidèle de la moyenne de la population. 

**Distribution d'échantillonnage standardisée**

La notion de **score centré et réduit**, ou **score Z** a été présentée dans l'unité portant sur les distributions de probabilité: 
$$Z = \frac{X - \mu}{\sigma}$$
La conversion de données brutes en scores Z permet d'obtenir une distribution standardisée dont la moyenne est nulle, et l'écart-type est unitaire. Le code R suivant effectue la standardisation de la distribution d'échantillonnage des moyennes de CondxVie: les écarts entre les moyennes échantillonnales et la moyenne de la population sont divisés par l'écart-type de la distribution des moyennes (erreur-standard):
$$Z = \frac{\bar X - \mu}{\sigma_\bar X}$$

On a établi plus haut que l'erreur standard des moyennes dans le cas présent est  $\sigma_\bar{X}$ = `r round(sd(xb),3)`. On trouve donc:

```{r mean_dist_std}
z = (xb-(mean(xb)))/sd(xb)
nc=0.95 # Niveau de confiance
intc=quantile(z,c(.5*(1-nc),nc+.5*(1-nc)))     # Calcul des percentiles
gr1=hist(z,breaks="FD",plot=FALSE)             # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des moyennes (Standardisée)",xlab="Z", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(z),3))
txt2=paste("Erreur Std = ",round(sd(z),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```
On note que la forme de la distribution demeure inchangée par cette standardisation. Par contre, cette distribution est maintenant centrée sur 0, et son écart-type est égal à 1. On note également que les frontières séparant le probable de l'improbable se trouvent à environ 2 écart-types de la moyenne. Puisque la distribution est standardisée, on pourrait s'attendre à ce que ces limites puissent s'appliquer dans plusieurs situations. Toujours en considérant une zone de valeurs probables occupant les 95% centraux de la distribution, ceci nous permet de formuler une approximation de la **marge d'erreur**, définie plus haut:
$Marge\;d'erreur \approx2\sigma_\bar{X}\approx$ `r round(2*sd(xb),3)`

## Retour à la réalité!

Rappelons que le chercheur qui tente d'estimer la moyenne de la population ne dispose que d'un de ces échantillons, généré ci-dessous:

```{r 05-Chap5-6}
#set.seed(1235645)
x = sample(xcv,25,replace=FALSE)
cat("Moyenne échantillonnale = ",mean(x))
cat("Écart-type              = ",sd(x))
hist(x,main="Distribution Échantillonnale", xlab="CondxVie",breaks="FD")
```
Il obtient donc une estimation ponctuelle de la moyenne de la population: $\bar X =$ `r round(mean(x),3)`, et une estimation ponctuelle de son écart-type: $s =$ `r round(sd(x),3)`.

En utilisant l'erreur-standard des moyennes calculé à partir de la distribution d'échantillonnage, on peut estimer la marge d'erreur égale à:

```{r 05-Chap5-7}
ME = 2*sd(xb)
cat("Marge d'erreur = ",ME)
```
Le chercheur peut donc conclure que l'intervalle compris entre $\bar X \pm ME$, soit entre `r round(mean(x)-ME,3)` et `r round(mean(x)+ME,3)` a de bonnes chances (95%) d'inclure ou de capturer la moyenne de la population. Il aura ainsi proposé un **intervalle de confiance**, pour un **niveau de confiance** de 95%. En répétant l'expérience un nombre infini de fois, il aura raison dans 95% des cas, et se trompera dans 5% des cas. Formellement, on pourra exprimer  le résultat de cette estimation par:
  
C[`r round(mean(x)-ME,3)`< $\mu$ <`r round(mean(x)+ME,3)`]=95%

Généralement, on n'a pas accès à la distribution d'échantillonnage, de sorte que son erreur-standard est inconnu. Il est nécessaire, dans ces cas, d'estimer cette quantité par:  
  
$\sigma_\bar{X}\approx \frac {s_X}{\sqrt{n}}\approx$ `r ME = round(2*sd(x)/sqrt(length(x)),3);ME` 
  
## En résumé

Considérons maintenant comme acquis que:

  1. la moyenne de la distribution des moyennes est égale à la moyenne de la population
  2. l'écart-type de cette distribution (erreur-standard) est une fonction de la taille de l'échantillon: plus *n* est grand, plus l'erreur-standard des moyennes sera petit...
  
On peut donc utiliser la moyenne d'un échantillon en tant qu'**estimateur** de la moyenne de la population. Si l'échantillon est représentatif de la population, la moyenne de l'échantillon ne devrait pas être très éloignée de la moyenne de la population: l'erreur d'échantillonnage ne devrait pas être trop grande. Par contre, rien ne nous informe quant à la distance qui sépare ces deux moyennes! On a besoin d'information supplémentaire. La moyenne de l'échantillon ne nous fournit qu'une **estimation ponctuelle** de la moyenne de la population. Et ce n'est pas suffisant!... Il nous faut tenir compte de la dispersion des moyennes pour établir une étendue de valeurs parmi lesquelles pourrait se trouver la moyenne de la population que l'on tente d'estimer.


Étant donné un échantillon, l'estimation d'une moyenne consiste à définir une zone, au centre de la distribution d'échantillonnage des moyennes, qui a de fortes chances de capturer la moyenne de la population. La question qu'il nous faut considérer ici est de savoir en quoi consiste une *forte probabilité*?  

  - Par convention, on considère que si notre intervalle capture la moyenne de la population 19 fois sur 20 $\left[(1-\alpha) = 0.95\right ]$, on peut dormir tranquille! On ne se trompera qu'une fois sur 20 $(\alpha=0.05)$. Mais dans certains cas, on peut accepter de se tromper 2 fois sur 20 $\left[(1-\alpha)=0.90,\; \alpha=0.10\right ]$ ou seulement 0.5 fois sur 20 $\left[(1-\alpha)=0.99,\; \alpha=0.01\right ]$. Le choix d'une probabilité revient à l'analyste, en fonction des circonstances propres à la recherche effectuée. Cette probabilité porte le nom de **niveau de confiance = **$(1-\alpha)$ 

En bref, le problème à résoudre est d'obtenir un intervalle utile, susceptible avec une probabilité connue de capturer le paramètre de la population que l'on veut estimer avec un degré satisfaisant de confiance.

Étant donné que dans un contexte de recherche nous ne disposons que d'un échantillon, le prolème est de spécifier la distribution d'échantillonnage des moyennes. Des méthodes analytiques classiques existent, mais ces méthodes, qui seront discutées plus tard, reposent sur des postulats et prérequis qui souvent ne sont pas respectés. La méthode proposée ici n'implique aucun autre prérequis que la représentativité de l'échantillon. 

### Le ré-échantillonnage ('*Bootstrap*')

Supposons que dans une recherche on doive estimer la moyenne d'une population à l'aide d'un échantillon aléatoire de 25 observations.Cet échantillon est le seul à notre disposition. Sa moyenne est une estimation ponctuelle de la moyenne de la population, mais la distance qui sépare cette estimation de la véritable moyenne de la population demeure inconnue.

Le ré-échantillonnage nous permet de combler le besoin que nous avons d'obtenir un grand nombre d'échantillons et de construire la distribution d'échantillonnage. Essentiellement, la procédure se réalise en 4 étapes:

  1. On tire un grand nombre d'échantillons de la même taille que notre échantillon original, **AVEC replacement**
  2. Pour chaque échantillon, on calcule la moyenne (ou toute autre statistique d'intérêt!)
  3. On dresse la distribution de ces moyennes
  4. On obtient les percentiles correspondants à $\alpha/2$ et à $(1-\alpha/2)$, qui se traduisent par l'intervalle de confiance recherché.
  
### Exemple

On tente d'estimer la moyenne de l'indice de Condition de Vie au niveau de la population Haitienne. On obtient un échantillon aléatoire composé de 25 individus adultes (AGE > 18). On sera satisfait si on obtient un intervalle de confiance susceptible de capturer la moyenne de la population avec un niveau de confiance de 95%. Le code R génère d'abord les données échantillonnales, et procède au ré-échantillonnage (100,000  ré-échantillonnages):

```{r resample_1}
set.seed(1234567)
# 
# Sous-population étudiée
subpop = subset(HaitiPop,(AGE>18),CondxVie)
#
# On obtient un échantillon aléatoire, tiré de la population ciblée
n = 25                # Taille de l'échantillon
xcv = sample(subpop$CondxVie,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .95			        # Niveau de confiance
ns=100000			        # Nombre de ré-échantillonnage
xb = replicate(ns,mean(sample(xcv,n,replace=TRUE)))
#
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
#
# Affichage graphique
gr1=hist(xb,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des moyennes\n (Ré-échantillonnage)", xlab="Moyennes", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Écart-Type = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```

 On trouve que:  
 
 C[`r round(intc[1],3)`< $\mu$ <  `r round(intc[2],3)`] = 95%   
 
Bref, on a 95% des chances que l'intervalle s'étendant de `r round(intc[1],3)` à `r round(intc[2],3)` inclut la moyenne de la population. On a 5% des chance que cela soit faux.

Cet intervalle porte le nom d'**intervalle de confiance** et la proportion de l'aire que cet intervalle occupe sous la distribution s'appelle **niveau de confiance**, sympolisé par $(1-\alpha)$.

Pour comparaison, quel intervalle trouverait-on en utilisant la distribution d'échantillonnage formée d'un grand nombre d'échantillons (*n*=25) effectivement tirés de la population:

```{r resample_2}

set.seed(1234567)
n = 25     # Taille de l'échantillon
nc = 0.95  # Niveau de confiance
ns = 10000 # Nombre d'échantillons à tirer
xdat = subpop$CondxVie
xb = replicate(ns,mean(sample(xdat,n,replace=FALSE)))
#
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
gr1=hist(xb,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des moyennes", xlab="Moyennes", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Écart-Type = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```

On constate que l'intervalle de confiance obtenu par ré-échantillonnage n'est pas très différent de celui qu'on obtiendrait dans des conditions idéales, s'il était possible de tirer un très grand nombre d'échantillons. Ce résultat est très satisfaisant, pour autant que l'échantillon original soit représentatif de la population.

### Anatomie d'un intervalle de confiance

La figure suivante illustre la nature d'un intervalle de confiance:

![Intervalle de Confiance](./images/IntervalleConf.png)
  
La statistique échantillonnale est un estimateur du paramètre correspondant de la population. Il s'agit d'une estimation ponctuelle de ce paramètre. On sait que généralement, cet estimateur se trouvera à une certaine distance, inconnue, du paramètre de la population, ce qui constitue l'erreur d'échantillonnage:
$$Erreur\; d'échantillonnage = statistique - paramètre$$
Par exemple, dans le cas de l'estimation d'une moyenne, l'erreur d'échantillonnage sera $(\overline{X}-\mu)$. En considérant un nombre infini de moyennes échantillonnales, chacune d'elles présentant une certaine erreur d'échantillonnage, on peut déterminer les limites à l'intérieur desquelles on retrouvera une importante proportion de ces moyennes (95%, par exemple). Et cet intervalle incluera certainement le paramètre de la population. La distance qui sépare l'estimation ponctuelle de l'une ou l'autre des limites correspond à la **marge d'erreur (ME)**. On peut donc définir l'intervalle de confiance par l'expression:
$$\overline{X}\pm ME$$
Considérant l'intervalle de confiance obtenu par ré-échantillonnage dans l'exemple précédent, on a:
 $$C\left[22.89 < \mu < 25.88 \right] = 95\% $$
 qui correspond à:
 $$24.385\;\pm\;1.495$$

La marge d'erreur est égale à 1.495, pour un niveau de confiance de 95%. On estime que la moyenne de la population d'où est tiré l'échantillon est égale à 24.385 avec une marge d'erreur de 1.495, 19 fois sur 20. En d'autres termes, on a 95% des chances que l'intervalle obtenu capture la moyenne de la population.

### Simulation I:
La démonstration suivante illustre l'ensemble de la situation: 
[SIMULATION IC](https://wise1.cgu.edu/vis/ci_creation)

Pour l'utiliser,   
  
    a. Choisissez la forme de la distribution de la population  
    b. Choisissez le niveau de confiance  
    c. Sélectionnez la taille de l'échantillon  
    d. Cliquez sur SAMPLE aussi souvent que nécessaire  
 
### Simulation II:

La fonction **cisim()** de la librairie **PASWR2** permet de simuler des intervalles de confiance pour moyennes, variances et proportions, et d'afficher graphiquement les résultats. Par exemple:

```{r 05-Chap5-8, message=FALSE }

library(PASWR2)
#Intervalle de confiance pour une moyenne
cisim(samples = 100, n = 20, parameter = 100, sigma = 15, conf.level = 0.95)

```
### Simulation III:
  
L'ensemble de fonctions présentées ci-dessous permettent de calculer l'intervalle de confiance pour chacun de *k* échantillons d'une certaine taille, de les représenter graphiquement tout en fournissant la proportion des intervalles capturant la moyenne de la population. 

La fonction **exper()** ré-échantillonne un grand nombre de fois (nrep=5000 par défaut) un échantillon tiré d'une distribution Gaussienne ($\mu = 50$ et $\sigma = 8$ par défaut). Par défaut, le niveau de confiance est fixé à 95%.

```{r Fonction exper}
#set.seed(1234567)
# Fonction exper: Effectue une expérience aléatoire

exper = function(x,nrep=5000,n=25,mu=50,sig=8,nconf=0.95){
  xb = replicate(nrep,mean(sample(x,n,replace=TRUE)))
  popmean = mean(xb)
  stderr = sd(xb)
  lims = quantile(xb,c((1-nconf)/2,1-(1-nconf)/2))
  out = c(popmean,stderr,lims[1],lims[2])
  return(out)
}
```

La fonction **plot_ci()** a été extraite de la librairie **Openintro**. Elle permet d''afficher les intervalles de confiance en indiquant la proportion de ceux qui ne capturent pas la moyenne de la population.

```{r Fns_plot_ci}
# Fonction plot_ci: tirée de la librairie Openintro
#' Plot confidence intervals
#'
#' @description This function takes vectors of upper and lower confidence
#' interval bounds, as well as poplutation mean, and plots which confidence
#' intervals intersect the mean
#' @param lo a vector of lower bounds for confidence intervals
#' @param hi a vector of upper bounds for confidence intervals
#' @param m the population mean
#' @export
#'

plot_ci <- function(lo, hi, m) {
  nhit = sum((m>=lo)&(m<=hi))/length(lo)
  par(mar=c(5, 4, 4, 2), mgp=c(2.7, 0.7, 0))
  k <- length(lo)
  ci.max <- max(rowSums(matrix(c(-1*lo,hi),ncol=2)))

  xR <- m + ci.max*c(-1, 1)
  yR <- c(0, 41*k/40)

  plot(xR, yR, type='n', xlab='Variable X', ylab='Échantillon', axes=TRUE)
  abline(v=m, lty=2, lwd=2,col='darkgreen')
  axis(3, at=m, paste("mu = ",round(m,4)), cex.axis=1.15)
  
  #axis(2)
  for(i in 1:k){
    x <- mean(c(hi[i],lo[i]))
    ci <- c(lo[i],hi[i])
    if((m < hi[i] & m > lo[i])==FALSE){
      col <- "#F05133"
      points(x, i, cex=1.4, col=col)
      #		  points(x, i, pch=20, cex=1.2, col=col)
      lines(ci, rep(i, 2), col=col, lwd=3)
    } else{
      col <- 1
      points(x, i, pch=20, cex=1.2, col=col)
      lines(ci, rep(i, 2), col=col)
    }
  }
  mtext(text=paste("Taux de capture = ",nhit*100,"%"),side=4,line=0.5,outer=FALSE,cex=1.0)
}
```

Finalement, le script suivant produit la simulation:

```{r 05-Chap5-9}

# Simulateur

nhit = 0      # Nombre de capture de mu
nexp = 50     # Nombre d'échantillons
nrep = 5000   # Nombre de ré-échantillonnage
n = 25        # Taille de l'échantillon
mu = 50       # Moyenne de la population
stdev = 8     # Écart-type de la population
nconf = 0.95  # Niveau de confiance
CIdat = matrix(NA,nrow=nexp,ncol=4)
for (i in 1:nexp){
  x = rnorm(n,mu,stdev)
  CIdat[i,] = exper(x,nrep,n,mu,stdev,nconf)
  nhit = nhit + ((CIdat[i,3]<=mu) & (CIdat[i,4]>=mu))
}
cat("Proportion de capture = ",nhit/nexp)
Sample = 1:nexp
Mean = CIdat[,2]
lower = CIdat[,3]
upper = CIdat[,4]
plot_ci(lower,upper,mu)

```

## Estimation d'une proportion 


L'estimation d'une proportion implique les mêmes concepts que ceux que nous avons discutés dans la section précédente. En effet, une proportion n'est qu'une moyenne calculée pour une variable dichotomique (0/1). Supposons que l'on tente d'estimer la proportion de la population favorisant un candidat particulier à la présidence. On tire un échantillon de 200 sujets de cette population et on comptabilise le nombre de sujets favorisant ce candidat. Le code R suivant réalise une telle expérience:

```{r Resampling_2}
set.seed(1234567)
x = sample(c(0,1),200,replace=TRUE,prob=c(0.8,0.2))		# Échantillon unique: n = 200, p = 0.2
nc = .95			        # Niveau de confiance
ns=100000			      # Nombre de ré-échantillonnage à effectuer
xb = replicate(ns,mean(sample(x,200,replace=TRUE)))
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
gr1=hist(xb,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des Proportions\n (Ré-échantillonnage)", xlab="Proportions", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Écart-Type = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```

Encore une fois, comparons ce résultat avec une distribution d'échantillonnage d'une proportion construite en tirant un nombre infini d'échantillons de la population:

```{r 05-Chap5-10}
echmoy = function(n=200,p=0.2){
  x = sample(c(0,1),200,replace=TRUE,prob=c(1-p,p))	
  moy = mean(x)
  return(moy)
}
set.seed(1234567)
nc = 0.95
ns = 10000 # Nombre d'échantillons à tirer
xb = replicate(ns,echmoy())
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
gr1=hist(xb,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des Proportions", xlab="Proportions", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Écart-Type = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```
Les résultats sont essentiellement identiques.

Comme pour l'estimation d'une moyenne,  la fonction **cisim()** de la librairie **PASWR2** permet de simuler des intervalles de confiance pour proportions:

```{r 05-Chap5-11}

library(PASWR2)

#Intervalle de confiance pour une variance
cisim(20, 200, 0.2, type = "Pi")


```
### Intervalle de confiance pour variances
 
Tout ce qui a été dit concernant l'intervalle de confiance pour une moyenne peut être répété pour toute statistique que l'on peut obtenir d'un échantillon. La variance ne fait pas exception.

Le code R suivant détermine l'intervalle de confiance d'une variance en utilisant le ré-échantillonnage:

```{r resample_3}
set.seed(1234567)
x = rnorm(25,100,15)		# Échantillon unique: n = 25, mu = 25, sigma = 8
nc = .95			        # Niveau de confiance
ns=100000			      # Nombre de ré-échantillonnage à effectuer
variance = replicate(ns,var(sample(x,100,replace=TRUE)))
intc=quantile(variance,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
gr1=hist(variance,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution des variances\n (Ré-échantillonnage)", xlab="Variances", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(variance),3))
txt2=paste("Écart-Type = ",round(sd(variance),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```

#### Simulation d'intervalles de confiance pour variances

Le code R suivant produit une simulation d'intervalles de confiance pour variances.

```{r 05-Chap5-12}
# Intervalle de confiance d'une variance
exper = function(x,nrep=5000,n=25,mu=50,sig=8,nconf=0.95){
  variance = replicate(nrep,var(sample(x,n,replace=TRUE)))
  varmean = mean(variance)
  stderr = sd(variance)
  lims = quantile(variance,c((1-nconf)/2,1-(1-nconf)/2))
  out = c(varmean,stderr,lims[1],lims[2])
  return(out)
}

#set.seed(1234567)
nhit = 0      # Nombre de capture de mu
nexp = 50     # Nombre d'échantillons
nrep = 5000   # Nombre de ré-échantillonnage
n = 25        # Taille de l'échantillon
mu = 50       # Moyenne de la population
stdev = 8     # Écart-type de la population
variance = stdev^2
nconf = 0.95  # Niveau de confiance
CIdat = matrix(NA,nrow=nexp,ncol=4)
for (i in 1:nexp){
  x = rnorm(n,mu,stdev)
  CIdat[i,] = exper(x,nrep,n,mu,stdev,nconf)
  nhit = nhit + ((CIdat[i,3]<=variance) & (CIdat[i,4]>=variance))
}
cat("Proportion de capture = ",nhit/nexp)
Sample = 1:nexp
Mean = CIdat[,2]
lower = CIdat[,3]
upper = CIdat[,4]
plot_ci(lower,upper,stdev^2)

```
La fonction cisim() permet également l'examen d'intervalles de confiance pour variances:

```{r 05-Chap5-13}

library(PASWR2)

#Intervalle de confiance pour une variance
cisim(100, 20, 100, 15, type = "Var")

```


## Exercices

1. Examinez l'indice de propension à la criminalité (CRIMIDX) dans le département de l'Ouest (DEPARTEMENT==8) en estimant les différents paramètres de la population, en utilisant un niveau de confiance de 90%. Utilisez un échantillon de 50 sujets.
  
    a. Estimez la moyenne:

```{r 05-Chap5-14}
# Population ciblée
subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 50    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000 		      # Nombre de ré-échantillonnage
xb = replicate(ns,mean(sample(xcv,n,replace=TRUE)))
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< μ < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Moyennes
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Moyennes\n (Ré-échantillonnage)",xlab="Moyennes",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```

  b. Estimez la médiane:
  
```{r 05-Chap5-15}
# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 50    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000 		      # Nombre de ré-échantillonnage
xb = replicate(ns,median(sample(xcv,n,replace=TRUE)))
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< Médiane < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Médianes
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Médianes\n (Ré-échantillonnage)",xlab="Médianes",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```
  c. Estimez la proportion des individus dont l'indice de criminalité est supérieur à 140.
  
```{r 05-Chap5-16}

# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 50    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000 		      # Nombre de ré-échantillonnage
# x = sum(sample(xcv,n,replace=TRUE)>140)/n
xb = replicate(ns,sum(sample(xcv,n,replace=TRUE)>140)/n)
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< P(CRIMIDX > 140) < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Proportions (P(CRIMIDX)>140)
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des P(CRIMIDX > 140)\n (Ré-échantillonnage)",xlab="P(CRIMIDX>140)",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
``` 
  d. Estimez le rapport entre l'écart-type et la moyenne, multiplié par 100: $CV = \frac{\sigma}{\mu}\times 100$

```{r 05-Chap5-17}
# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 50    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000 		      # Nombre de ré-échantillonnage
cv = vector()

for (i in 1:ns){
  x = sample(xcv,n,replace=TRUE)
  cv[i] = sd(x)/mean(x)*100
}

#
cat("Moyenne         = ",mean(cv))
cat("Erreur Standard = ",sd(cv))
# Détermination de l'intervalle de confiance
intc=quantile(cv,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< CV < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Coefficients de Variation
  gr1=hist(cv,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des CVs\n (Ré-échantillonnage)",xlab="Coefficients de Variation",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(cv),3))
  txt2=paste("Erreur Std = ",round(sd(cv),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```

  e. Estimez le 75ième percentile.
  
```{r 05-Chap5-18}
# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 50    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
p75 = replicate(ns,quantile(sample(xcv,n,replace=TRUE),0.75))
#
cat("Moyenne         = ",mean(p75))
cat("Erreur Standard = ",sd(p75))
# Détermination de l'intervalle de confiance
intc=quantile(p75,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< P75 < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Q3
  gr1=hist(p75,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des P75\n (Ré-échantillonnage)",xlab="P75",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```
  f. Répétez ces estimations, et utilisant des échantillons de *n* = 500 observations. Que remarquez-vous?  
    
    a. Estimez la moyenne:

```{r 05-Chap5-19}
# Population ciblée
subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 500    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
xb = replicate(ns,mean(sample(xcv,n,replace=TRUE)))
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< μ < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Moyennes
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Moyennes\n (Ré-échantillonnage)",xlab="Moyennes",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```

  b. Estimez la médiane:
  
```{r 05-Chap5-20}
# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 500    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
xb = replicate(ns,median(sample(xcv,n,replace=TRUE)))
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< Médiane < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Médianes
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Médianes\n (Ré-échantillonnage)",xlab="Médianes",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```
  c. Estimez la proportion des individus dont l'indice de criminalité est supérieur à 140.
  
```{r 05-Chap5-21}

# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 500    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
# x = sum(sample(xcv,n,replace=TRUE)>140)/n
xb = replicate(ns,sum(sample(xcv,n,replace=TRUE)>140)/n)
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< P(CRIMIDX > 140) < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Proportions (P(CRIMIDX)>140)
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des P(CRIMIDX > 140)\n (Ré-échantillonnage)",xlab="P(CRIMIDX>140)",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
``` 
  d. Estimez le rapport entre l'écart-type et la moyenne, multiplié par 100: $CV = \frac{\sigma}{\mu}\times 100$

```{r 05-Chap5-22}
# Population ciblée
# subpop = subset(HaitiPop,(DEPARTEMENT==8)&(AGE>=15),select=CRIMIDX)

n = 500    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
cv = vector()

for (i in 1:ns){
  x = sample(xcv,n,replace=TRUE)
  cv[i] = sd(x)/mean(x)*100
}

#
cat("Moyenne         = ",mean(cv))
cat("Erreur Standard = ",sd(cv))
# Détermination de l'intervalle de confiance
intc=quantile(cv,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< CV < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Coefficients de Variation
  gr1=hist(cv,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des CVs\n (Ré-échantillonnage)",xlab="Coefficients de Variation",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(cv),3))
  txt2=paste("Erreur Std = ",round(sd(cv),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
```

  e. Estimez le 75ième percentile.
  
```{r 05-Chap5-23}
# Population ciblée
library(data.table)
DT <- as.data.table(HaitiPop)     # data.table
subpop = DT[(DEPARTEMENT==8)&(AGE>=15)]  # Population ciblée

n = 500    # Taille de l'échantillon
xcv = sample(subpop$CRIMIDX,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .90			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
p75 = replicate(ns,quantile(sample(xcv,n,replace=TRUE),0.75))
#
cat("Moyenne         = ",mean(p75))
cat("Erreur Standard = ",sd(p75))
# Détermination de l'intervalle de confiance
intc=quantile(p75,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< P75 < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Q3
  gr1=hist(p75,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des P75\n (Ré-échantillonnage)",xlab="P75",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
``` 
  f. Estimez le consensus par rapport au projet d'une nouvelle Constitution, au niveau national, avec un niveau de confiance de 95% et un échantillon de 300. Rappelons que seuls les individus dont l'âge est égal ou supérieur à 18 ans sont sondés!
  
```{r 05-Chap5-24}
library(data.table)
library(agrmt)
DT <- as.data.table(HaitiPop)     # data.table
subpop = DT[AGE >= 18]$OpinConst  # Population ciblée

n = 300    # Taille de l'échantillon
xcv = sample(subpop,n,replace=FALSE)
#
# Ré-échantillonnage
nc = .95			        # Niveau de confiance
ns = 100000		      # Nombre de ré-échantillonnage
xb = replicate(ns,consensus(table(sample(xcv,n,replace=TRUE))))
#
cat("Moyenne         = ",mean(xb))
cat("Erreur Standard = ",sd(xb))
# Détermination de l'intervalle de confiance
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< Consensus < ",round(intc[2],3),"] = ",100*nc,"%")
# Distribution des Consensus
  gr1=hist(xb,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Consensus\n (Ré-échantillonnage)",xlab="Consensus",     col=c("red", "cadetblue1", "red")[cuts])
  txt1=paste("Moyenne    = ",round(mean(xb),3))
  txt2=paste("Erreur Std = ",round(sd(xb),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")
``` 


## En option...

R offre des fonctions que permettent de faciliter grandement le travail de ré-échantillonnage. En voici un exemple, appliqué au problème de l'estimation d'une médiane:

```{r 05-Chap5-25}
boot.median <- function(data, num) {
  resamples <- lapply(1:num, function(i) sample(data, replace=T))
  r.median <- sapply(resamples, median)
  std.err <- sqrt(var(r.median))
  list(std.err=std.err, resamples=resamples, medians=r.median)   
}

dat = rnorm(50,50,8)
bootstr = boot.median(dat,100000)
cat("Moyenne des médianes         = ",mean(bootstr$medians))
cat("Erreur-Standard des médianes = ",bootstr$std.err)
hist(bootstr$medians,breaks="FD")

```

La même stratégie peut être appliquée à l'estimation du consensus, Dans ce cas, on définit une fonction (**constabl**) qui compile le tableau des fréquences pour la variable ordinale **X**, et à partir de ce tableau, calcule et retourne l'indice de consensus. La fonction **b.consensus** génère **num** ré-échantillonnage des données (**data**), appelle la fonction **constabl** pour chacun de ces ré-échantillonnage, pour ensuite fournir les éléments permettant de spécifier la distribution d'échantillonnage des consensus.

```{r 05-Chap5-26}
constabl=function(x){
  tbl=table(x)
  consens=consensus(tbl)
  return(consens)
}

b.consensus <- function(data, num) {
  resamples <- lapply(1:num, function(i) sample(data, replace=T))
  r.consensus <- sapply(resamples, constabl)
  std.err <- sd(r.consensus)
  list(std.err=std.err, resamples=resamples, consensus=r.consensus)   
}

x = sample(1:7,50,replace=TRUE)

bootstr = b.consensus(x,10000)
cat("Moyenne des consensus         = ",mean(bootstr$consensus))
cat("Erreur-Standard des consensus = ",bootstr$std.err)
hist(bootstr$consensus,breaks="FD")
```
## Fonction **boot()** et **boot.ci()**

La librairie **boot** permet d'obtenir des estimations de différents paramètres d'une population tout en offrant une vaste gamme d'options pertinentes. Un exemple d'utilisation de ces fonctions est reproduit ci-dessous. Les arguments de la fonction boot sont:
  - x: le vecteur contenant les données
  - function(x,i) mean(x)[i]: **i** est généré automatiquement et représente l'indice des différentes observations. La fonction **mean()** sera utilisée.
  - R: nombre de ré-échantillonnage à effectuer
  
La fonction **boot.ci()** reprend la sortie de boot() et calcule différents types d'intervalles de confiance. Les arguments de cette fonction sont:
  - Mboot: la sortie de boot()
  - conf: le niveau de confiance
  - type: les types d'intervalles désirés:
      a. Classique, fondé sur la distribution normale (Gaussienne)
      b. Basic: Considère la nature aléatoire du paramètre estimé 
      c. perc: utilise les percentiles de la distribution de ré-échantillonnage
      d. BCa: corrige pour le biais et l'asymétrie d'une distribution
      e. Stud: fondé sur la distribution t de Student (nécessite les estimations des variances)

```{r 05-Chap5-27}
library(boot)

Mboot = boot(x,function(x,i) mean(x[i]),R=10000)
Mboot
boot.ci(Mboot,conf=0.95,type=c("norm","basic","perc","bca"))
```


## Appendice: Fonctions définies pour cette section

```{r 05-Chap5-28, eval=FALSE}
HaitiPopGen = function(){
set.seed(1234567)
# HaitiPop: Simultation de la population Haitienne (2019)
# Genre Féminin, effectif par groupe d'âge (données de 2019)
Age_female=c(620842, 608143, 590011, 565536, 531966, 493101, 458841,
             410081, 309636, 245577, 218103, 187285, 149090, 118763,
             80994, 60334, 34915, 15440, 5211, 1397, 217) 

# Genre Masculin, effectif par groupe d'âge (données de 2019)
Age_male=c(645049, 629568, 607263, 571570, 525211, 479976, 442822, 
           390944, 286062, 221042, 199898, 169226, 135904, 102412, 
           64604, 47325, 24681, 10151, 3085, 720, 83)  
PopTotale = sum(Age_male)+sum(Age_female)

# Génération des âges et du genre
age = c(seq(4,104,by=5))
AGE = vector()
GENRE = vector()
for(i in 1:length(age)){
  AGE = append(AGE,floor(runif(Age_male[i],(age[i]-4),age[i]+5)))
  AGE = append(AGE,floor(runif(Age_female[i],(age[i]-4),age[i]+5)))
  GENRE = append(GENRE,rep("M",Age_male[i]))
  GENRE = append(GENRE,rep("F",Age_female[i]))
}
Tdt = data.frame(AGE,GENRE)
Tdt = Tdt[sample(nrow(Tdt)),1:2]
# Génération des Départements et répartition de la population
Dept = matrix(c(1:10,"Artibonite","Centre","Grand'Anse","Nippes",
                "Nord","Nord-Est","Nord-Ouest","Ouest","Sud-Est",
                "Sud"),nrow=2,byrow=TRUE)
# cat("Identification des Départements: ","\n",t(Dept))
# Proportion de la population par département
PropDept = c(0.158316776, 0.037133348, 0.024201755, 0.018140699, 
             0.057563731, 0.022548639, 0.042675426, 0.24647869, 
             0.051350001, 0.066312107) 
DEPARTEMENT = sample(Dept[1,],size=PopTotale,replace=TRUE, prob=PropDept)
# Indice de condition de vie, distribution asymétrique
CondxVie = rchisq(PopTotale,6)
Tdt = data.frame(Tdt,DEPARTEMENT,CondxVie)
# Génération des niveaux de scolarité atteints, population adulte
EDUC=rep("NA",PopTotale)
NivScol = c("Aucun","PréScol","Fond.1","Fond.2","Fond.2","Second",
            "Univ.1","Univ.2","Univ.3")
PropNivScol = c(0.2, 0.07, 0.2, 0.18, 0.12, 0.09, 0.06, 0.05, 0.03) 
EDUC[Tdt$AGE>25] = sample(NivScol,sum(Tdt$AGE>25),prob=PropNivScol,replace=TRUE)

# Génération de l'indice de criminalité

CRIMIDX = rep("NA",PopTotale)
idx = vector()
for(i in 1:10){
  idx[i] = sample(c(1,-1),1)*runif(1,0,30)
}

CRIMIDX = 0.3*CondxVie + 3.5*(Tdt$GENRE=="M")-1.2*Tdt$AGE
CRIMIDX = CRIMIDX +1.5*(EDUC<"Second")+10 +rnorm(1,0,4)
CRIMIDX = CRIMIDX + idx[as.numeric(DEPARTEMENT)]
CRIMIDX = CRIMIDX + (min(CRIMIDX)<0)*abs(min(CRIMIDX))
CRIMIDX[which(Tdt$AGE<15)] = NA

# Génération de l'indice d'habiletés cognitives
HabilCog = rnorm(PopTotale,100,15)
idx = vector()
for(i in Dept){
  idx[i] = sample(c(1,-1),1)*runif(1,0,10)
}
HabilCog = HabilCog+idx[DEPARTEMENT]

# Génération de la préférence pour un candidat présidentiel
# et de l'Opinion concernant la proposition d'une nouvelle Constitution

library(wakefield)
PrefPres = rep(NA,PopTotale)
OpinConst = rep(NA,PopTotale)

# Probabilités associées aux variables PresProb et OpinProb
PresProb = matrix(c(0.58, 0.41, 0.003, 0.004, 0.003, # Nord-Ouest, Nord et Nord-Est
                    0.41, 0.54, 0.003, 0.004, 0.043, # Artibonite et Centre
                    0.27, 0.65, 0.006, 0.01, 0.064,  # Ouest
                    0.15, 0.23, 0.38, 0.19, 0.05),   # Grand<Anse, Nippes, Sud et Sud-Est
          nrow = 4, ncol = 5, byrow = TRUE)
OpinProb = matrix(c(0.5,0.3,0.1,0.05,0.05,           # Nord-Ouest, Nord et Nord-Est
                    0.2,0.4,0.1,0.1,0.2,             # Artibonite et Centre
                    0.01,0.09,0.2,0.4,0.3,           # Ouest
                    0.4,0.1,0.0,0.1,0.4),            # Grand<Anse, Nippes, Sud et Sud-Est
          nrow = 4, ncol = 5, byrow = TRUE)
Regions = list(c("5","6","7"), c("1","2"), c("8"),c("3","4","9","10"))         
OptPres = c("CandA", "CandB", "CandC", "CandD", "CandE")
OptOpin = c("Tout à fait d'accord","D'accord","Indécis","Pas d'accord","Pas du tout d'accord")

for (i in 1:length(Regions)){
  select = (Tdt$AGE>=18)&(DEPARTEMENT %in% Regions[[i]])
  PrefPres[select] = political(n=sum(select), x = OptPres,prob = PresProb[i,], name = "PrefPres")
  OpinC = r_sample_ordered(n=sum(select),x = OptOpin,prob = OpinProb[i,])
  OpinConst[select] = OpinC
  }

# DATA.FRAME de sortie
HaitiPop = data.frame(Tdt,EDUC,CRIMIDX,HabilCog,PrefPres,OpinConst)
return(HaitiPop)
}


```


<!--chapter:end:05-Chap5.Rmd-->

---
title: "Inférence Statistique - Tests d'hypothèse"
author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
---

```{r 06-Chap6-1, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```

```{r 06-Chap6-2, include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r 06-Chap6-3,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(moments,formatR,kableExtra,sampler,dplyr,data.table,isteahMAT104)
```
```{r 06-Chap6-4, include=FALSE}
# Importer HaitiPop dans l'environnement
# if(!exists(HaitiPop)){
 load("I:/MAT104/HaitiPop_Env.RData")
# }
```

# Inférence statistique: Test d'hypothèse, échantillon unique

Les intervalles de confiance nous permettent d'estimer un paramètre d'une population à partir des observations faites sur un échantillon. À la fin de l'exercice, on est en mesure de spécifier, avec un certain niveau de confiance, que le paramètre de la population est capturé par l'intervalle obtenu.

Dans un grand nombre de situations, on peut avoir une idée de la valeur d'un paramètre d'une population, et on peut formuler des hypothèses à cet égard. Par exemple, supposons que l'OCDE estime que l'indice de condition de vie moyen à l'échelle nationale est égal à 4.7. Puisque le département du Nord est une composante du pays, on pourrait penser ou croire que la moyenne de cet indice et également 4.7 dans cette région. Cette croyance pourrait nous amener à formuler une hypothèse telle que:  

  
 **La moyenne de CondxVie dans le département du Nord est égale à 4.7**
  
Cette hypothèse peut être vraie, ou elle peut être fausse. Il nous faut un moyen de la confirmer ou de l'infirmer, sur la base d'observations que l'on obtiendrait auprès d'un échantillon aléatoire d'Haitiens vivant dans le département concerné. Et c'est dans ce type de contexte que les tests d'hypothèse entrent en jeu. Nous reviendrons sur cette question un peu plus loin. Un exemple légèrement plus simple nous permettra d'introduire les concepts nécessaires dans la vérification d'une hypothèse.
  
## Exemple 1:

Dans la région des Gonaives, le Père Gérard produit des sachets de thé de Moringa dans un contentant sur lequel est indiqué le poids: 50 grammes. Se pourrait-il que le poids indiqué soit erronné? 
![Contenant de Thé de Moringa](./images/moringa.jpg){height=50%}

Pour répondre à une question de ce type, il convient de formuler au préalable une hypothèse spécifique. Par exemple, on pourra énoncer l'hypothèse selon laquelle la moyenne de la population des poids de contenants de thé de Moringa est égale à 50 grammes. Par la suite, il suffira de vérifier si les observations faites auprès d'un échantillon aléatoire appuient cette hypothèse, ou tendent plutôt à l'infirmer.

### Les hypothèses statistiques

L'hypothèse voulant que *la moyenne de la population des poids de contenants de thé de Moringa est égale à 50 grammes* se traduit formellement par une expression telle que:
$$H_0:\mu = 50$$
Il s'agit de l'hypothèse dite **nulle**. Elle propose que le paramètre de la population d'où est tiré l'échantillon est égale à une valeur spécifique. C'est l'hypothèse sur laquelle porte l'ensemble de la procédure; c'est cette hypothèse que l'on retiendra ou que l'on rejettera. Elle implique **toujours** une égalité (**=**).

Advenant le cas où $H_0$ doit être rejetée, on considérera son opposée, l'hypothèse **alternative**. Si on ne peut retenir l'hypothèse que $\mu = 50$, alors on pourra croire que $\mu \neq 50$, ou que $\mu < 50$, ou encore que $\mu > 50$.  Laquelle de ces trois possibilité sera choisie dépend de la question de recherche qui a conduit à l'énoncé de l'hypothèse nulle:  
  
  - Est-ce que le poids des contenants de thé est **différent** de celui indiqué?  $\mu \neq 50$  
  - Est-ce que le poids des contenants de thé est **inférieur** à celui indiqué?  $\mu < 50$  
  - Est-ce que le poids des contenants de thé est **supérieur** à celui indiqué?  $\mu > 50$  

Dans le premier cas, on qualifiera de **non-directionnel** ou de **bilatéral** le test d'hypothèse; dans les deux autres cas, on utilisera le terme **directionnel** ou **unilatéral** pour qualifier ce test.

### La procédure générale

Un échantillon aléatoire de 25 contenants, que vous prenez soin de peser à l'aide d'un instrument de haute précision, vous apporte les éléments nécessaires pour répondre à cette question. La stratégie est la suivante:

  - Calculons la moyenne des poids de contenants de thé pour l'échantillon que nous avons tiré aléatoirement de la population de l'ensemble des contenants produits au cours de la dernière année
  - Spécifions la distribution d'échantillonnage des moyennes
  - Centrons cette distribution sur ce que nous croyons être la moyenne de la population: 50 grammes dans le cas présent
  - Obtenons de cette distribution, la probabilité d'obtenir une moyenne échantillonnale égale ou plus extrême que celle obtenue pour notre échantillon
  - Prenons une décision sur la base de cette probabilité: 
    - si elle est élevée, les observations appuient notre hypothèse et on la retient
    - si elle est faible, les observations vont à l'encontre de notre hypothèse, et on la rejette...
    
### Le niveau de signification

La probabilité que l'on définit comme critère de la prise de décision concernant le rejet ou non de $H_0$ porte le nom de **niveau de signification** et est symbolisée par $\alpha$. Il s'agit de la probabilité de rejeter $H_0$, si cette hypothèse est **vraie**. Formellement, il s'agit d'une probabilité conditionnelle: 
$$\alpha = P(Rejeter\;H_0 | H_0\; vraie)$$
C'est donc la probabilité de commettre une erreur, en rejetant une hypothèse qui, au niveau de la population, est vraie. On veut donc que cette probabilité ne soit pas trop élevée: on veut se tromper le moins souvent possible! 

On se souviendra du **niveau de confiance** introduit dans un contexte d'estimation d'un paramètre d'une population. Il s'agissait de la probabilité qu'un intervalle donné capture le paramètre recherché. On voulait cette probabilité élevée, de manière à pouvoir obtenir des estimations justes le plus souvent possible!

Les notions de niveau de confiance $(1-\alpha)$ et de niveau de signification $(\alpha)$ sont donc complémentaires. Dans le cas de l'estimation, on utilise souvent $(\alpha)=0.95$; dans une vérification d'hypothèse, on utilisera souvent $\alpha=0.05$. Mais ce choix n'est pas absolu: l'analyste conservera toujours la liberté de choisir le niveau de signification en fonction du contexte de la recherche. Et on verra qu'au-delà du niveau de signification, il y a d'autres critères à considérer lorsqu'arrive le temps de prendre une décision quant au rejet ou non de $H_0$.

### Retour à l'exemple 1

Vérifions l’hypothèse au niveau de signification $\alpha$ = 0.05. Notons qu'on cherche à savoir s'il pourrait y avoir une erreur dans l'affichage du poids des contenants, que cette erreur se traduise par un poids inférieur, ou supérieur, à celui indiqué sur le contenant. Il s'agit donc d'une hypothèse bilatérale, non-directionnelle.

Les données recueillies sont les suivantes:

```{r 06-Chap6-5}
set.seed(123)
x = round(rnorm(25,51,4),2) # Génération des données
xb = round(mean(x),2)
sdx = round(sd(x),2)
cat("Données: \n")
matrix(x,nrow=5,byrow=TRUE)
cat("Moyenne    = ",round(xb,2))
cat("Écart-Type = ",round(sdx,2))
```
Les hypothèses statistiques:  

  - $H_0:\mu = 50$
  - $H_1:\mu \neq 50$

Dans cette situation, l'hypothèse nulle est que la moyenne de la population d'où est tiré l'échantillon est $\mu=50$. Ultimement, la question que l'on pose est: 

'Si H_0 est vraie, quelle est la probabilité d'observer $\bar X=$ `r xb` ou une valeur plus extrême?' 

La probabilité que l'on cherche correspond à une aire sous la distribution décrite par H_0, c'est-à-dire une distribution dont la moyenne est $\mu=50$. Conceptuellement, Il est donc nécessaire de centrer la distribution d'échantillonnage sur cette valeur:

```{r 06-Chap6-6}
mu0 = 50
H0Dat<-x-xb+mu0   # (X-M)+mu

```
Cette transformation est linéaire et ne change en rien ni la forme, ni la dispersion de la distribution des données. Elle permet simplement d'obtenir une distribution supposant que $H_0$ est vraie.

Pour effectuer une vérification de l'hypothèse nulle, nous ne disposons que d'un seul échantillon, mais nous devons évidemment nous référer à une distribution d'échantillonnage appropriée à la situation. Comme pour l'estimation d'un paramètre d'une population, le ré-échantillonnage nous permet d'établir cette distribution:
  
```{r 06-Chap6-7}
# Fonction boot.mean()
# Arguments: data = vecteur des données échantillonnales
#            num = Nombre de ré-échantillonnage à effectuer
# Sortie: std.err = Erreur-Standard de la distribution d'écahantillonnage
#         resamples = liste des num échantillons
#         means = vecteur des moyennes observées pour chaque échantillon

boot.mean <- function(data, num) {
  resamples <- lapply(1:num, function(i) sample(data, replace=TRUE))
  r.mean <- sapply(resamples, mean)
  std.err <- sd(r.mean)
  list(std.err=std.err, resamples=resamples, means=r.mean)   
}

#
# Ré-échantillonnage et affichage des résultats
#
bootstr = boot.mean(H0Dat,100000)
cat("Moyenne des moyennes         = ",mean(bootstr$means))
cat("Erreur-Standard des moyennes = ",bootstr$std.err)

# Détermination de l'intervalle de confiance
nc = 0.95
intc=quantile(bootstr$means,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< μ < ",round(intc[2],3),"] = ",100*nc,"%")

# Distribution des Moyennes
  gr1=hist(bootstr$means,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Moyennes\n (Ré-échantillonnage)",xlab="Moyennes",     col=c("red", "cadetblue1", "red")[cuts])
  abline(v=xb,col="red",lwd=3,lty=2)
  txt1=paste("Moyenne    = ",round(mean(bootstr$means),3))
  txt2=paste("Erreur Std = ",round(sd(bootstr$means),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

# Conclusion:
  
  cat("Pour un test unilatéral droit:")
    pval1<-mean(bootstr$means>=xb)
    cat("p = ",pval1)
    if(pval1<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
    
  cat("Pour un test unilatéral gauche:")
    pval2<-mean(bootstr$means<=xb)
    cat("p = ",pval2)
    if(pval2<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
      
  cat("Pour un test bilatéral:")
    pval3<-min(mean(bootstr$means>=xb), mean(bootstr$means<=xb))*2
    cat("p = ",pval3)
    if(pval3<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
     
 
```
On trouve donc que s'il est vrai que le poids moyen des contenants de Moringa est de 50 grammes, la probabilité que la moyenne d'un échantillon tiré de cette population soit égale ou plus extrême que `r xb` est élevée (p = `r pval3`): elle est largement supérieur au niveau de signification choisi. De ce fait, on ne peut pas rejeter l'hypothèse nulle. On n'a pas suffisamment d'évidence pour affirmer que le poids indiqué sur les contenants de Moringa est incorrect.

## Exemple 2: Test d'hypothèse pour une moyenne

Est-ce que l'indice de condition de vie moyen dans le Département du Nord est supérieur à 4.7? 
Pour répondre à cette question, formons un échantillon aléatoire de 50 individus vivant dans le département du Nord (DEPARTEMENT==5):

```{r 06-Chap6-8}
set.seed(23434334)
subpop = subset(HaitiPop,(DEPARTEMENT==5),CondxVie)
xech = sample(subpop$CondxVie,50,replace=FALSE)
mu0 = 4.7
xb = mean(xech)
cat("Moyenne = ",xb)
H0Dat<-xech-xb+mu0   # Recentrage des données sur H0
```
Les hypothèses statistiques:  

  - $H_0:\mu = 4.7$
  - $H_0:\mu > 4.7$

Par ré-échantillonnage (100,000 échantillons), on obtient:

```{r 06-Chap6-9}
#
# Ré-échantillonnage et affichage des résultats
#
bootstr = boot.mean(H0Dat,100000)
cat("Moyenne des moyennes         = ",mean(bootstr$means))
cat("Erreur-Standard des moyennes = ",bootstr$std.err)

# Détermination de l'intervalle de confiance
nc = 0.95
intc=quantile(bootstr$means,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< μ < ",round(intc[2],3),"] = ",100*nc,"%")

# Distribution des Moyennes
  gr1=hist(bootstr$means,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des Moyennes\n (Ré-échantillonnage)",xlab="Moyennes",     col=c("red", "cadetblue1", "red")[cuts])
  abline(v=xb,col="red",lwd=3,lty=2)
  txt1=paste("Moyenne    = ",round(mean(bootstr$means),3))
  txt2=paste("Erreur Std = ",round(sd(bootstr$means),3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

# Conclusion:
  
  cat("Pour un test unilatéral droit:")
    pval1<-mean(bootstr$means>=xb)
    cat("p = ",pval1)
    if(pval1<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
    
  cat("Pour un test unilatéral gauche:")
    pval2<-mean(bootstr$means<=xb)
    cat("p = ",pval2)
    if(pval2<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
      
  cat("Pour un test bilatéral:")
    pval3<-min(mean(bootstr$means>=xb), mean(bootstr$means<=mu0))*2
    cat("p = ",pval3)
    if(pval3<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
 
```

Pour une hypothèse unidirectionnelle à droite, on trouve p = `r pval1`, que est en-deça du seuil de signification. S'il est vrai que la moyenne de la population est égale à 4.7, la probabilité qu'un échantillon tiré de cette population présente une moyenne supérieure ou égale à `r round(xb,3)` est faible. On a donc un appui à l'idée que l'indice de condition de vie moyen dans le département du Nord est supérieur à ce que l'OCDE estime au niveau national.

Les résultats de l'analyse incluent aussi un intervalle de confiance. On trouve:


 C[`r round(intc[1],3)`< $\mu_0$ <  `r round(intc[2],3)`] = 95% 


Cet intervalle représente l'ensemble des valeurs probables des moyennes d'échantillons, **si l'hypothèse nulle est vraie**. Il est donc distinct de celui que l'on obtient à partir des données échantillonnales non-centrées:
```{r 06-Chap6-10}
nc = 0.95
intc=quantile(bootstr$means,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3)+xb-mu0,"< μ < ",round(intc[2],3)+xb-mu0,"] = ",100*nc,"%")
```
Dans le premier cas, on considère la distribution décrite par l'hypothèse nulle, alors que dans le second cas, on n'impose pas de contrainte sur la valeur de la moyenne de la population. On peut vérifier à l'aide des fonctions **boot()** et **boot.ci()**:

```{r 06-Chap6-11}
library(boot)

Mboot1 = boot(xech,function(x,i) mean(x[i]),R=100000)
Mboot1
bootic1 = boot.ci(Mboot1,conf=0.95,type="perc")
bootic1

Mboot2 = boot(H0Dat,function(x,i) mean(x[i]),R=100000)
Mboot2
bootic2 = boot.ci(Mboot2,conf=0.95,type="perc")
bootic2

lo = c(bootic1$percent[4],bootic2$percent[4])
hi = c(bootic1$percent[5],bootic2$percent[5])
plot_ci(lo,hi,mu0)
```
On note dans le graphique, que l'intervalle de confiance obtenu à partir des données non-centrées (originales) ne capture pas la moyenne de la population qui a été proposée dans l'hypothèse nulle. On peut donc conclure qu'effectivement, la moyenne de l'échantillon est improbable si l'hypothèse nulle est vraie. De ce fait, on peut constater que l'estimation statistique et le test d'hypothèse sont les deux côtés d'une même médaille: ils ne peuvent fournir des indications contraires.

## Exemple 3: Test d'hypothèse pour une médiane

Est-ce que la l'indice de Condition de Vie médian dans le département du Nord pourrait être supérieur à 5? Pour répondre à cette question, on peut introduire la fonction **boot.one.per()** de la librairie **wBoot**. Cette fonction offre la possibilité d'effectuer un test d'hypothèse autant sur une variance qu'une moyenne, une médiane, ou toute autre fonction définie par l'analyste. On obtient également les intervalles de confiance pertinents au problème.  Les paramètres de la fonction sont:  

  - x: le vecteur des données  
  - parameter: la fonction définissant e paramètre à étudier (eg: mean, median, var, sd, etc.)  
  - null.hyp: paramètre proposé dans l'hypothèse nulle  
  - alternative: c("two.sided","less","greater"): Directionalité de l'hypothèse  
  - conf.level: niveau de confiance  
  - R: Nombre de ré-échantillonnage à effectuer  
    
Un échantillon aléatoire tiré de la population ciblée fournit les données suivantes:

```{r 06-Chap6-12}
set.seed(2234343)
library(wBoot)

subpop = subset(HaitiPop,(DEPARTEMENT==10),CondxVie)
xech = sample(subpop$CondxVie,50,replace=FALSE)
cat("Médiane = ",median(xech))
md0 = 5   # Médiane sous H0

```
Les hypothèses statistiques:  

  - $H_0:Md = 5$
  - $H_1:Md > 5$

Par ré-échantillonnage (100000 échantillons), on obtient:

```{r 06-Chap6-13}
#
# Ré-échantillonnage et affichage des résultats
#
b.med = boot.one.per(xech,median,null.hyp=md0,alternative="greater",conf.level=0.95,R=100000)
print.boot.one(b.med)
 
```
Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir une $médiane\geq$ `r round(b.med$Observed,3)` est égale à `r b.med$p.value`. En conséquence, on `r if(b.med$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$.  

## Exemple 4: Test d'hypothèse pour une variance

Selon le Bureau des Statistiques Sociales d'Haiti, la variance de l'indice de criminalité pour l'ensemble du pays est égale à 550. Un chercher en Criminologie de l'ISTEAH, suite à des études préparatoires sur le terrain, croit qu'en Artibonite, les individus sont relativement homogènes à ce niveau,ce qui se traduirait pas une variance plus faible que dans l'ensemble du territoire Haitien. Pour s'en convaincre, il forme un échantillon aléatoire de 100 résidents d'Artibonite et mesure chez chacun d'eux l'indice de criminalité en utilisant une échelle psychométrique standardisée. Son hypothèse, selon laquelle la variance de la population d'Artibonite est plus faible qu'à l'échelle nationale, se traduit par les hypothèses statistiques suivantes:  

  - $H_0:\sigma^2 = 500$
  - $H_1:\sigma^2 < 500$
  
Un échantillon aléatoire composé de 100 individus vivant en Artibonite est formé et on évalue l'indice de criminalité chez chacun d'eux. On obtient ensuite les données échantillonnales:

```{r 06-Chap6-14}
subpop = subset(HaitiPop,(AGE>=15)&(DEPARTEMENT=="1"),CRIMIDX)
xcrim = sample(subpop$CRIMIDX,100,replace=FALSE)
xb = mean(xcrim)
vcrim = var(xcrim)
cat("Moyenne  = ",xb)
cat("Variance = ",vcrim)
var0 = 500    # Variance de la population sous H0

```

Encore une fois, la fonction **boot.one.per()** nous permettra de répondre à cette question:


```{r 06-Chap6-15}
library(wBoot)
b.var = boot.one.per(xcrim,var,null.hyp=500,alternative="less",conf.level=0.95,R=100000)
print.boot.one(b.var)

```
Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir une $s^2\leq$ `r round(b.var$Observed,3)` est égale à `r b.var$p.value`. En conséquence, on `r if(b.var$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$.
  
## Exemple 5: Test d'hypothèse pour un consensus

On croit que le consensus concernant le projet de nouvelle Constitution est plus élevé dans la région du Nord (départements 5, 6 et 7) que dans l'ensemble du pays, que l'on estime à 0.56. Est-ce que cette croyance pourrait être se vérifier, à l'aide d'un échantillon de 250 résidents du Nord?

L'échantillon tiré de la population du Nord est constitué:

```{r 06-Chap6-16}
library(data.table)
library(agrmt)
DT <- as.data.table(HaitiPop)     # data.table
subpop = DT[(AGE >= 18)&(DEPARTEMENT %in% c("5","6","7"))]$OpinConst  # Population ciblée
xech = subpop[sample(1:length(subpop),250,replace=FALSE)]
tbl = table(xech)
cat("Répartition observée :")
tbl
```

Les hypothèses statistiques:  

  - $H_0:Consensus = 0.56$
  - $H_1:Consensus \gt 0.56$

Pour effectuer cette analyse, il sera nécessaire de définir une fonction qui produira la répartition des réponses et calculera l'indice de consensus pour chacun des ré-échantillons:


```{r 06-Chap6-17}
constabl=function(x){
  tbl=table(x)
  consens=consensus(tbl)
  return(consens)
}

```
Finalement, l'analyse se poursuit avec 100,000 ré-échantillonnages:


```{r 06-Chap6-18}
library(wBoot)
b.cons = boot.one.per(xech,constabl,null.hyp=0.56,alternative="greater",conf.level=0.95,R=100000)
print.boot.one(b.cons)

```

Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir un $consensus\ge$ `r round(b.cons$Observed,3)` est égale à `r b.cons$p.value`. En conséquence, on `r if(b.cons$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On constate que le consensus au niveau de la population du Nord concernant le projet de constitution `r if(b.cons$p.value<=0.05){"est "}else{"n'est pas "}` plus élevé que celui de l'ensemble des Haitiens.

## Exemple 6: Test d'hypothèse pour une proportion

La proportion des Haitiens qui ont l'intention de voter pour le candidat A à l'élection présidentielle, pour l'ensemble du pays s'établit à 23%, selon le plus récent sondage. Est-ce que cette proportion pourrait être différente dans le Département des Nippes, où on dispose d'un échantillon de 400 sujets?

L'échantillon obtenu aléatoirement fournit les données suivantes:

```{r 06-Chap6-19}
library(data.table)
library(agrmt)
DT <- as.data.table(HaitiPop)     # data.table
subpop = DT[(AGE >= 18)&(DEPARTEMENT =="4")]$PrefPres  # Population ciblée
xech = subpop[sample(1:length(subpop),400,replace=FALSE)]
propA = mean(xech==1)
cat("Proportion des intentions de vote (Candidat A) = ",propA)
```


  - $H_0:\pi = 0.23$
  - $H_0:\pi \neq 0.23$
  
La fonction suivante permettra de calculer la proportion des individus favorisant le Candidat A, au niveau d'un échantillon:

```{r 06-Chap6-20}
PropA = function(x){p = mean(x == 1)}
```

Finalement, on obtient les résultats suivant, suite à 100,000 ré-échantillonnages:

```{r 06-Chap6-21}
library(wBoot)
b.propA = boot.one.per(xech,PropA,null.hyp=0.23,alternative="two.sided",conf.level=0.95,R=100000)
print.boot.one(b.propA)

```
Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir une proportion en faveur du Candidat A égale ou plus extrême que la proportion observée (`r format(round(b.propA$Observed,3),scientific=FALSE)`) est égale à `r b.propA$p.value`. En conséquence, on `r if(b.propA$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que la population du Nord se démarque significativement de l'ensemble de la population Haitienne, en ce qui concerne le Candidat A.

## En bref...

Conceptuellement, la pratique du ré-échantillonnage avec l'objectif d'estimer un paramètre d'une population ou de vérifier une hypothèse concernant ce paramètre est simple. Sous R, on peut effectuer les tâches requises de plusieurs manières, ce qui devrait être manifeste à l'examen des différents exemples présentés dans ce document. La procédure la plus simple pourrait se décrire de la manière suivante:
  
  1. Définir la population cible, dont on veut estimer ou vérifier un paramètre donné.
    - La fonction **DT()** de la librairie **data.table**, ou la fonction **subset()** sont probablement les façons les plus simples d'effectuer cette définition:

***
|       **Avec data.table:**
  
|           library(data.table)  
|           DT <- as.data.table(*DataFrame*)   
|           subpop <- DT[*Condition de sélection*)]$*Variables*
  
|       **Avec subset()**:  
    
|          subpop <- subset(*DataFrame*,(*Condition de sélection*),*Variables*) 

***

  2. Tirer un échantillon aléatoire d'une certaine taille de cette population, en utilisant une méthode d'échantillonnage appropriée à la situation.
  
***  
|     **Avec sample() et une sélection aléatoire d'indices:**
  
|       x = subpop[sample(1:length(subpop),n,replace=FALSE)]
  
|     **Avec sample() et une sélection aléatoire des données:**
  
|       x = sample(subpop$*Variable*,n,replace=FALSE)

***

  3. Définir une fonction permettant de calculer l'estimateur du paramètre (ie la statistique échantillonnale). Il peut s'agir d'une fonction existante dans R, ou d'une fonction créée par l'analyste:

*** 
|   **Fonction existante:**
    
|       mean, median, var, sd, etc.
  
|   **Fonction définie par l'analyste:**
  
|       *fns* <- function(x){
|       *sortie* <- *calcul d'une statistique*
|       }

***

  4. Ré-échantillonner un grand nombre de fois, avec remise, et obtenir la distribution d'échantillonnage appropriée. La manière la plus simple pourrait être:
  
***
|   library(wBoot)
|   *Variable* = boot.one.per(x,*Fonction*,null.hyp=*Paramètre*,alternative="two.sided",conf.level=0.95,R=100000)
  
  **Exemple**:
|   CalcY = function(x){
|       xyz = sum(x)/sd(x)
|       return(xyz)
|       }
|   R.var = boot.one.per(x,CalcY,null.hyp=8.2,alternative="two.sided",conf.level=0.95,R=100000) 
***

  5. Compléter l'estimation (obtenir l'intervalle de confiance) et/ou le test d'hypothèse à partir de la distribution d'échantillonnage obtenue
  
***
|   La fonction **print.boot.one()** affiche les détails de l'estimation et du test d'hypothèse
  
|       print.boot.one(*Variable*)
  
|   Exemple:
  
|       print.boot.one(R.var)

***

## Exercices

1. Le comité administratif de la Grand'Anse, désireux d'attirer l'attention du Ministre des Affaires Sociales, affirme qu'au niveau du département, les conditions de vie sont meilleures que dans l'ensemble du pays.  En effet, d'après un rapport de l'OCDE, l'indice CondxVie moyen serait égal à 5.2 au niveau national. Pour vérifier les dires du comité administratif, le Ministre demande à un chercheur de l'ISTEAH d'effectuer une recherche qui dispose du financement lui permettant un échantillon de 150 sujets. Les données recueillies par ce chercheur sont les suivantes:


2. Le Comité Administratif du département du Nord-Ouest (DEPARTEMENT=="7") pourrait-il affirmer que l'indice de Condition de Vie moyen dans leur département diffère de la moyenne nationale, telle que quantifiée par l'OCDE?

3. Dans Le Nouvelliste, un titre attire l'attention du Ministre de la Sécurité Publique: '**L'indice de criminalité est nettement plus faible dans le département du Centre que dans l'Artibonite**'. Si cette assertion était véridique, il pourrait déplacer du personnel du Centre vers l'Artibonite, de manière à y réduire la criminalité. Le Ministre vous offre un contrat de recherche afin de vérifier la situation.  Votre plan de recherche est donc:
  - estimer la moyenne de l'indice de criminalité en Artibonite. Votre budget de recherche vous permet un échantillon de 300 individus. Cette moyenne deviendra celle proposée dans l'hypothèse nulle, à l'étape suivante.
  - effectuer un test d'hypothèse afin de vérifier les faits exposés par Le Nouvelliste.
  - Faire rapport au Ministre sur l'état de la situation.
  
4. Pourrait-on croire que l'indice de condition de vie chez les femmes âgées de 60 ans et plus est inférieure à la moyenne nationale, établie à 6.5, selon l'OCDE? Un échantillon de 30 sujets est mis à votre disposition. 
  
5. Le candidat B aux élections présidentielles affire haut et fort qu'il obtiendra au moins 51% des votes, à l'échelle nationale, aux prochaines élections et qu'il sera le prochain président. Formez un échantillon de 300 électeurs, et vérifiez les affirmations du candidat.

6. Un groupe de députés chargé d'examiner l'intérêt de la population pour une nouvelle Constitution croit que le consensus en défaveur de tout changement constitutionnel est nettement plus important dans le département du Nord que dans le reste du pays, où on note un indice de consensus égal à 0.53. Utilisez un échantillon de 250 sujets en âge de voter pour vérifier cette assertion.


## Solutionnaire

>1. Le comité administratif de la Grand'Anse, désireux d'attirer l'attention du Ministre des Affaires Sociales, affirme qu'au niveau du département, les conditions de vie sont meilleures que dans l'ensemble du pays.  En effet, d'après un rapport de l'OCDE, l'indice CondxVie moyen serait égal à 5.2 au niveau national. Pour vérifier les dires du comité administratif, le Ministre demande à un chercheur de l'ISTEAH d'effectuer une recherche qui dispose du financement lui permettant un échantillon de 150 sujets. Les données recueillies par ce chercheur sont les suivantes:
  
```{r 06-Chap6-22}
subpop = subset(HaitiPop,(DEPARTEMENT=="3"),CondxVie)
idx = sample(1:length(subpop$CondxVie),150,replace=FALSE)
xech = subpop$CondxVie[idx]
```
Le chercheur de l'ISTEAH vous accorde un contrat pour l'analyse des données obtenues. Procédez à l'analyse et formulez les conclusions.

Les hypothèses statistiques:  

  - $H_0:\mu = 5.2$
  - $H_0:\mu > 5.2$

```{r 06-Chap6-23}
mu0 = 5.2
xb = mean(xech)
cat("Moyenne = ",xb)
#
library(wBoot)
b.means = boot.one.per(xech,mean,null.hyp=5.2,alternative="greater",conf.level=0.95,R=100000)
print.boot.one(b.means)
```
  
Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir une moyenne de CondxVie égale ou plus extrême que la moyenne observée ($\bar X=$ `r (round(b.means$Observed,3))`) est égale à `r b.means$p.value`. En conséquence, on `r if(b.means$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que la population du département de la Grand'Anse ont de meilleures conditions de vie que dans l'ensemble du pays, ce qui confirme les dires du comité administratif.

>2. Le Comité Administratif du département du Nord-Ouest (DEPARTEMENT=="7") pourrait-il affirmer que l'indice de Condition de Vie moyen dans leur département diffère de la moyenne nationale, telle que quantifiée par l'OCDE?
  
```{r 06-Chap6-24}
subpop = subset(HaitiPop,(DEPARTEMENT=="7"),CondxVie)
idx = sample(1:length(subpop$CondxVie),150,replace=FALSE)
xech = subpop$CondxVie[idx]
```

```{r 06-Chap6-25}
mu0 = 5.2
xb = mean(xech)
cat("Moyenne = ",xb)
```
Les hypothèses statistiques:  

  - $H_0:\mu = 5.2$
  - $H_0:\mu > 5.2$
  

```{r 06-Chap6-26}
library(wBoot)
b.means = boot.one.per(xech,mean,null.hyp=5.2,alternative="greater",conf.level=0.95,R=100000)
print.boot.one(b.means)
```
  
Les résultats indiquent que si $H_0$ est vraie, la probabilité d'obtenir une moyenne de CondxVie égale ou plus extrême que la moyenne observée ( $\bar X=$ `r (round(b.means$Observed,3))`) est égale à `r b.means$p.value`. En conséquence, on `r if(b.means$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que la population du département de la Grand'Anse a de meilleures conditions de vie que dans l'ensemble du pays, ce qui confirme les dires du comité administratif.


>3. Dans Le Nouvelliste, un titre attire l'attention du Ministre de la Sécurité Publique: '**L'indice de criminalité est nettement plus faible dans le département du Centre que dans l'Artibonite**'. Si cette assertion était véridique, il pourrait déplacer du personnel du Centre vers l'Artibonite, de manière à y réduire la criminalité. Le Ministre vous offre un contrat de recherche afin de vérifier la situation.  Votre plan de recherche est donc:
  - estimer la moyenne de l'indice de criminalité en Artibonite. Votre budget de recherche vous permet un échantillon de 300 individus. Cette moyenne deviendra celle proposée dans l'hypothèse nulle, à l'étape suivante.
  - effectuer un test d'hypothèse afin de vérifier les faits exposés par Le Nouvelliste.
  - Faire rapport au Ministre sur l'état de la situation.


```{r 06-Chap6-27}
# Estimation de la moyenne de CRIMIDX en Artibonite
subpop = subset(HaitiPop,((AGE>=15)&(DEPARTEMENT=="1")),CRIMIDX)
idx = sample(1:length(subpop$CRIMIDX),300,replace=FALSE)
xech = subpop$CRIMIDX[idx]
#
library(wBoot)
b.moy1 = boot.one.per(xech,mean,null.hyp=NULL,conf.level=0.95,R=100000)
print.boot.one(b.moy1)
mu0 = b.moy1$Observed
```

Les hypothèses statistiques:  

  - $H_0:\mu =$ `r round(mu0,3) `

  
  - $H_0:\mu <$ `r round(mu0,3) `
  
```{r 06-Chap6-28}

# CRIMIDX plus faible dans le département du Centre?
#
subpop = subset(HaitiPop,((AGE>=15)&(DEPARTEMENT=="2")),CRIMIDX)
idx = sample(1:length(subpop$CRIMIDX),300,replace=FALSE)
xech = subpop$CRIMIDX[idx]
#
b.moy2 = boot.one.per(xech,mean,null.hyp=mu0,conf.level=0.95,R=100000)
print.boot.one(b.moy2)

```
L'estimation de la moyenne de CRIMIDX en Artibonite prend la forme d'un intervalle de confiance:
  
C[`r round(b.moy1$Confidence.limits[1],3)` $< \mu <$ `r round(b.moy1$Confidence.limits[2],3)`]=95%
  
On peut donc considérer que $\mu_0 =$ `r round(b.moy1$Observed,3)`.  

Les résultats indiquent que si $H_0$ est vraie, la probabilité que la moyenne de CRIMIDX pour la population du département du Centre soit égale ou plus extrême que la moyenne observée ($\bar X=$ `r (round(b.moy2$Observed,3))`) est égale à `r b.moy2$p.value`. En conséquence, on `r if(b.moy2$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que la population du département du Centre a une propension à la criminalité plus faible que la population d'Artibonite, ce qui confirme les faits rapportés par Le Nouvelliste.
  
  
>4. Pourrait-on croire que l'indice de condition de vie chez les femmes âgées de 60 ans et plus est inférieure à la moyenne nationale, établie à 6.5, selon l'OCDE? Un échantillon de 30 sujets est mis à votre disposition. 
  
Les hypothèses statistiques:  

  - $H_0:\mu = 6.5$ 
  - $H_0:\mu < 6.5$ 

```{r 06-Chap6-29}
# Estimation de la moyenne de CondxVie chez les femmes âgées de 60+ ans
subpop = subset(HaitiPop,(AGE>=60),CondxVie)
idx = sample(1:length(subpop$CondxVie),30,replace=FALSE)
xech = subpop$CondxVie[idx]
#
library(wBoot)
b.moy = boot.one.per(xech,mean,null.hyp=6.5,alternative="less",conf.level=0.95,R=100000)
print.boot.one(b.moy)

```
Les résultats indiquent que si $H_0$ est vraie, la probabilité que la moyenne de CondxVie chez les femmes âgées de 60+ ans soit égale ou plus extrême que la moyenne observée ($\bar X=$ `r (round(b.moy$Observed,3))`) est égale à `r b.moy$p.value`. En conséquence, on `r if(b.moy$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure qu'il `r if(b.moy$p.value<=0.05){"y a évidence "}else{"n'y a pas d'évidence "}` indiquant que la population des femmes âgées de 60+ ans ont un indice de condition de vie inférieur à ce qu'on note au niveau national.


>5. Le candidat B aux élections présidentielles affire haut et fort qu'il obtiendra 51% des votes, à l'échelle nationale, aux prochaines élections et qu'il sera le prochain président. Formez un échantillon de 300 électeurs, et vérifiez les affirmations du candidat.

Les hypothèses statistiques:  

  - $H_0:\pi = 0.51$ 
  - $H_0:\pi < 0.51$ 

```{r 06-Chap6-30}
subpop = subset(HaitiPop,(AGE>=18),PrefPres)
idx = sample(1:length(subpop$PrefPres),300,replace=FALSE)
xech = subpop$PrefPres[idx]
#
# Calcul de la proportion d'intentions de votes pour le CandidatB (#2)
CalcProp = function(x){
  prop = mean(x==2)
  return(prop)
}
#
library(wBoot)
b.prop = boot.one.per(xech,CalcProp,null.hyp=0.51,alternative="less",conf.level=0.95,R=100000)
print.boot.one(b.prop)
```
Les résultats indiquent que si $H_0$ est vraie, la probabilité que la proportion des votes en faveur du Candidat B soit égale ou plus extrême que la proportion observée ($\pi=$ `r (round(b.prop$Observed,3))`) est égale à `r b.prop$p.value`. En conséquence, on `r if(b.prop$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que le Candidat B `r if(b.moy$p.value<=0.05){"n'a pas raison "}else{"a raison "}` lorsqu'il affirme qu'il pourrait être le prochain président.


>6. Un groupe de députés chargé d'examiner l'intérêt de la population pour une nouvelle Constitution croit que le consensus en défaveur de tout changement constitutionnel est nettement plus important dans le département du Nord que dans le reste du pays, où on note un indice de consensus égal à 0.53. Utilisez un échantillon de 250 sujets en âge de voter pour vérifier cette assertion.

  
Les hypothèses statistiques:  

  - $H_0:Consensus = 0.53$ 
  - $H_0:Consensus < 0.53$ 

```{r 06-Chap6-31}
subpop = subset(HaitiPop,(AGE>=18),OpinConst)
idx = sample(1:length(subpop$OpinConst),250,replace=FALSE)
xech = subpop$OpinConst[idx]
#
# Calcul du Consensus
library(agrmt)

Constabl = function(x){
  tbl = table(x)
  cons = consensus(tbl)
  return(cons)
}
#
library(wBoot)
b.cons = boot.one.per(xech,Constabl,null.hyp=0.53,alternative="less",conf.level=0.95,R=100000)
print.boot.one(b.cons)
```
Les résultats indiquent que si $H_0$ est vraie, la probabilité que le consensus à l'encontre de la nouvelle Constitution soit égal ou plus extrême que le consensus observée (Cons= `r (round(b.cons$Observed,3))`) est égale à `r b.cons$p.value`. En conséquence, on `r if(b.cons$p.value<=0.05){"rejette "}else{"ne rejette pas "}` $H_0$. On peut donc conclure que l'indice de consensus dans le département du Nord `r if(b.cons$p.value<=0.05){"indique une tendance à la dissension par rapport au projet de Constitution. "}else{"n'est pas différent de la population générale. "}` 



<!--chapter:end:06-Chap6.Rmd-->

---
title: "MAT-104: Probabilités et Statistiques"

author: "Daniel Coulombe"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    highlight: pygments
    theme: spacelab
    fig_width: 5.6
    fig_height: 4
---

```{r 07-Chap7-1, include = FALSE}
knitr::opts_chunk$set(out.height = "\\textheight",  out.width = "\\textwidth")
library(formatR)
```

```{r 07-Chap7-2, include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

```{r 07-Chap7-3,message=FALSE,echo=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(formatR,data.table,wBoot,MASS)
```
```{r 07-Chap7-4, include=FALSE}
# Importer HaitiPop dans l'environnement
# if(!exists(HaitiPop)){
 load("I:/MAT104/HaitiPop_Env.RData")
# }
```

```{r 07-Chap7-5, include=FALSE}
RandomUnif = function(n=50,a=c(10,20),b=c(25,50)){
#a = c(10,20)  # étendue de la limite inférieure (population)
#b = c(25,50)  # étendue de la limite supérieure (population)
liminf = runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup = runif(1,b[1],b[2]) # limite supérieure (Pop)
x = runif(n,liminf,limsup)
out = list(x,liminf,limsup)
return(out)
}
```
# Inférence Statistique: deux échantillons

## Deux échantillons: indépendants ou dépendants?

Deux situations se présentent, lorsqu'on examine les différences pouvant exister entre deux (ou plusieurs) échantillons:  
  
  1. D'une part, les observations faites sur ces échantillons peuvent être indépendantes non seulement à l'intérieur de chacun des échantillons, mais également **entre** les échantillons. En d'autres termes, les observations faites sur un échantillon ne nous apprennent absolument rien concernant les observations faites sur l'autre (ou les autres) échantillon. Les données sont indépendantes, n'ont aucun lien entre elles. Par exemple, lorsqu'on compare la performance d'une classe de mathématiques enseignée par le professeur X à celle d'une classe enseignée par le professeur Y, on peut considérer que nous sommes en présence d'échantillons indépendants.  
    
  2. D'autre part, les observations faites sur un échantillon peuvent être liées. ou associées à celles faites sur un ou plusieurs autres échantillons. Ce lien se traduit par une corrélation, ou une association, entre les données: en connaissant une observation provenant du premier échantillon, on peut apprendre quelque chose sur l'observation qui lui est appariée dans le second échantillon. Par exemple, supposons un échantillon dont la performance des sujets dans une tâche donnée sont évalués **avant** et **après** une formation visant à améliorer leur efficacité. Dans un tel cas, ce sont les mêmes sujets que l'on évalue à deux (ou plusieurs) reprises, de sorte qu'une portion importante des facteurs susceptibles d'affecter la performance est *égalisée* entre les deux évaluations. En conséquence, les données sont corrélées, et l'analyse doit tenir compte de ce fait. 


## Estimation: deux échantillons indépendants

Les concepts définis et évoqués pour l'estimation de paramètres à partir d'un échantillon unique sont aisément applicables aux cas impliquant deux échantillons. On pourra s'intéresser, par exemple, à la différence pouvant exister entre les moyennes, les médianes, les variances, les proportions ou toute autre quantité définissant les caractéristiques de deux populations. Le résultat de l'opération sera un intervalle susceptible de capturer la différence entre les paramètres de deux populations, avec un certain niveau de confiance.On cherchera à répondre à des questions telles que:  

**Quel intervalle pourrait capturer la différence entre la performance des garçons et celle des filles en mathématiques, au niveau fondamental?**

Pour obtenir cet intervalle, il sera nécessaire de spécifier la distribution d'échantillonnage des différences entre les paramètres. On pourra tirer un échantillon de la première population et calculer sa moyenne; on fera de même pour un échantillon tiré de la seconde population, pour finalement obtenir la différence entre les deux moyennes échantillonnales. En répétant l'exercice un grand nombre de fois, on pourra obtenir la distribution d'échantillonnage essentielle au processus d'estimation. C'est l'approche que nous avons utilisée pour l'estimation d'un paramètre d'une seule population.


### Intervalle de confiance pour groupes indépendants: ré-échantillonnage


Supposons deux échantillons indépendants de $n_1 =$ 25 et $n_2 =$ 35 observations, générés à l'aide de la fonction **RandomUnif()**. Pour les besoins de la démonstration, les moyennes des populations d'où sont tirés les échantillons sont affichées. Mais rappelons que dans le monde réel, ces paramètres sont généralement inconnus.


```{r 07-Chap7-6, eval=FALSE}
# Générateur de nombre aléatoires, distribution uniforme
# Limites de distribution aléatoires
# Sortie: vecteur de données, limites de la distribution (pour référence!)
RandomUnif = function(n=50,a=c(10,20),b=c(25,50)){
#a = c(10,20)  # étendue de la limite inférieure (population)
#b = c(25,50)  # étendue de la limite supérieure (population)
liminf = runif(1,a[1],a[2]) # limite inférieure (Pop)
limsup = runif(1,b[1],b[2]) # limite supérieure (Pop)
x = runif(n,liminf,limsup)
out = list(x,liminf,limsup)
return(out)
}
```


```{r 07-Chap7-7}
x1 = RandomUnif(n=25)
xb1 = (x1[[3]]-x1[[2]])/2
cat("Moyenne de la Population 1: ",xb1)
x2 = RandomUnif(n=35)
xb2 = (x2[[3]]-x2[[2]])/2
cat("Moyenne de la Population 2: ",xb2)
cat("Différence (μ1 - μ2) = ",xb1-xb2)
x1 = x1[[1]] # Extraction de x1 de la liste
x2 = x2[[1]] # Extraction de x2 de la liste
```
Quelle pourrait être la différence entre les moyennes des populations d'où sont tirés ces échantillons (en supposant qu'on ignore leurs valeurs!)? On peut calculer la différence entre les moyennes observées, mais le résultat ne peut être considéré que comme un reflet de la différence réelle existant entre les moyennes des populations:

```{r 07-Chap7-8}
cat("Différence de moyennes = ",mean(x1)-mean(x2))
```
Procédons par ré-échantillonnage:

```{r 07-Chap7-9}
nrep = 10000
moydiff = replicate(nrep,mean(sample(x1,length(x1),replace=TRUE))-mean(sample(x2,length(x2),replace=TRUE)))
hist(moydiff,main="Distribution d'échantillonnage\n des différences de moyennes",xlab="Différences de moyennes",breaks="FD")
lims = quantile(moydiff,c(0.025,0.975))
abline(v=lims,col="red",lwd=3)

```
De cette distribution, on trouve:

$C[$ `r round(lims[1],3)` $< \mu <$ `r round(lims[2],3)`$]=95\%$

En répétant l'expérience un nombre infini de fois, 95% des intervalles de confiance pour différences de moyennes indépendantes captureraient la véritable différence des moyennes des populations.


La fonction **boot.two.per()** de la librairie **wBoot** nous permet d'obtenir des résultats similaires:

```{r 07-Chap7-10}
library(wBoot)
b.ind2 = boot.two.per(x1, x2, mean, stacked = FALSE, variable = NULL,null.hyp = NULL, alternative = "two.sided",conf.level = 0.95, type = NULL, R = 10000)
print.boot.two(b.ind2)
```

#### Exemple 1

Estimons la différence qui pourrait exister entre la performance moyenne des hommes et des femmes, au test de HabilCog, dans le département des Nippes, à l'aide d'un échantillon de 35 hommes et de 40 femmes.

```{r 07-Chap7-11}
# Création des sous-populations
subpop = subset(HaitiPop,(DEPARTEMENT=="4"),c(GENRE,HabilCog))
subpopH = subset(subpop,GENRE=="M",HabilCog)
subpopF = subset(subpop,GENRE=="F",HabilCog)
# Création des échantillons
nH = 35
nF = 40
xechH = subpopH$HabilCog[sample(1:nrow(subpopH),nH,replace=FALSE)]
xechF = subpopF$HabilCog[sample(1:nrow(subpopF),nF,replace=FALSE)]
cat("Moyenne (Hommes) = ",mean(xechH))
cat("Moyenne (Femmes) = ",mean(xechF))
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechH,xechF,mean,conf.level=0.95,R=100000,stacked=FALSE)
b.Diff

```
Les résultats indiquent que si on répétait l'expérience un nombre infini de fois, la véritable différence entre les moyennes des Hommes et des Femmes au test d'Habileté Cognitive, au niveau de la population, serait capturée par l'intervalle `r b.Diff$Confidence.interval` 19 fois sur 20 $[(1-\alpha)=0.95]$. Notons que la valeur '0' (indiquant l'égalité entre les performances des hommes et des femmes) fait partie de cet intervalle. On peut donc conclure que les performance des hommes et des femmes sont probablement égales.

#### Exemple 2

Estimons la différence qui pourrait exister entre l'indice de criminalité moyen chez les hommes vivant en Artibonite et ceux vivant dans le département du Centre. On peut disposer d'un échantillon de 50 sujets en Artibonite, et de 45 sujets dans le département du Centre.

```{r 07-Chap7-12}
# Création des sous-populations
subpopA = subset(HaitiPop,((DEPARTEMENT=="1")&(GENRE=="M")&(AGE>=15)),CRIMIDX)
subpopC = subset(HaitiPop,((DEPARTEMENT=="2")&(GENRE=="M")&(AGE>=15)),CRIMIDX)

# Création des échantillons
nA = 50
nC = 45
xechA = subpopA$CRIMIDX[sample(1:nrow(subpopA),nA,replace=FALSE)]
xechC = subpopC$CRIMIDX[sample(1:nrow(subpopC),nC,replace=FALSE)]
cat("Moyenne (Artibonite) = ",mean(xechA))
cat("Moyenne (Centre)     = ",mean(xechC))
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechA,xechC,mean,conf.level=0.95,R=100000,stacked=FALSE)
print.boot.two(b.Diff)

```

Les résultats indiquent que si on répétait l'expérience un nombre infini de fois, la véritable différence entre les moyennes des Hommes d'Artibonite et ceux du département du Centre en ce qui concerne l'indice de criminalité serait capturée par l'intervalle `r b.Diff$Confidence.interval` 19 fois sur 20 $[(1-\alpha)=0.95]$. Notons que la valeur '0' (indiquant l'égalité entre les performances des hommes et des femmes) ne fait pas partie de cet intervalle. On peut donc conclure qu'il pourrait y avoir une différence entre les moyennes des deux groupes. 

#### Exemple 3

Estimons la différence dans la proportion des adultes accordant leur confiance au Candidat A des départements du Nord et de l'Ouest. On dispose d'un échantillon de 250 sujets dans le département du Nord, et de 325 sujets dans de département de l'Ouest.

```{r 07-Chap7-13}
# Création des sous-populations
subpopN = subset(HaitiPop,((DEPARTEMENT=="5")&(AGE>=18)),PrefPres)
subpopO = subset(HaitiPop,((DEPARTEMENT=="8")&(AGE>=18)),PrefPres)

# Création des échantillons
nN = 250
nO = 325
xechN = subpopN$PrefPres[sample(1:nrow(subpopN),nN,replace=FALSE)]==1
xechO = subpopO$PrefPres[sample(1:nrow(subpopO),nO,replace=FALSE)]==1
cat("Proportion (Nord)  = ",mean(xechN))
cat("Proportion (Ouest) = ",mean(xechO))
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechN,xechO,mean,conf.level=0.95,R=100000,stacked=FALSE)
print.boot.two(b.Diff)
```
Les résultats indiquent que si on répétait l'expérience un nombre infini de fois, la véritable différence entre les proportions des individus favorisant le Candidat A aux prochaines élections présidentielles des départements du Nord et de l'Ouest serait capturée par l'intervalle `r b.Diff$Confidence.interval` 19 fois sur 20 $[(1-\alpha)=0.95]$. Notons que la valeur '0' (indiquant l'égalité des proportions entre les deux régions du pays) ne fait pas partie de cet intervalle. On peut donc conclure qu'il pourrait y avoir une différence entre les proportions de voteurs favorisant le Candidat A, dans les deux départements. 

#### Exemple 4

Estimons la différence dans le niveau de consensus concernant le projet d'une nouvelle Constitution au pays, au niveau des populations des régions nordiques (départements du Nord-Est, du Nord et du Nord-Ouest) et l'ensemble des départements du Centre et de l'Ouest. On dispose d'un échantillon de 250 sujets dans les départements du Nord, et de 325 sujets dans les départements du Centre et de l'Ouest.

```{r 07-Chap7-14}
library(agrmt)

# Fonction cons()
cons = function(x){
  tbl = table(x)
  cns = consensus(tbl)
  return(cns)
}

# Création des sous-populations
subpopN = subset(HaitiPop,((DEPARTEMENT %in% c("5","6","7"))&(AGE>=18)),OpinConst)
subpopO = subset(HaitiPop,((DEPARTEMENT %in% c("2","8"))&(AGE>=18)),OpinConst)

# Création des échantillons
nN = 250
nO = 325
xechN = subpopN$OpinConst[sample(1:nrow(subpopN),nN,replace=FALSE)]==1
xechO = subpopO$OpinConst[sample(1:nrow(subpopO),nO,replace=FALSE)]==1
cat("Consensus (Nord)  = ",cons(xechN))
cat("Consensus (Ouest) = ",cons(xechO))
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechN,xechO,cons,conf.level=0.95,R=100000,stacked=FALSE)
print.boot.two(b.Diff)
```
Les résultats indiquent que si on répétait l'expérience un nombre infini de fois, la véritable différence entre les niveaux de consensus concernant le projet de nouvelle Constitution au niveau des départements nordiques et des départements l'Ouest et du Centre serait capturée par l'intervalle `r b.Diff$Confidence.interval` 19 fois sur 20 $[(1-\alpha)=0.95]$. Notons que la valeur '0' (indiquant l'égalité des niveaux de consensus entre les deux régions du pays) ne fait pas partie de cet intervalle. On peut donc conclure qu'il pourrait y avoir une différence entre les niveaux de consensus dans les deux régions du pays. Dans le Nord, il y a un fort consensus en faveur du projet, alors que dans l'Ouest et le Centre il y a une très forte dissension.


## Test de d'hypothèse pour groupes indépendants

Dans le cas où on s'intéresse à deux populations, il existe une autre approche pour effectuer les comparaisons entre les paramètres de ces populations: le test de permutations. On utilise ce test pour générer une distribution sous $H_0$ pour une statistique mesurant une association entre deux variables, ou la différence entre deux (ou plusieurs) groupes. 
  
Supposons deux échantillons indépendants  tirés d'une population. La répartition des observations dans les deux groupes peut être considérée comme un parmi  un très grand nombre d'arrangements possibles. Plutôt que de ré-échantillonner chacun des échantillons de manière indépendante, on peut combiner les deux groupes de données, et les re-distribuer aléatoirement pour obtenir un nouvel arrangement. En répétant un grand nombre de fois ce processus, calculant la différence entre les estimateurs à chaque fois, on obtient la distribution d'échantillonnage nécessaire.


Considérons les données suivantes, analogues au tout premier exemple de cette section:

```{r 07-Chap7-15}
x1 = RandomUnif(n=25)
xb1 = (x1[[3]]-x1[[2]])/2
cat("Moyenne de la Population 1: ",xb1)
x2 = RandomUnif(n=35)
xb2 = (x2[[3]]-x2[[2]])/2
cat("Moyenne de la Population 2: ",xb2)
cat("Différence (μ1 - μ2) = ",xb1-xb2)
x1 = x1[[1]] # Extraction de x1 de la liste
x2 = x2[[1]] # Extraction de x2 de la liste
moyDiff = mean(x1)-mean(x2)

```

Encore une fois, les moyennes des deux populations sont affichées, mais uniquement dans un but de comparaisons avec les résultats des analyses à venir. Dans le monde réel, ces données sont inconnues et sont ce que l'on cherche à estimer.

Un test de permutations se déroule suivant les étapes suivantes:
  
  1. Calcul de la différence de la statistique échantillonnale, entre les deux groupes: on cherche la probabilité d'obtenir une telle valeur, ou une valeur plus extrême...
  2. Combinaison des deux groupes pour former un seul ensemble, composé de $N = n_1 + n_2$ observations
  3. Permutation de l'ensemble
  4. Répartition des observations dans les deux groupes, en respectant la taille de chacun
  5. Calcul des statistiques et de leur différence, que l'on collige
  6. Retour à (2) et répéter un grand nombre de fois.
  7. La distribution des différences observées est la distribution d'échantillonnage dont on a besoin pour compléter le test.
  8. Évaluation de la probabilité associée à la différence notée en (1)
  
Le code R suivant permet d'effectuer ces étapes:

```{r 07-Chap7-16}
N = (n1 = length(x1))+(n2 = length(x2)) # Nombre total d'observations
xt = c(x1,x2)                           # Observations rassemblées
nsig = .05				                      # Niveau de confiance
ns=100000				                        # Nombre de permutations à effectuer
xbd=rep(NA,ns) 			                    # Initialisation du vecteur des différences
for(i in 1:ns){			                    
	rd = sample(xt,N,replace=FALSE)       # Permutations
	xbd[i] = mean(rd[1:n1])-mean(rd[(n1+1):N]) # Répartition dans les groupes et calcul de la différence
}	
p = mean(xbd>=moyDiff)
cat("Différence observée = ",moyDiff,"   p = ",p)
hist(xbd,main="Distribution d'échantillonnage\n des différences de moyennes",xlab="Différences de moyennes")
lims = quantile(xbd,c(0.025,0.975))
abline(v=lims,col="red",lwd=3)
abline(v=moyDiff,col="green",lwd=3)
plot(density(xbd),main="Courbe de densité des Moyennes")
abline(v=lims,col="red",lwd=3)
abline(v=moyDiff,col="green",lwd=3)
```
Les résultats indiquent que la différence observée entre les moyennes des deux échantillons est un événement `r if(p<=0.05){"improbable"}else{"probable"}` . Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` .

Analysons les mêmes données à l'aide de la fonction **boot.two.per()** de la librairie **wBoot**:

```{r 07-Chap7-17}
library(wBoot)
b.moyDiff = boot.two.per(x1,x2,mean,stacked=FALSE,null.hyp=0,alternative="two.sided",conf.level=0.95,R=100000)
print.boot.two(b.moyDiff)
```
Ces résultats sont pratiquement identiques à ceux obtenus précédemment.

### Exemple 5: Test d'hypothèse pour deux moyennes indépendantes
  
  Peut-on affirmer que  l'indice de Condition de Vie (CondxVie) moyen en Artibonite est différent de ce qu'on trouve dans le département du Centre? Appliquons le test de permutations afin d'élucider la question. Nous disposons d'un échantillon de 25 sujets vivant en Artibonite et d'un échantillon de 30 sujets provenant du département du Centre:
  

```{r 07-Chap7-18}

# Création des sous-populations
subpop1 = subset(HaitiPop,(DEPARTEMENT=="1"),CondxVie)
subpop2 = subset(HaitiPop,(DEPARTEMENT=="2"),CondxVie)

# Création des échantillons
nA = 25
nC = 30
xechA = subpop1$CondxVie[sample(1:nrow(subpop1),nA,replace=FALSE)]
xechC = subpop2$CondxVie[sample(1:nrow(subpop2),nC,replace=FALSE)]
MxechA = mean(xechA)
MxechC = mean(xechC)
cat("Moyenne (Artibonite)  = ",MxechA)
cat("Moyenne (Centre)      = ",MxechC)
moyDiff = MxechA - MxechC

```
Le test de permutations permet le test d'hypothèse approprié à la situation:

```{r 07-Chap7-19}
N = (nA = length(xechA))+(nC = length(xechC)) # Nombre total d'observations
xt = c(xechA,xechC)                           # Observations rassemblées
nsig = .05				                      # Niveau de confiance
ns=100000				                        # Nombre de permutations à effectuer
xbd=rep(NA,ns) 			                    # Initialisation du vecteur des différences
for(i in 1:ns){			                    
	rd = sample(xt,N,replace=FALSE)       # Permutations
	xbd[i] = mean(rd[1:n1])-mean(rd[(n1+1):N]) # Répartition dans les groupes et calcul de la différence
}	
p = mean(xbd>=moyDiff)
cat("Différence observée = ",moyDiff,"   p = ",p)
hist(xbd,main="Distribution d'échantillonnage\n des différences de moyennes",xlab="Différences de moyennes")
lims = quantile(xbd,c(0.025,0.975))
abline(v=lims,col="red",lwd=3)
abline(v=moyDiff,col="green",lwd=3)
plot(density(xbd),main="Courbe de densité des Moyennes")
abline(v=lims,col="red",lwd=3)
abline(v=moyDiff,col="green",lwd=3)
```
Les résultats indiquent que la différence observée entre les moyennes des deux échantillons est un événement `r if(p<=0.05){"improbable"}else{"probable"}` . Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` .

En utilisant la fonction **boot.two,per**, on trouve:
```{r 07-Chap7-20}
library(wBoot)
b.moyDiff = boot.two.per(xechA,xechC,mean,stacked=FALSE,null.hyp=0,alternative="two.sided",conf.level=0.95,R=100000)
print.boot.two(b.moyDiff)
```
  
Les résultats sont analogues et conduisent aux mêmes conclusions.

### Exemple 6: Test d'hypothèse pour deux variances indépendantes

Certaines questions de recherche nécessitent la comparaison de variances. Est-ce que l'indice d'habileté cognitive (HabilCog) est plus homogène  dans le département de l'Ouest qu'en Artibonite? Est-ce que l'indice de condition de vie est plus variable chez les femmes que chez les hommes?

La comparaison de deux variances ne repose pas sur leur différence, mais plutôt sur leur rapport, que l'on nomme F:

$$F = \frac{s_1^2}{s_2^2}$$
Ce rapport prend toujours une valeur positive, puisqu'une variance ne peut qu'être positive. Si les deux variances sont égales, F = 1.  De plus, en s'assurant que le numérateur du rapport est occupé par la plus grande variance, F sera toujours plus grand ou égal à 1, et seule l'extrémité droite de la distribution d'échantillonnage de F sera nécessaire. 
  
À titre d'exemple, comparons la variance de l'indice d'habileté cognitive (HabilCog) entre les résidents du département de l'Ouest et ceux d'Artibonite.

  a. On forme un premier échantillon de 30 sujets en Artibonite, et un échantillon de 25 sujets dans le département de l'Ouest.  
  b. On calcule la variance pour chacun des échantillons  
  c. On obtient le rapport F en divisant la plus grande variance par la plus petite.  
  d. En utilisant le test de permutations, on obtient un grand nombre de rapports F, desquels on obtient la distribution d'échantillonnage de ce rapport.  
  e. La probabilité d'obtenir le rapport F observée initialement pour les échantillons ou un rapport plus extrême est évaluée. Rappelons que puisque la variance la plus élevée occupe le numérateur, on ne s'occupera que de l'extrémité supérieure de la distribution.  
  f. Une décision est prise quant au rejet ou non de $H_0$.
  
Le code R suivant effectue des étapes successives:

```{r 07-Chap7-21}

# Création des sous-populations
subpop1 = subset(HaitiPop,(DEPARTEMENT=="8"),HabilCog)
subpop2 = subset(HaitiPop,(DEPARTEMENT=="1"),HabilCog)

# Création des échantillons
nO = 25
nA = 30
xechO = subpop1$HabilCog[sample(1:nrow(subpop1),nO,replace=FALSE)]
xechA = subpop2$HabilCog[sample(1:nrow(subpop2),nA,replace=FALSE)]
VxechO = var(xechO)
VxechA = var(xechA)
cat("Variance (Ouest)      = ",VxechO)
cat("Variance (Artibonite) = ",VxechA)
RapF = max(VxechO,VxechA)/min(VxechO,VxechA)

```
Les hypothèses statistiques sont:

$H_0:\frac{\sigma_{Ouest}^2}{\sigma_{Artibonite}^2}=1$ 
  
$H_1:\frac{\sigma_{Ouest}^2}{\sigma_{Artibonite}^2}>1$

Pour obtenir la distribution d'échantillonnage du rapport F, on peut utiliser soit le ré-échantillonnage, soit le test de permutation.

  1. **Ré-échantillonnage**

```{r 07-Chap7-22}
#
# Ré-échantillonnage et distribution d'échantillonnage
#

nsig = .05				                      # Niveau de confiance
ns=100000				                        # Nombre de permutations à effectuer
xbd=rep(NA,ns) 			                    # Initialisation du vecteur des différences
for(i in 1:ns){			                    
  x1 = sample(xechO,nO,replace=TRUE) 
  x2 = sample(xechA,nA,replace=TRUE) 

  v1 = var(x1)
  v2 = var(x2)
  xbd[i] = max(v1,v2)/min(v1,v2)
}	
lims = quantile(xbd,c(0.025,0.975))
p = mean(xbd>=RapF)
cat("Rapport F observée = ",RapF,"   p = ",p)
hist(xbd,main="Distribution d'échantillonnage\n des Rapports F",xlab="Rappors F")

abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)
plot(density(xbd),main="Courbe de densité des Rapports F")
abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)

```
  
  2. **Permutations**

```{r 07-Chap7-23}
# Test de permutation
#
N = (n1 = length(xechO))+(n2 = length(xechA)) # Nombre total d'observations
xt = c(xechO,xechA)                           # Observations rassemblées
nsig = .05				                      # Niveau de confiance
ns=100000				                        # Nombre de permutations à effectuer
xbd=rep(NA,ns) 			                    # Initialisation du vecteur des différences
for(i in 1:ns){			                    
	rd = sample(xt,N,replace=FALSE)       # Permutations
	v1 = var(rd[1:n1])
	v2 = var(rd[(n1+1):N])
	xbd[i] = max(v1,v2)/min(v1,v2)
}	

lims = quantile(xbd,c(0.025,0.975))
p = mean(xbd>=RapF)
cat("Rapport F observée = ",RapF,"   p = ",p)
hist(xbd,main="Distribution d'échantillonnage\n des Rapports F",xlab="Rappors F")

abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)
plot(density(xbd),main="Courbe de densité des Rapports F")
abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)
```

Les résultats indiquent que la différence observée entre les variances de l'indice HabilCog dans le département de l'Ouest et l'Artibonite est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r p` ). Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que la variance de HabilCog est plus faible dans le département de l'Ouest qu'en Artibonite. 

Pour les fins d'illustration, examinons une situation dans laquelle le rapport F est significativement supérieur à 1. On sait que la multiplication d'un ensemble de données par une constante augmente la variance de cet ensemble par un facteur égal au carré de la constante. Reprenons l'expérience précédente, après avoir multiplié des données de l'échantillon provenant du département de l'Ouest par 2:

```{r 07-Chap7-24}
# Création des échantillons
nO = 25
nA = 30
xechO = subpop1$HabilCog[sample(1:nrow(subpop1),nO,replace=FALSE)]*2
xechA = subpop2$HabilCog[sample(1:nrow(subpop2),nA,replace=FALSE)]
VxechO = var(xechO)
VxechA = var(xechA)
cat("Variance (Ouest)      = ",VxechO)
cat("Variance (Artibonite) = ",VxechA)
RapF = max(VxechO,VxechA)/min(VxechO,VxechA)

# Test de permutation
#
N = (n1 = length(xechO))+(n2 = length(xechA)) # Nombre total d'observations
xt = c(xechO,xechA)                           # Observations rassemblées
nsig = .05				                      # Niveau de confiance
ns=100000				                        # Nombre de permutations à effectuer
xbd=rep(NA,ns) 			                    # Initialisation du vecteur des différences
for(i in 1:ns){			                    
	rd = sample(xt,N,replace=FALSE)       # Permutations
	v1 = var(rd[1:n1])
	v2 = var(rd[(n1+1):N])
	xbd[i] = max(v1,v2)/min(v1,v2)
}	

lims = quantile(xbd,0.95)
p = mean(xbd>=RapF)
cat("Rapport F observée = ",RapF,"   p = ",p)
hist(xbd,main="Distribution d'échantillonnage\n des Rapports F",xlab="Rappors F")
abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)
plot(density(xbd),main="Courbe de densité des Rapports F")
abline(v=lims,col="red",lwd=3)
abline(v=RapF,col="green",lwd=3)


```
Les résultats indiquent que la différence observée entre les variances de l'indice HabilCog dans le département de l'Ouest et l'Artibonite est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r p` ). Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que la variance de HabilCog est plus faible dans le département de l'Ouest qu'en Artibonite.

### Exemple 7: Test d'hypothèse pour deux proportions indépendantes
  
Peut-on affirmer que la proportion des adultes accordant leur confiance au Candidat A dans le département du Nord est supérieure à cette proportion dans le département de l'Ouest? On dispose d'un échantillon de 250 sujets dans le département du Nord, et de 325 sujets dans de département de l'Ouest pour examiner la question.

```{r 07-Chap7-25}
# Création des sous-populations
subpopN = subset(HaitiPop,((DEPARTEMENT=="5")&(AGE>=18)),PrefPres)
subpopO = subset(HaitiPop,((DEPARTEMENT=="8")&(AGE>=18)),PrefPres)

# Création des échantillons
nN = 250
nO = 325
xechN = subpopN$PrefPres[sample(1:nrow(subpopN),nN,replace=FALSE)]==1
xechO = subpopO$PrefPres[sample(1:nrow(subpopO),nO,replace=FALSE)]==1
cat("Proportion (Nord)  = ",mean(xechN))
cat("Proportion (Ouest) = ",mean(xechO))
```
Les hypothèses statistiques sont:

$H_0:\pi_{Nord}-\pi_{Ouest}=0$
  
$H_0:\pi_{Nord}-\pi_{Ouest}\gt0$

On note une différence entre les proportions observées pour les échantillons. Mais est-ce que cette différence réflète un réalité au niveau des populations d'où proviennent ces échantillons, ou peut-on croire que la différence observée est le simple fait des fluctuations d'échantillonnage?


```{r 07-Chap7-26}
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechN,xechO,mean,null.hyp=0,alternative="greater",conf.level=0.95,R=100000,stacked=FALSE)
print.boot.two(b.Diff)
p = b.Diff$p.value

```
Les résultats indiquent que la différence observée entre les proportions de préférence pour le Candidat A dans le département du Nord et celui de l'Ouest est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r b.Diff$p.value` ) . Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que la proportion d'appui au Candidat A est plus élevée dans le département du Nord que dans celui de l'Ouest.


### Exemple 8: Test d'hypothèse pour deux consensus

Est-ce que le niveau de consensus concernant le projet d'une nouvelle Constitution au pays est différent lorsqu'on considère les populations des régions nordiques (départements du Nord-Est, du Nord et du Nord-Ouest) et l'ensemble des départements du Centre et de l'Ouest? On dispose d'un échantillon de 250 sujets dans les départements du Nord, et de 325 sujets dans les départements du Centre et de l'Ouest pour examiner cette question.

```{r 07-Chap7-27}
library(agrmt)

# Fonction cons()
cons = function(x){
  tbl = table(x)
  cns = consensus(tbl)
  return(cns)
}

# Création des sous-populations
subpopN = subset(HaitiPop,((DEPARTEMENT %in% c("5","6","7"))&(AGE>=18)),OpinConst)
subpopO = subset(HaitiPop,((DEPARTEMENT %in% c("2","8"))&(AGE>=18)),OpinConst)

# Création des échantillons
nN = 250
nO = 325
xechN = subpopN$OpinConst[sample(1:nrow(subpopN),nN,replace=FALSE)]==1
xechO = subpopO$OpinConst[sample(1:nrow(subpopO),nO,replace=FALSE)]==1
cat("Consensus (Nord)  = ",cons(xechN))
cat("Consensus (Ouest) = ",cons(xechO))
```
On note une différence entre les consensus observés au niveau des échantillons. Mais est-ce que cette différence réflète un réalité au niveau des populations d'où proviennent ces échantillons, ou peut-on croire que la différence observée est le simple fait des fluctuations d'échantillonnage?

Les hypothèses statistiques sont:

$H_0:Cns_{Nord}-Cns_{Ouest}= 0$  
  
$H_0:Cns_{Nord}-Cns_{Ouest}\neq 0$


```{r 07-Chap7-28}
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechN,xechO,cons,null.hyp=0,alternative="two.sided",conf.level=0.95,R=100000,stacked=FALSE)
print.boot.two(b.Diff)
p = b.Diff$p.value
```
Les résultats indiquent que la différence observée entre les niveaux de consensus diffèrent entre les régions nordiques et le groupe formé des départements du Centre et de l'Ouest est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r b.Diff$p.value` ) . Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que le niveau de consensus dans la population nordique diffère de celui de l'ensemble composé des départements du Centre et de l'Ouest.

### Exemple 9: Test d'hypothèse pour Rapport de Variation

On se souviendra qu'une mesure de dispersion pour variables nominales est donnée par la proportion des observations qui ne font pas partie de la catégorie modale. Il s'agit d'une quantité qui, étant obtenue d'un échantillon, peut servir d'estimateur du paramètre correspondant de la population.

Posons la question de recherche suivante: '**Est-ce que le rapport de variation de la distribution des appuis aux différents candidats à la présidence diffère entre les trois départements du nord et les quatre départements du sud?**'

Le code R suivant permettra, dans un premier temps, d'obtenir les échantillons nécessaires, et ensuite, vérifier l'hypothèse d'égalité dans le RV des deux régions du pays. Il nous est permis d'utiliser un échantillon de 400 sujets dans les régions du Nord, et de 350 sujets dans les régions du sud:

```{r 07-Chap7-29}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

RV = function(x){  
  mod = getmode(x)
  fm = sum(x==mod)
  rv = 1-fm/length(x)

}

rvDiff = function(x,y){ # Différence de RV 
  rv1 = RV(x)
  rv2 = RV(y)
  DiffRV = rv1 - rv2
  return(DiffRV)
}

# Création des sous-populations
subpopN = subset(HaitiPop,((DEPARTEMENT %in% c("5","6","7"))&(AGE>=18)),PrefPres)
subpopS = subset(HaitiPop,((DEPARTEMENT %in% c("3","4","9","10"))&(AGE>=18)),PrefPres)


# Création des échantillons
nN = 400
nS = 350
xechN = subpopN$PrefPres[sample(1:nrow(subpopN),nN,replace=FALSE)]
xechS = subpopS$PrefPres[sample(1:nrow(subpopS),nS,replace=FALSE)]
RVech = rvDiff(xechN,xechS)
cat("RV. Diff = ",RVech)

```
Les hypothèses statistiques sont:

$H_0:RV_{Nord}-RV_{Sud}=0$
  
$H_0:RV_{Nord}-RV_{Sud}\neq 0$

On note une différence entre les Rapports de Variation au niveau des échantillons. Mais est-ce que cette différence réflète un réalité au niveau des populations d'où proviennent ces échantillons, ou peut-on croire que la différence observée est le simple fait des fluctuations d'échantillonnage?


```{r 07-Chap7-30}
#
# Ré-échantillonnage et affichage des résultats
#
  num =  100000      # Nombre de ré-échantillonnage
  b.RV = vector()
  options(warn=2)
  for(i in 1:num){
    x.b = sample(xechN,length(xechN),replace=TRUE)
    y.b = sample(xechS,length(xechS),replace=TRUE)
    b.RV[i] = as.numeric(rvDiff(x.b,y.b))
  }
  std.err <- sd(b.RV)

cat("Moyenne des RVs         = ",mean(b.RV))
cat("Erreur-Standard des RVs = ",std.err)

# Détermination de l'intervalle de confiance
nc = 0.95
intc=quantile(b.RV,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
cat("C[",round(intc[1],3),"< RV < ",round(intc[2],3),"] = ",100*nc,"%")

# Distribution des RVs
  gr1=hist(b.RV,breaks="FD",plot=FALSE)
  cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
  plot(gr1,main="Distribution des RVs\n (Ré-échantillonnage)",xlab="Rapports de Variation",     col=c("red", "cadetblue1", "red")[cuts])
  abline(v=RVech,col="red",lwd=3,lty=2)
  txt1=paste("Moyenne    = ",round(mean(b.RV),3))
  txt2=paste("Erreur Std = ",round(std.err,3))
  legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
  txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",", round(intc[2],3),"]")
  legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

# Conclusion:
  
  cat("Pour un test unilatéral droit:")
    pval1<-mean(b.RV>=RVech)
    cat("p = ",pval1)
    if(pval1<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
    
  cat("Pour un test unilatéral gauche:")
    pval2<-mean(b.RV<=RVech)
    cat("p = ",pval2)
    if(pval2<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}
      
  cat("Pour un test bilatéral:")
    pval3<-min(mean(b.RV>=RVech), mean(b.RV<=RVech))*2
    cat("p = ",pval3)
    if(pval3<=(1-nc)) {print("REJET de H0!")}else{print("Non-Rejet de H0")}


```
Les résultats indiquent que la différence observée entre les Rapports de Variation dans les départements du Nord et dans les départements du Sud est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r b.Diff$p.value` ) . Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que les rapports de variations dans les deux pôles du pays sont différents l'un de l'autre.

## Estimation: deux échantillons corrélés (dépendants)

Lorsque deux échantillons sont dépendants (ou corrélés, ou appariés) et que l'objectif d'une analyse est d'évaluer la différence pouvant exister entre entre ces échantillons, que ce soit concernant leur moyenne, leur variance, ou toute autre statistique pertinente pour la question de recherche qui est posée, il est nécessaire de tenir compte de la relation existant entre les deux ensembles de données. Une approche que l'on peut utiliser pour cette prise en compte est de considérer les différences observées entre chaque paire de données. Par exemple, pour des échantillons indépendants, on s'intéresse à évaluer $(\bar {X_1} - \bar {X_2})$. Pour des données corrélées, on examinera d'abord $(X_{i,Condx_1} - X_{i,Condx_2})$ pour chaque paires d'observations. Le résultat est une nouvelle variable formée de ces différences. Dans le cas d'une comparaison de moyennes, la moyenne de ces différences sera nulle, si ce qui distingue les deux mesures n'a pas d'impact.

Un exemple concret permettra de mieux comprendre la situation. Imaginons une classe de mathématiques au niveau Fondamental. On évalue les compétences en calcul au début d'une session académique, puis on ré-évalue les mêmes compétences à la fin de la session. On s'intéresse évidemment à savoir si l'enseignement reçu durant la session a permis d'améliorer les compétences des élèves, en calcul. On peut très bien imaginer que les élèves qui ont de très bonnes habiletés au début de la session auront aussi de bonnes habiletés à la fin, et que ceux qui éprouvent des difficultés au début pourraient encore éprouver quelques difficultés à la fin. Si l'enseignement n'a pas d'impact sur les compétences, alors la moyenne des différences sera près de 0. Si l'enseignement a permis à la plupart des élèves d'améliorer leurs compétences, la moyenne des différences sera négative; dans le cas contraire, elle sera positive. De fait, 
$$(\bar X_1 - \bar X_2)=\bar D$$

Pour les besoins de la discussion, générons un ensemble de données représentant la performance en mathématiques pour un groupe de 40 élèves, évalués au début, puis à la fin de la formation:

```{r 07-Chap7-31}
 n = 40       
 r = 0.7      
 x1 = rnorm(n)  
 x2 = rnorm(n)  
 y1 = r*x2+sqrt(1-r^2)*x1 
 x1 = x1*6+60
 y1 = y1*6+63
 d = x1-y1

 
 cat("Données Observées:")
 data.frame(x1,y1,d)
 cat("Moyenne au début        = ",mean(x1))
 cat("Moyenne à la fin        = ",mean(y1))
 cat("Différence de moyennes  = ",mean(x1)-mean(y1))
 cat("Moyenne des différences = ",mean(d))
 cat("Corrélation (x1,y1)     = ",cor(x1,y1))

boxplot(x1,y1, main="Score Maths / Condition",
   xlab="Score en Maths", ylab="Condition",horizontal=TRUE, names=c("Au début","À la fin")) 
```
### Estimation de la différence de moyennes: données corrélées

L'estimation de la différence de moyennes pour échantillons corrélés se résume à l'estimation de la moyenne des différences. La procédure est donc identique à l'estimation d'une moyenne unique que nous avons étudiée antérieurement. En utilisant le ré-échantillonnage, on obtient:

```{r 07-Chap7-32}
set.seed(1234567)

nc = .95			        # Niveau de confiance
ns=1000000			      # Nombre de ré-échantillonnage à effectuer
xb = replicate(ns,mean(sample(d,n,replace=TRUE)))
intc=quantile(xb,c(.5*(1-nc),nc+.5*(1-nc)))		# Calcul des percentiles
gr1=hist(xb,breaks="FD",plot=FALSE)			      # Création de l'histogramme
cuts=cut(gr1$breaks,c(-Inf,intc[1],intc[2],Inf))
plot(gr1,main="Distribution de la moyenne\n des différences", xlab="Moyennes des différences", col=c("red", "cadetblue1", "red")[cuts])
txt1=paste("Moyenne    = ",round(mean(xb),3))
txt2=paste("Écart-Type = ",round(sd(xb),3))
legend("topleft",legend=c(txt1,txt2),lty=0:0,cex=.8,bg="lightgreen")
txt3=paste("IC(",nc*100,"%) = [",round(intc[1],3),",",round(intc[2],3),"]")
legend("topright",legend=c(txt3),lty=0:0,cex=.8,bg="cadetblue1")

```
 On trouve que:  
 
 C[ `r intc[1]` $< \mu_D < $ `r intc[2]` ] = 95%  
 
Bref, on a 95% des chances que l'intervalle s'étendant de `r intc[1]` à `r intc[2]` inclut la moyenne des différences au niveau de la population. On a 5% des chance que cela soit faux.

Des résultats similaires peuvent s'obtenir à l'aide de la fonction **boot.paired.per()** de la librairie **wBoot**:

```{r 07-Chap7-33}
library(wBoot)
b.Diff = boot.paired.per(x1, y1, conf.level = 0.95, R = 9999)
print.boot.paired(b.Diff)

```
On obtiendra évidemment des résultats similaires en utilisant la fonction **boot.one.per()** sur l'ensemble des différences **_d_**:

```{r 07-Chap7-34}
library(wBoot)
b.Diff = boot.one.per(d,mean, conf.level = 0.95, R = 9999)
print.boot.one(b.Diff)

```

### Tests d'hypothèse: deux échantillons corrélés

Un test d'hypothèse concernant deux échantillons corrélés impliquera les différences observées pour chaque paire d'observations, comme c'est le cas pour une estimation de la moyenne de ces différences. De ce fait, la procédure sera identique à celle utilisée pour un test d'hypothèse sur une moyenne unique, en utilisant le vecteur des différences comme variable analysée.  

### Différence de moyennes

Pour les données définies dans l'exemple précédent, supposons qu'on tente de vérifier si l'enseignement a été profitable aux élèves, c'est-à-dire que la performance à la fin de la session a été meilleure qu'au début: la différence des moyennes devrait être négative. Utilisons la fonction **boot.one.per()** de la librairie **wBoot**, pour obtenir:

```{r 07-Chap7-35}
b.Diff = boot.one.per(d,mean,null.hyp=0,alternative="less",R=100000)
print.boot.one(b.Diff)
```
La fonction **boot.paired.per** donnera évidemment des résultats similaires, en utilisant les données originales (x1 et y1):

```{r 07-Chap7-36}
#boot.paired.bca(x1,y1,mean,null.hyp=0,alternative="less",)
b.Diff = boot.paired.per(x1, y1, variable = "Maths", null.hyp = 0,
alternative = "less", conf.level = 0.95, type = NULL, R = 100000)
b.Diff
```



## Exercices

### 1. Comparaison de moyennes I
  
Pourrait-il exister une différence, au niveau de la population, entre la performance moyenne des hommes et des femmes au test de HabilCog, dans le département des Nippes? Pour répondre à cette question, considérons des échantillons de 35 hommes et de 40 femmes.

```{r 07-Chap7-37}
# Création des sous-populations
subpop = subset(HaitiPop,(DEPARTEMENT=="4"),c(GENRE,HabilCog))
subpopH = subset(subpop,GENRE=="M",HabilCog)
subpopF = subset(subpop,GENRE=="F",HabilCog)
# Création des échantillons
nH = 35
nF = 40
xechH = subpopH$HabilCog[sample(1:nrow(subpopH),nH,replace=FALSE)]
xechF = subpopF$HabilCog[sample(1:nrow(subpopF),nF,replace=FALSE)]
cat("Moyenne (Hommes) = ",mean(xechH))
cat("Moyenne (Femmes) = ",mean(xechF))
```
Le diagramme suivant illustre et résume la situation:

```{r 07-Chap7-38}
boxplot(xechH,xechF, main="HabilCog vx. GENRE",
   xlab="Genre", ylab="HabilCog",horizontal=TRUE, names=c("Hommes","Femmes")) 
```


Les moyennes des échantillons sont de toute évidence différentes, est-ce que cette différence est le simple fait de l'erreur d'échantillonnage, ou réflète-t'elle une différence réelle entre les moyennes des populations d'où ces échantillons ont été tirés?

Les hypothèses statistiques sont:

$H_0:\mu_{Hommes}-\mu_{Femmes}=0$ 
  
$H_0:\mu_{Hommes}-\mu_{Femmes}\neq0$

```{r 07-Chap7-39}
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechH,xechF,mean,null.hyp=0,alternative="two.sided",conf.level=0.95,R=100000,stacked=FALSE)
b.Diff
print.boot.two(b.Diff)
p = b.Diff$p.value
```
Les résultats indiquent que la différence observée entre les moyennes des deux échantillons (hommes et femmes) est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r p` ). Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que la performance des hommes au test d'HabilCog diffère de celle des femmes. 

###2. Comparaison de moyennes II
  
Est-ce qu'il y a une différence entre l'indice de criminalité chez les hommes vivant en Artibonite et ceux vivant dans le département du Centre? On peut disposer d'un échantillon de 50 sujets en Artibonite, et de 45 sujets dans le département du Centre pour examiner la question.

```{r 07-Chap7-40}
# Création des sous-populations
subpopA = subset(HaitiPop,((DEPARTEMENT=="1")&(GENRE=="M")&(AGE>=15)),CRIMIDX)
subpopC = subset(HaitiPop,((DEPARTEMENT=="2")&(GENRE=="M")&(AGE>=15)),CRIMIDX)

# Création des échantillons
nA = 50
nC = 45
xechA = subpopA$CRIMIDX[sample(1:nrow(subpopA),nA,replace=FALSE)]
xechC = subpopC$CRIMIDX[sample(1:nrow(subpopC),nC,replace=FALSE)]
cat("Moyenne (Artibonite) = ",mean(xechA))
cat("Moyenne (Centre)     = ",mean(xechC))

boxplot(xechA,xechC, main="CRIMIDX vx. Département",
   xlab="CRIMIDX", ylab="Département",horizontal=TRUE, names=c("Artibonite","Centre")) 
```
On constate que les deux moyennes échantillonnales sont différentes. Est-ce que cette différence est le simple fait de l'erreur d'échantillonnage, ou réflète-t'il une différence réelle entre les moyennes des populations d'où ces échantillons ont été tirés?

Les hypothèses statistiques sont:

$H_0:\mu_{Artibonite}-\mu_{Centre}=0$
  
$H_0:\mu_{Artibonite}-\mu_{Centre}\neq0$

```{r 07-Chap7-41}
#
# Ré-échantillonnage et distribution d'échantillonnage
#
library(wBoot)
b.Diff = boot.two.per(xechA,xechC,mean,null.hyp=0,alternative="two.sided",conf.level=0.95,R=100000,stacked=FALSE)
b.Diff
print.boot.two(b.Diff)
p = b.Diff$p.value
```
Les résultats indiquent que la différence observée entre les moyennes des deux échantillons (Artibonite et Centre) est un événement `r if(p<=0.05){"improbable"}else{"probable"}` (p = `r b.Diff$p.value` ). Ce résultat va `r if(p<=0.05){"à l'encontre"}else{"dans le sens"}` de ce que propose $H_0$. On  `r if(p<=0.05){"peut"}else{"ne peut pas"}` rejeter cette hypothèse. La différence observée `r if(p<=0.05){"réflète"}else{"ne réflète pas"}` une réalité au niveau de la population, et on la considère comme statistiquement `r if(p<=0.05){"significative"}else{"non-significative"}` . Il `r if(p<=0.05){"y a "}else{"n'y a pas"}` évidence à l'effet que l'indice moyen de criminalité en Artibonite diffère de celui du département du Centre. 

### 3. Comparaison de deux proportions (Échantillons indépendants)

Considérez la proportion des femmes ayant l'intention de voter pour le Candidat B aux prochaines élections présidentielles. Est-ce que cette proportion diffère entre le département du Sud-Est et le département du Nord-Ouest?

### 4. Comparaison de deux écarts-types (Échantillons indépendants)

Est-ce que, à l'échelle du pays, la dispersion de l'indice de criminalité (CRIMIDX) varie selon le sexe des sujets?

### 5. Comparaison de moyennes (Échantillons corrélés)

L'appréciation d'un candidat à l'élection présidentielle est mesurée sur une échelle de 1(aucune affinité avec le candidat) à 50 (Confiance absolue pour le candidat), avant et après un débat politique public. Un échantillon de 50 sujets est formé afin d'examiner l'impact qu'a eu le débat sur la mesure d'appréciation. Les données sont générées à l'aide du code R suivant (prenez soin d'installer la librairie **MASS** au préalable):

```{r 07-Chap7-42}

library(MASS)
n = 50
mu = c(15,12)
A = matrix(runif(4)*2-1, ncol=2) 
sigma = matrix(c(82.7,-45.46,-45.46,34.36),nrow=2)
x = abs(round(mvrnorm(n=50, mu, sigma, empirical = TRUE)))
head(x)
cat("Corrélation = ",cor(x)[1,2])

```
Est-ce que le débat public a eu un impact significatif sur l'appréciation du candidat?

<!--chapter:end:07-Chap7.Rmd-->

